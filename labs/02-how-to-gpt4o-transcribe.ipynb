{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d474318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "import asyncio\n",
    "import wave\n",
    "from datetime import datetime\n",
    "from typing import Optional, Callable, Dict, Any\n",
    "\n",
    "import pyaudio\n",
    "import websockets\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1db1c4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_audio_input_devices() -> None:\n",
    "    \"\"\"\n",
    "    Print all available input devices (microphones) for user selection.\n",
    "    \"\"\"\n",
    "    p = pyaudio.PyAudio()\n",
    "    print(\"\\nAvailable audio input devices:\")\n",
    "    for i in range(p.get_device_count()):\n",
    "        dev = p.get_device_info_by_index(i)\n",
    "        if dev[\"maxInputChannels\"] > 0:\n",
    "            print(f\"{i}: {dev['name']}\")\n",
    "    p.terminate()\n",
    "\n",
    "\n",
    "def choose_default_audio_device() -> int:\n",
    "    \"\"\"\n",
    "    Return the index of the default audio input device, or prompt user if >1.\n",
    "    \"\"\"\n",
    "    p = pyaudio.PyAudio()\n",
    "    mic_indices = [\n",
    "        i\n",
    "        for i in range(p.get_device_count())\n",
    "        if p.get_device_info_by_index(i)[\"maxInputChannels\"] > 0\n",
    "    ]\n",
    "    p.terminate()\n",
    "    if len(mic_indices) == 0:\n",
    "        raise RuntimeError(\"No microphone devices found.\")\n",
    "    if len(mic_indices) == 1:\n",
    "        print(f\"Auto-selecting only available input device: {mic_indices[0]}\")\n",
    "        return mic_indices[0]\n",
    "    list_audio_input_devices()\n",
    "    try:\n",
    "        idx = int(\n",
    "            input(f\"Select audio input device index [{mic_indices[0]}]: \")\n",
    "            or mic_indices[0]\n",
    "        )\n",
    "    except Exception:\n",
    "        idx = mic_indices[0]\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4edd8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioRecorder:\n",
    "    \"\"\"\n",
    "    Async audio recorder using PyAudio.\n",
    "    Allows independent recording (to memory and .wav) and streaming (for STT).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        rate: int,\n",
    "        channels: int,\n",
    "        format_: int,\n",
    "        chunk: int,\n",
    "        device_index: Optional[int] = None,\n",
    "    ):\n",
    "        self.rate = rate\n",
    "        self.channels = channels\n",
    "        self.format = format_\n",
    "        self.chunk = chunk\n",
    "        self.device_index = (\n",
    "            device_index if device_index is not None else choose_default_audio_device()\n",
    "        )\n",
    "        self.p = pyaudio.PyAudio()\n",
    "        self.stream = None\n",
    "        self.frames = []\n",
    "        self.audio_queue: asyncio.Queue[bytes] = asyncio.Queue()\n",
    "        self._loop = asyncio.get_event_loop()\n",
    "        self._running = False\n",
    "\n",
    "    def start(self) -> None:\n",
    "        \"\"\"\n",
    "        Start the audio stream and begin capturing to the queue.\n",
    "        \"\"\"\n",
    "\n",
    "        def callback(in_data, frame_count, time_info, status):\n",
    "            self.frames.append(in_data)\n",
    "            self._loop.call_soon_threadsafe(self.audio_queue.put_nowait, in_data)\n",
    "            return (None, pyaudio.paContinue)\n",
    "\n",
    "        self.stream = self.p.open(\n",
    "            format=self.format,\n",
    "            channels=self.channels,\n",
    "            rate=self.rate,\n",
    "            input=True,\n",
    "            input_device_index=self.device_index,\n",
    "            frames_per_buffer=self.chunk,\n",
    "            stream_callback=callback,\n",
    "        )\n",
    "        self._running = True\n",
    "        self.stream.start_stream()\n",
    "\n",
    "    def stop(self) -> None:\n",
    "        \"\"\"\n",
    "        Stop and close the stream, release audio resources.\n",
    "        \"\"\"\n",
    "        self._running = False\n",
    "        if self.stream is not None:\n",
    "            self.stream.stop_stream()\n",
    "            self.stream.close()\n",
    "        self.p.terminate()\n",
    "\n",
    "    def save_wav(self, filename: str) -> None:\n",
    "        \"\"\"\n",
    "        Save the recorded audio to a .wav file.\n",
    "        \"\"\"\n",
    "        wf = wave.open(filename, \"wb\")\n",
    "        wf.setnchannels(self.channels)\n",
    "        wf.setsampwidth(self.p.get_sample_size(self.format))\n",
    "        wf.setframerate(self.rate)\n",
    "        wf.writeframes(b\"\".join(self.frames))\n",
    "        wf.close()\n",
    "        print(f\"üéôÔ∏è Audio saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "633c4bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranscriptionClient:\n",
    "    \"\"\"\n",
    "    Handles async websocket transcription session to Azure OpenAI STT.\n",
    "    Can be used independently: just supply an async generator of audio chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        url: str,\n",
    "        headers: dict,\n",
    "        session_config: Dict[str, Any],\n",
    "        on_delta: Optional[Callable[[str], None]] = None,\n",
    "        on_transcript: Optional[Callable[[str], None]] = None,\n",
    "    ):\n",
    "        self.url = url\n",
    "        self.headers = headers\n",
    "        self.session_config = session_config\n",
    "        self.ws: Optional[websockets.WebSocketClientProtocol] = None\n",
    "        self._on_delta = on_delta\n",
    "        self._on_transcript = on_transcript\n",
    "        self._running = False\n",
    "        self._send_task = None\n",
    "        self._recv_task = None\n",
    "\n",
    "    async def __aenter__(self):\n",
    "        try:\n",
    "            self.ws = await websockets.connect(\n",
    "                self.url, additional_headers=self.headers\n",
    "            )\n",
    "        except TypeError:\n",
    "            self.ws = await websockets.connect(self.url, extra_headers=self.headers)\n",
    "        self._running = True\n",
    "        return self\n",
    "\n",
    "    async def __aexit__(self, exc_type, exc, tb):\n",
    "        self._running = False\n",
    "        if self.ws:\n",
    "            await self.ws.close()\n",
    "        if self._send_task:\n",
    "            self._send_task.cancel()\n",
    "        if self._recv_task:\n",
    "            self._recv_task.cancel()\n",
    "\n",
    "    async def send_json(self, data: dict) -> None:\n",
    "        if self.ws:\n",
    "            await self.ws.send(json.dumps(data))\n",
    "\n",
    "    async def send_audio_chunk(self, audio_data: bytes) -> None:\n",
    "        audio_base64 = base64.b64encode(audio_data).decode(\"utf-8\")\n",
    "        await self.send_json(\n",
    "            {\"type\": \"input_audio_buffer.append\", \"audio\": audio_base64}\n",
    "        )\n",
    "\n",
    "    async def start_session(self, rate: int, channels: int) -> None:\n",
    "        session_config = {\n",
    "            \"type\": \"transcription_session.update\",\n",
    "            \"session\": self.session_config,\n",
    "        }\n",
    "        await self.send_json(session_config)\n",
    "        await self.send_json(\n",
    "            {\n",
    "                \"type\": \"audio_start\",\n",
    "                \"data\": {\"encoding\": \"pcm\", \"sample_rate\": rate, \"channels\": channels},\n",
    "            }\n",
    "        )\n",
    "\n",
    "    async def receive_loop(self) -> None:\n",
    "        async for message in self.ws:\n",
    "            try:\n",
    "                data = json.loads(message)\n",
    "                event_type = data.get(\"type\", \"\")\n",
    "                if event_type == \"conversation.item.input_audio_transcription.delta\":\n",
    "                    delta = data.get(\"delta\", \"\")\n",
    "                    if delta and self._on_delta:\n",
    "                        self._on_delta(delta)\n",
    "                elif (\n",
    "                    event_type\n",
    "                    == \"conversation.item.input_audio_transcription.completed\"\n",
    "                ):\n",
    "                    transcript = data.get(\"transcript\", \"\")\n",
    "                    if transcript and self._on_transcript:\n",
    "                        self._on_transcript(transcript)\n",
    "                elif event_type == \"conversation.item.created\":\n",
    "                    transcript = data.get(\"item\", \"\")\n",
    "                    if (\n",
    "                        isinstance(transcript, dict)\n",
    "                        and \"content\" in transcript\n",
    "                        and transcript[\"content\"]\n",
    "                    ):\n",
    "                        t = transcript[\"content\"][0].get(\"transcript\")\n",
    "                        if t and self._on_transcript:\n",
    "                            self._on_transcript(t)\n",
    "                    elif transcript and self._on_transcript:\n",
    "                        self._on_transcript(str(transcript))\n",
    "            except Exception as e:\n",
    "                print(\"‚ùå Error parsing message:\", e)\n",
    "\n",
    "    async def run(self, audio_chunk_iter: asyncio.Queue, rate: int, channels: int):\n",
    "        \"\"\"\n",
    "        Main loop: configure session, send audio from queue, receive results.\n",
    "        \"\"\"\n",
    "        await self.start_session(rate, channels)\n",
    "        self._send_task = asyncio.create_task(self._send_audio_loop(audio_chunk_iter))\n",
    "        self._recv_task = asyncio.create_task(self.receive_loop())\n",
    "        done, pending = await asyncio.wait(\n",
    "            [self._send_task, self._recv_task], return_when=asyncio.FIRST_COMPLETED\n",
    "        )\n",
    "        for task in pending:\n",
    "            task.cancel()\n",
    "\n",
    "    async def _send_audio_loop(self, audio_queue: asyncio.Queue):\n",
    "        while self._running:\n",
    "            try:\n",
    "                audio_data = await audio_queue.get()\n",
    "                if audio_data is None:\n",
    "                    break\n",
    "                await self.send_audio_chunk(audio_data)\n",
    "            except asyncio.CancelledError:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3724fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioTranscriber:\n",
    "    \"\"\"\n",
    "    High-level orchestrator for audio recording and real-time transcription.\n",
    "    Use as: record only, transcribe only, or chain both (record+transcribe).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        url: str,\n",
    "        headers: dict,\n",
    "        rate: int,\n",
    "        channels: int,\n",
    "        format_: int,\n",
    "        chunk: int,\n",
    "        device_index: Optional[int] = None,\n",
    "    ):\n",
    "        self.url = url\n",
    "        self.headers = headers\n",
    "        self.rate = rate\n",
    "        self.channels = channels\n",
    "        self.format = format_\n",
    "        self.chunk = chunk\n",
    "        self.device_index = device_index\n",
    "\n",
    "    async def record(\n",
    "        self, duration: Optional[float] = None, output_file: Optional[str] = None\n",
    "    ) -> AudioRecorder:\n",
    "        \"\"\"\n",
    "        Record audio from mic. Returns AudioRecorder.\n",
    "        Optionally, specify duration (seconds). Use output_file to auto-save.\n",
    "        \"\"\"\n",
    "        recorder = AudioRecorder(\n",
    "            rate=self.rate,\n",
    "            channels=self.channels,\n",
    "            format_=self.format,\n",
    "            chunk=self.chunk,\n",
    "            device_index=self.device_index,\n",
    "        )\n",
    "        recorder.start()\n",
    "        print(\n",
    "            f\"Recording{' for ' + str(duration) + ' seconds' if duration else ' (Ctrl+C to stop)'}...\"\n",
    "        )\n",
    "        try:\n",
    "            if duration:\n",
    "                await asyncio.sleep(duration)\n",
    "            else:\n",
    "                while True:\n",
    "                    await asyncio.sleep(0.5)\n",
    "        except (KeyboardInterrupt, asyncio.CancelledError):\n",
    "            pass\n",
    "        finally:\n",
    "            recorder.stop()\n",
    "            if output_file:\n",
    "                recorder.save_wav(output_file)\n",
    "        return recorder\n",
    "\n",
    "    async def transcribe(\n",
    "        self,\n",
    "        audio_queue: Optional[asyncio.Queue] = None,\n",
    "        model: str = \"gpt-4o-transcribe\",\n",
    "        prompt: Optional[str] = \"Respond in English.\",\n",
    "        language: Optional[str] = None,\n",
    "        noise_reduction: str = \"near_field\",\n",
    "        vad_type: str = \"server_vad\",\n",
    "        vad_config: Optional[dict] = None,\n",
    "        on_delta: Optional[Callable[[str], None]] = None,\n",
    "        on_transcript: Optional[Callable[[str], None]] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Run a transcription session with full model/config control.\n",
    "        If audio_queue is None, uses a live AudioRecorder.\n",
    "        \"\"\"\n",
    "        if audio_queue is None:\n",
    "            recorder = AudioRecorder(\n",
    "                rate=self.rate,\n",
    "                channels=self.channels,\n",
    "                format_=self.format,\n",
    "                chunk=self.chunk,\n",
    "                device_index=self.device_index,\n",
    "            )\n",
    "            recorder.start()\n",
    "            audio_queue = recorder.audio_queue\n",
    "        else:\n",
    "            recorder = None\n",
    "\n",
    "        session_config = {\n",
    "            \"input_audio_format\": \"pcm16\",\n",
    "            \"input_audio_transcription\": {\n",
    "                \"model\": model,\n",
    "                \"prompt\": prompt,\n",
    "            },\n",
    "            \"input_audio_noise_reduction\": {\"type\": noise_reduction},\n",
    "            \"turn_detection\": {\"type\": vad_type} if vad_type else None,\n",
    "        }\n",
    "        if vad_config:\n",
    "            session_config[\"turn_detection\"].update(vad_config)\n",
    "        if language:\n",
    "            session_config[\"input_audio_transcription\"][\"language\"] = language\n",
    "\n",
    "        async with TranscriptionClient(\n",
    "            self.url, self.headers, session_config, on_delta, on_transcript\n",
    "        ) as client:\n",
    "            try:\n",
    "                await client.run(audio_queue, self.rate, self.channels)\n",
    "            except asyncio.CancelledError:\n",
    "                print(\"Transcription cancelled.\")\n",
    "            finally:\n",
    "                if recorder:\n",
    "                    recorder.stop()\n",
    "                    filename = f\"microphone_capture_{datetime.now():%Y%m%d_%H%M%S}.wav\"\n",
    "                    recorder.save_wav(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8f798bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    load_dotenv()\n",
    "    OPENAI_API_KEY = os.environ.get(\"AZURE_OPENAI_STT_TTS_KEY\")\n",
    "    AZURE_OPENAI_ENDPOINT = os.environ.get(\"AZURE_OPENAI_STT_TTS_ENDPOINT\")\n",
    "    if not OPENAI_API_KEY or not OPENAI_API_KEY:\n",
    "        raise RuntimeError(\"‚ùå API key or endpoint missing in environment.\")\n",
    "\n",
    "    url = f\"{AZURE_OPENAI_ENDPOINT.replace('https', 'wss')}/openai/realtime?api-version=2025-04-01-preview&intent=transcription\"\n",
    "    headers = {\"api-key\": OPENAI_API_KEY}\n",
    "    RATE = 24000\n",
    "    CHANNELS = 1\n",
    "    FORMAT = pyaudio.paInt16\n",
    "    CHUNK = 1024\n",
    "\n",
    "    device_index = choose_default_audio_device()\n",
    "\n",
    "    transcriber = AudioTranscriber(\n",
    "        url=url,\n",
    "        headers=headers,\n",
    "        rate=RATE,\n",
    "        channels=CHANNELS,\n",
    "        format_=FORMAT,\n",
    "        chunk=CHUNK,\n",
    "        device_index=device_index,\n",
    "    )\n",
    "\n",
    "    def print_delta(delta: str):\n",
    "        print(delta, end=\" \", flush=True)\n",
    "\n",
    "    def print_transcript(transcript: str):\n",
    "        print(f\"\\n‚úÖ Transcript: {transcript}\")\n",
    "\n",
    "    print(\">>> Starting real-time transcription session. Ctrl+C to stop.\")\n",
    "    try:\n",
    "        await transcriber.transcribe(\n",
    "            model=\"gpt-4o-transcribe\",\n",
    "            prompt=\"Respond in English. This is a medical environment.\",\n",
    "            noise_reduction=\"near_field\",\n",
    "            vad_type=\"server_vad\",\n",
    "            vad_config={\n",
    "                \"threshold\": 0.5,\n",
    "                \"prefix_padding_ms\": 300,\n",
    "                \"silence_duration_ms\": 200,\n",
    "            },\n",
    "            on_delta=print_delta,\n",
    "            on_transcript=print_transcript,\n",
    "        )\n",
    "    except (KeyboardInterrupt, asyncio.CancelledError):\n",
    "        print(\"\\nüõë Interrupted by user. Exiting...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cf60104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available audio input devices:\n",
      "0: Microsoft Sound Mapper - Input\n",
      "1: Surface Stereo Microphones (Sur\n",
      "2: Echo Cancelling Speakerphone (M\n",
      "3: Microphone (Lumina Camera - Raw\n",
      "8: Primary Sound Capture Driver\n",
      "9: Surface Stereo Microphones (Surface High Definition Audio)\n",
      "10: Echo Cancelling Speakerphone (Microsoft Audio Dock)\n",
      "11: Microphone (Lumina Camera - Raw)\n",
      "19: Echo Cancelling Speakerphone (Microsoft Audio Dock)\n",
      "20: Microphone (Lumina Camera - Raw)\n",
      "21: Surface Stereo Microphones (Surface High Definition Audio)\n",
      "24: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Shiva‚Äôs AirPods Pro #2))\n",
      "26: Microphone (Dell USB Audio)\n",
      "28: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Shiva‚Äôs AirPods Pro #2 - Find My))\n",
      "31: PC Speaker (Realtek HD Audio 2nd output with SST)\n",
      "34: PC Speaker (Realtek HD Audio output with SST)\n",
      "35: Microphone Array (Realtek HD Audio Mic input)\n",
      "36: Headset Microphone (Headset Microphone)\n",
      "38: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Pablo‚Äôs AirPods #3))\n",
      "40: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Pablo‚Äôs AirPods #4))\n",
      "43: Microphone (Lumina Camera - Raw)\n",
      "45: Echo Cancelling Speakerphone (Microsoft Audio Dock)\n",
      "48: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Pablo‚Äôs AirPods Pro - Find My))\n",
      ">>> Starting real-time transcription session. Ctrl+C to stop.\n",
      "\n",
      "‚úÖ Transcript: Hey, how are you doing? Are you doing well?\n",
      "\n",
      "‚úÖ Transcript: Awesome.\n",
      "\n",
      "‚úÖ Transcript: Great.\n",
      "\n",
      "‚úÖ Transcript: See you.\n",
      "Transcription cancelled.\n",
      "üéôÔ∏è Audio saved to microphone_capture_20250516_085716.wav\n"
     ]
    }
   ],
   "source": [
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc49fe0b",
   "metadata": {},
   "source": [
    "## ACS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791231ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "from typing import Any, Dict, Optional\n",
    "import websockets\n",
    "from base64 import b64decode, b64encode\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"acs-openai-relay\")\n",
    "\n",
    "\n",
    "# Message transformation (see your provided functions)\n",
    "def transform_acs_to_openai_format(\n",
    "    msg_data: dict, model: str = \"gpt-4o-transcribe\"\n",
    ") -> Optional[dict]:\n",
    "    \"\"\"\n",
    "    Map ACS AudioData to OpenAI input_audio_buffer.append.\n",
    "    If AudioMetadata, send session config to OpenAI.\n",
    "    \"\"\"\n",
    "    if msg_data.get(\"kind\") == \"AudioMetadata\":\n",
    "        return {\n",
    "            \"type\": \"transcription_session.update\",\n",
    "            \"session\": {\n",
    "                \"input_audio_format\": \"pcm16\",\n",
    "                \"input_audio_transcription\": {\n",
    "                    \"model\": model,\n",
    "                    \"prompt\": \"Respond in English.\",\n",
    "                },\n",
    "                \"input_audio_noise_reduction\": {\"type\": \"near_field\"},\n",
    "                \"turn_detection\": {\n",
    "                    \"type\": \"server_vad\",\n",
    "                    \"threshold\": 0.7,\n",
    "                    \"prefix_padding_ms\": 300,\n",
    "                    \"silence_duration_ms\": 500,\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    elif msg_data.get(\"kind\") == \"AudioData\":\n",
    "        return {\n",
    "            \"type\": \"input_audio_buffer.append\",\n",
    "            \"audio\": msg_data[\"audioData\"][\"data\"],\n",
    "        }\n",
    "    return None\n",
    "\n",
    "\n",
    "def transform_openai_to_acs_format(msg_data: dict) -> Optional[dict]:\n",
    "    \"\"\"\n",
    "    Map OpenAI transcript/completion to ACS format for playback (e.g., TTS or text event).\n",
    "    \"\"\"\n",
    "    # For actual audio response, you'd encode the TTS audio as base64 and send as \"AudioData\"\n",
    "    if msg_data.get(\"type\") == \"conversation.item.input_audio_transcription.delta\":\n",
    "        # Live text: send as event or queue for TTS\n",
    "        return {\"kind\": \"TranscriptionDelta\", \"text\": msg_data.get(\"delta\")}\n",
    "    elif (\n",
    "        msg_data.get(\"type\") == \"conversation.item.input_audio_transcription.completed\"\n",
    "    ):\n",
    "        # Final transcript: send to ACS as event or trigger TTS\n",
    "        return {\"kind\": \"TranscriptionCompleted\", \"text\": msg_data.get(\"transcript\")}\n",
    "    # Add more types as needed\n",
    "    return None\n",
    "\n",
    "\n",
    "async def relay_acs_to_openai(acs_ws, openai_ws, model: str):\n",
    "    \"\"\"\n",
    "    Relay messages from ACS to OpenAI (audio).\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            msg = await acs_ws.receive_text()\n",
    "            data = json.loads(msg)\n",
    "            mapped = transform_acs_to_openai_format(data, model=model)\n",
    "            if mapped:\n",
    "                await openai_ws.send(json.dumps(mapped))\n",
    "        except Exception as e:\n",
    "            logger.error(f\"ACS ‚Üí OpenAI relay error: {e}\")\n",
    "            break\n",
    "\n",
    "\n",
    "async def relay_openai_to_acs(openai_ws, acs_ws):\n",
    "    \"\"\"\n",
    "    Relay messages from OpenAI to ACS (text, function calls, events).\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            msg = await openai_ws.recv()\n",
    "            data = json.loads(msg)\n",
    "            mapped = transform_openai_to_acs_format(data)\n",
    "            if mapped:\n",
    "                await acs_ws.send_text(json.dumps(mapped))\n",
    "        except Exception as e:\n",
    "            logger.error(f\"OpenAI ‚Üí ACS relay error: {e}\")\n",
    "            break\n",
    "\n",
    "\n",
    "async def main_acs_openai_relay(\n",
    "    acs_ws_url: str,\n",
    "    openai_ws_url: str,\n",
    "    openai_api_key: str,\n",
    "    model: str = \"gpt-4o-transcribe\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Opens two WebSockets and relays audio and transcription between ACS and OpenAI Realtime STT.\n",
    "    \"\"\"\n",
    "    # Connect to ACS as a client WebSocket (simulate incoming call, or use FastAPI's ws param in handler)\n",
    "    async with websockets.connect(acs_ws_url) as acs_ws:\n",
    "        logger.info(f\"Connected to ACS WebSocket at {acs_ws_url}\")\n",
    "\n",
    "        # Connect to Azure OpenAI Realtime STT\n",
    "        openai_headers = {\"api-key\": openai_api_key}\n",
    "        async with websockets.connect(\n",
    "            openai_ws_url, extra_headers=openai_headers\n",
    "        ) as openai_ws:\n",
    "            logger.info(f\"Connected to OpenAI Realtime WebSocket at {openai_ws_url}\")\n",
    "\n",
    "            # Run both relays in parallel\n",
    "            relay_acs = asyncio.create_task(\n",
    "                relay_acs_to_openai(acs_ws, openai_ws, model)\n",
    "            )\n",
    "            relay_openai = asyncio.create_task(relay_openai_to_acs(openai_ws, acs_ws))\n",
    "            done, pending = await asyncio.wait(\n",
    "                [relay_acs, relay_openai],\n",
    "                return_when=asyncio.FIRST_COMPLETED,\n",
    "            )\n",
    "            for task in pending:\n",
    "                task.cancel()\n",
    "            logger.info(\"Relay session closed.\")\n",
    "\n",
    "\n",
    "# ----------- ENV & EXAMPLE USAGE -----------\n",
    "if __name__ == \"__main__\":\n",
    "    load_dotenv()\n",
    "    # Normally ACS WS comes from a FastAPI endpoint, but for demo:\n",
    "    ACS_WS_URL = os.getenv(\n",
    "        \"ACS_TEST_WEBSOCKET_URL\"\n",
    "    )  # ws://localhost:9000/acs-test (simulate)\n",
    "    AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_STT_TTS_ENDPOINT\")\n",
    "    OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_STT_TTS_KEY\")\n",
    "\n",
    "    model = \"gpt-4o-transcribe\"  # or \"gpt-4o-mini-transcribe\"\n",
    "\n",
    "    # Azure OpenAI real-time WebSocket URL\n",
    "    openai_ws_url = f\"{AZURE_OPENAI_ENDPOINT.replace('https', 'wss')}/openai/realtime?api-version=2025-04-01-preview&intent=transcription\"\n",
    "\n",
    "    # You would use this relay as part of your /realtime-acs FastAPI websocket handler,\n",
    "    # passing the FastAPI websocket as `acs_ws` and launching the relay to OpenAI.\n",
    "    # For demo, both URLs should be testable WebSocket echo endpoints.\n",
    "\n",
    "    if not ACS_WS_URL or not OPENAI_API_KEY or not AZURE_OPENAI_ENDPOINT:\n",
    "        print(\n",
    "            \"Set ACS_TEST_WEBSOCKET_URL, AZURE_OPENAI_STT_TTS_ENDPOINT, and AZURE_OPENAI_STT_TTS_KEY in your .env file!\"\n",
    "        )\n",
    "    else:\n",
    "        asyncio.run(\n",
    "            main_acs_openai_relay(\n",
    "                acs_ws_url=ACS_WS_URL,\n",
    "                openai_ws_url=openai_ws_url,\n",
    "                openai_api_key=OPENAI_API_KEY,\n",
    "                model=model,\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf2a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not already installed, install these\n",
    "!pip install azure-communication-callautomation python-dotenv websockets aiohttp openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42e877e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "import websockets\n",
    "from base64 import b64decode, b64encode\n",
    "from azure.communication.callautomation import (\n",
    "    CallAutomationClient,\n",
    "    CallInvite,\n",
    "    PhoneNumberIdentifier,\n",
    "    MediaStreamingOptions,\n",
    "    MediaStreamingTransportType,\n",
    "    MediaStreamingContentType,\n",
    "    MediaStreamingAudioChannelType,\n",
    "    AudioFormat,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64517c69",
   "metadata": {},
   "source": [
    "## Load Environment Variables\n",
    "\n",
    "Put your secrets in a .env file in the notebook‚Äôs directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fbebcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "ACS_CONNECTION_STRING = os.getenv(\"ACS_CONNECTION_STRING\")\n",
    "ACS_SOURCE_PHONE_NUMBER = os.getenv(\"ACS_SOURCE_PHONE_NUMBER\")\n",
    "ACS_TARGET_PHONE_NUMBER = os.getenv(\"ACS_TARGET_PHONE_NUMBER\")\n",
    "AZURE_OPENAI_STT_TTS_KEY = os.getenv(\"AZURE_OPENAI_STT_TTS_KEY\")\n",
    "AZURE_OPENAI_STT_TTS_ENDPOINT = os.getenv(\"AZURE_OPENAI_STT_TTS_ENDPOINT\")\n",
    "BASE_URL = os.getenv(\"BASE_URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a78a9d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import websockets\n",
    "import json\n",
    "from base64 import b64decode\n",
    "import wave\n",
    "\n",
    "# Store all audio for .wav\n",
    "all_pcm_bytes = []\n",
    "\n",
    "\n",
    "async def acs_ws_handler(websocket, path):\n",
    "    print(f\"ACS: WebSocket connection accepted from {websocket.remote_address}\")\n",
    "\n",
    "    # Connect to Azure OpenAI Realtime STT\n",
    "    openai_ws_url = f\"{AZURE_OPENAI_STT_TTS_ENDPOINT.replace('https', 'wss')}/openai/realtime?api-version=2025-04-01-preview&intent=transcription\"\n",
    "    openai_headers = {\"api-key\": AZURE_OPENAI_STT_TTS_KEY}\n",
    "    async with websockets.connect(\n",
    "        openai_ws_url, extra_headers=openai_headers\n",
    "    ) as openai_ws:\n",
    "        print(\"Connected to Azure OpenAI Realtime STT API\")\n",
    "\n",
    "        # Send session config\n",
    "        session_config = {\n",
    "            \"type\": \"transcription_session.update\",\n",
    "            \"session\": {\n",
    "                \"input_audio_format\": \"pcm16\",\n",
    "                \"input_audio_transcription\": {\n",
    "                    \"model\": \"gpt-4o-transcribe\",\n",
    "                    \"prompt\": \"Respond in English.\",\n",
    "                },\n",
    "                \"input_audio_noise_reduction\": {\"type\": \"near_field\"},\n",
    "                \"turn_detection\": {\"type\": \"server_vad\"},\n",
    "            },\n",
    "        }\n",
    "        await openai_ws.send(json.dumps(session_config))\n",
    "\n",
    "        # Send audio_start\n",
    "        await openai_ws.send(\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"type\": \"audio_start\",\n",
    "                    \"data\": {\"encoding\": \"pcm\", \"sample_rate\": 16000, \"channels\": 1},\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Start relays\n",
    "        relay1 = asyncio.create_task(acs_to_openai_relay(websocket, openai_ws))\n",
    "        relay2 = asyncio.create_task(openai_to_transcript_relay(openai_ws))\n",
    "        await asyncio.gather(relay1, relay2)\n",
    "\n",
    "\n",
    "async def acs_to_openai_relay(acs_ws, openai_ws):\n",
    "    while True:\n",
    "        try:\n",
    "            msg = await acs_ws.recv()\n",
    "            data = json.loads(msg)\n",
    "            if data.get(\"kind\") == \"AudioData\":\n",
    "                b64_audio = data[\"audioData\"][\"data\"]\n",
    "                pcm_bytes = b64decode(b64_audio)\n",
    "                all_pcm_bytes.append(pcm_bytes)\n",
    "                await openai_ws.send(\n",
    "                    json.dumps(\n",
    "                        {\"type\": \"input_audio_buffer.append\", \"audio\": b64_audio}\n",
    "                    )\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"ACS->OpenAI relay error: {e}\")\n",
    "            break\n",
    "\n",
    "\n",
    "async def openai_to_transcript_relay(openai_ws):\n",
    "    while True:\n",
    "        try:\n",
    "            msg = await openai_ws.recv()\n",
    "            data = json.loads(msg)\n",
    "            if data.get(\"type\") == \"conversation.item.input_audio_transcription.delta\":\n",
    "                delta = data.get(\"delta\", \"\")\n",
    "                if delta:\n",
    "                    print(\"Œî\", delta, end=\" \", flush=True)\n",
    "            if (\n",
    "                data.get(\"type\")\n",
    "                == \"conversation.item.input_audio_transcription.completed\"\n",
    "            ):\n",
    "                transcript = data.get(\"transcript\", \"\")\n",
    "                print(\"\\n‚úÖ Final transcript:\", transcript)\n",
    "        except Exception as e:\n",
    "            print(f\"OpenAI transcript relay error: {e}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57b55dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://xkrcv9t3-8010.use.devtunnels.ms'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0dc0c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "port = 8765\n",
    "ACS_WEBSOCKET_URL = f\"wss://xkrcv9t3-8010.use.devtunnels.ms:{port}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edde2e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your ACS WebSocket server is ws://YOUR_PUBLIC_HOSTNAME:8765/\n",
      "WebSocket server running. Ready for ACS audio!\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()  # So asyncio works in Jupyter\n",
    "\n",
    "port = 8765  # Pick an open port\n",
    "\n",
    "# Show the URL you need to put into your ACS streaming configuration\n",
    "print(f\"Your ACS WebSocket server is ws://YOUR_PUBLIC_HOSTNAME:{port}/\")\n",
    "\n",
    "start_server = websockets.serve(acs_ws_handler, \"0.0.0.0\", port)\n",
    "await start_server\n",
    "print(\"WebSocket server running. Ready for ACS audio!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb507a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wss://xkrcv9t3-8010.use.devtunnels.ms:8765/'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ACS_WEBSOCKET_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10c94372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.communication.callautomation import (\n",
    "    CallAutomationClient,\n",
    "    PhoneNumberIdentifier,\n",
    "    MediaStreamingOptions,\n",
    "    MediaStreamingTransportType,\n",
    "    MediaStreamingContentType,\n",
    "    MediaStreamingAudioChannelType,\n",
    "    AudioFormat,\n",
    ")\n",
    "\n",
    "\n",
    "def initiate_acs_call(target_number: str):\n",
    "    \"\"\"\n",
    "    Place a phone call to `target_number` via ACS,\n",
    "    streaming audio to the provided WebSocket URL.\n",
    "    \"\"\"\n",
    "    client = CallAutomationClient.from_connection_string(ACS_CONNECTION_STRING)\n",
    "    source = PhoneNumberIdentifier(ACS_SOURCE_PHONE_NUMBER)\n",
    "    target = PhoneNumberIdentifier(target_number)\n",
    "    media_streaming = MediaStreamingOptions(\n",
    "        transport_url=ACS_WEBSOCKET_URL,\n",
    "        transport_type=MediaStreamingTransportType.WEBSOCKET,\n",
    "        content_type=MediaStreamingContentType.AUDIO,\n",
    "        audio_channel_type=MediaStreamingAudioChannelType.UNMIXED,\n",
    "        start_media_streaming=True,\n",
    "        enable_bidirectional=False,\n",
    "        audio_format=AudioFormat.PCM16_K_MONO,\n",
    "    )\n",
    "    print(f\"Calling {target_number} from {ACS_SOURCE_PHONE_NUMBER}...\")\n",
    "    response = client.create_call(\n",
    "        target_participant=target,\n",
    "        callback_url=\"https://yourapp/callback\",  # Set to your callback if needed\n",
    "        media_streaming=media_streaming,\n",
    "        source_caller_id_number=source,\n",
    "    )\n",
    "    print(f\"Call initiated. Call Connection ID: {response.call_connection_id}\")\n",
    "    return response.call_connection_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "679479fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling +18165019907 from +18332397320...\n",
      "Call initiated. Call Connection ID: 27005880-557a-4133-858e-9713ca702a55\n",
      "Call is being placed. Connection ID: 27005880-557a-4133-858e-9713ca702a55\n"
     ]
    }
   ],
   "source": [
    "# Replace with your real phone number (E.164 format, e.g., +12225551234)\n",
    "target_phone = os.getenv(\"ACS_TARGET_PHONE_NUMBER\")\n",
    "call_id = initiate_acs_call(target_phone)\n",
    "print(f\"Call is being placed. Connection ID: {call_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54ba563d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ACS call audio to acs_recording.wav\n"
     ]
    }
   ],
   "source": [
    "output_filename = \"acs_recording.wav\"\n",
    "sample_rate = 16000\n",
    "channels = 1\n",
    "\n",
    "with wave.open(output_filename, \"wb\") as wf:\n",
    "    wf.setnchannels(channels)\n",
    "    wf.setsampwidth(2)\n",
    "    wf.setframerate(sample_rate)\n",
    "    wf.writeframes(b\"\".join(all_pcm_bytes))\n",
    "\n",
    "print(f\"Saved ACS call audio to {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1588a4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìû Call initiated! Call Connection ID: 23005880-323c-448b-b1dc-bdbf76720c6b\n"
     ]
    }
   ],
   "source": [
    "from azure.communication.callautomation import (\n",
    "    CallAutomationClient,\n",
    "    PhoneNumberIdentifier,\n",
    "    MediaStreamingOptions,\n",
    "    MediaStreamingTransportType,\n",
    "    MediaStreamingContentType,\n",
    "    MediaStreamingAudioChannelType,\n",
    "    AudioFormat,\n",
    ")\n",
    "\n",
    "acs_ws_path = \"realtime-acs\"\n",
    "acs_ws_url = f\"{BASE_URL.replace('https://', 'wss://').rstrip('/')}/{acs_ws_path}\"\n",
    "\n",
    "media_streaming_configuration = MediaStreamingOptions(\n",
    "    transport_url=acs_ws_url,\n",
    "    transport_type=MediaStreamingTransportType.WEBSOCKET,\n",
    "    content_type=MediaStreamingContentType.AUDIO,\n",
    "    audio_channel_type=MediaStreamingAudioChannelType.UNMIXED,\n",
    "    start_media_streaming=True,\n",
    "    enable_bidirectional=True,\n",
    "    audio_format=AudioFormat.PCM16_K_MONO,\n",
    ")\n",
    "\n",
    "client = CallAutomationClient.from_connection_string(ACS_CONNECTION_STRING)\n",
    "target = PhoneNumberIdentifier(ACS_TARGET_PHONE_NUMBER)\n",
    "source = PhoneNumberIdentifier(ACS_SOURCE_PHONE_NUMBER)\n",
    "\n",
    "response = client.create_call(\n",
    "    target_participant=target,\n",
    "    callback_url=f\"{BASE_URL}/api/acs/callback\",\n",
    "    media_streaming=media_streaming_configuration,\n",
    "    source_caller_id_number=source,\n",
    ")\n",
    "\n",
    "call_connection_id = response.call_connection_id\n",
    "print(\"üìû Call initiated! Call Connection ID:\", call_connection_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7831a1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_ws_url = f\"{AZURE_OPENAI_STT_TTS_ENDPOINT.replace('https', 'wss')}/openai/realtime?api-version=2025-04-01-preview&intent=transcription\"\n",
    "openai_headers = {\"api-key\": AZURE_OPENAI_STT_TTS_KEY}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2eacfcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_acs_to_openai_format(\n",
    "    msg_data: dict, model: str = \"gpt-4o-transcribe\"\n",
    ") -> dict:\n",
    "    if msg_data.get(\"kind\") == \"AudioMetadata\":\n",
    "        return {\n",
    "            \"type\": \"transcription_session.update\",\n",
    "            \"session\": {\n",
    "                \"input_audio_format\": \"pcm16\",\n",
    "                \"input_audio_transcription\": {\n",
    "                    \"model\": model,\n",
    "                    \"prompt\": \"Respond in English.\",\n",
    "                },\n",
    "                \"input_audio_noise_reduction\": {\"type\": \"near_field\"},\n",
    "                \"turn_detection\": {\n",
    "                    \"type\": \"server_vad\",\n",
    "                    \"threshold\": 0.7,\n",
    "                    \"prefix_padding_ms\": 300,\n",
    "                    \"silence_duration_ms\": 500,\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    elif msg_data.get(\"kind\") == \"AudioData\":\n",
    "        return {\n",
    "            \"type\": \"input_audio_buffer.append\",\n",
    "            \"audio\": msg_data[\"audioData\"][\"data\"],\n",
    "        }\n",
    "    return None\n",
    "\n",
    "\n",
    "def transform_openai_to_acs_format(msg_data: dict) -> dict:\n",
    "    if msg_data.get(\"type\") == \"conversation.item.input_audio_transcription.delta\":\n",
    "        return {\"kind\": \"TranscriptionDelta\", \"text\": msg_data.get(\"delta\")}\n",
    "    elif (\n",
    "        msg_data.get(\"type\") == \"conversation.item.input_audio_transcription.completed\"\n",
    "    ):\n",
    "        return {\"kind\": \"TranscriptionCompleted\", \"text\": msg_data.get(\"transcript\")}\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2644a460",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def relay_acs_to_openai(acs_ws, openai_ws, model: str):\n",
    "    while True:\n",
    "        try:\n",
    "            msg = await acs_ws.receive_text()\n",
    "            data = json.loads(msg)\n",
    "            mapped = transform_acs_to_openai_format(data, model=model)\n",
    "            if mapped:\n",
    "                await openai_ws.send(json.dumps(mapped))\n",
    "        except Exception as e:\n",
    "            print(\"ACS ‚Üí OpenAI relay error:\", e)\n",
    "            break\n",
    "\n",
    "\n",
    "async def relay_openai_to_acs(openai_ws, acs_ws):\n",
    "    while True:\n",
    "        try:\n",
    "            msg = await openai_ws.recv()\n",
    "            data = json.loads(msg)\n",
    "            mapped = transform_openai_to_acs_format(data)\n",
    "            if mapped:\n",
    "                await acs_ws.send_text(json.dumps(mapped))\n",
    "        except Exception as e:\n",
    "            print(\"OpenAI ‚Üí ACS relay error:\", e)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e24f837",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidStatus",
     "evalue": "server rejected WebSocket connection: HTTP 502",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidStatus\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m         t2 = asyncio.create_task(relay_openai_to_acs(openai_ws, acs_ws))\n\u001b[32m      7\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(t1, t2)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m run_relay(\n\u001b[32m     10\u001b[39m     acs_ws_url=acs_ws_url,\n\u001b[32m     11\u001b[39m     openai_ws_url=openai_ws_url,\n\u001b[32m     12\u001b[39m     openai_headers=openai_headers,\n\u001b[32m     13\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mgpt-4o-transcribe\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# or \"gpt-4o-mini-transcribe\"\u001b[39;00m\n\u001b[32m     14\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mrun_relay\u001b[39m\u001b[34m(acs_ws_url, openai_ws_url, openai_headers, model)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_relay\u001b[39m(acs_ws_url, openai_ws_url, openai_headers, model=\u001b[33m\"\u001b[39m\u001b[33mgpt-4o-transcribe\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m websockets.connect(acs_ws_url) \u001b[38;5;28;01mas\u001b[39;00m acs_ws, \\\n\u001b[32m      3\u001b[39m                websockets.connect(openai_ws_url, extra_headers=openai_headers) \u001b[38;5;28;01mas\u001b[39;00m openai_ws:\n\u001b[32m      4\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Both WebSocket connections established. Relaying audio in real-time...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m         t1 = asyncio.create_task(relay_acs_to_openai(acs_ws, openai_ws, model))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\audioagent\\Lib\\site-packages\\websockets\\asyncio\\client.py:487\u001b[39m, in \u001b[36mconnect.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    486\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> ClientConnection:\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\audioagent\\Lib\\site-packages\\websockets\\asyncio\\client.py:446\u001b[39m, in \u001b[36mconnect.__await_impl__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    444\u001b[39m \u001b[38;5;28mself\u001b[39m.connection = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.create_connection()\n\u001b[32m    445\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m446\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.connection.handshake(*\u001b[38;5;28mself\u001b[39m.handshake_args)\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m asyncio.CancelledError:\n\u001b[32m    448\u001b[39m     \u001b[38;5;28mself\u001b[39m.connection.transport.abort()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\audioagent\\Lib\\site-packages\\websockets\\asyncio\\client.py:104\u001b[39m, in \u001b[36mClientConnection.handshake\u001b[39m\u001b[34m(self, additional_headers, user_agent_header)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# self.protocol.handshake_exc is set when the connection is lost before\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# receiving a response, when the response cannot be parsed, or when the\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# response fails the handshake.\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.protocol.handshake_exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m.protocol.handshake_exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\audioagent\\Lib\\site-packages\\websockets\\client.py:340\u001b[39m, in \u001b[36mClientProtocol.parse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    337\u001b[39m         \u001b[38;5;28mself\u001b[39m.logger.debug(\u001b[33m\"\u001b[39m\u001b[33m< [body] (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m bytes)\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(response.body))\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocess_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidHandshake \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    342\u001b[39m     response._exception = exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\audioagent\\Lib\\site-packages\\websockets\\client.py:151\u001b[39m, in \u001b[36mClientProtocol.process_response\u001b[39m\u001b[34m(self, response)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    140\u001b[39m \u001b[33;03mCheck a handshake response.\u001b[39;00m\n\u001b[32m    141\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    147\u001b[39m \n\u001b[32m    148\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code != \u001b[32m101\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidStatus(response)\n\u001b[32m    153\u001b[39m headers = response.headers\n\u001b[32m    155\u001b[39m connection: \u001b[38;5;28mlist\u001b[39m[ConnectionOption] = \u001b[38;5;28msum\u001b[39m(\n\u001b[32m    156\u001b[39m     [parse_connection(value) \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m headers.get_all(\u001b[33m\"\u001b[39m\u001b[33mConnection\u001b[39m\u001b[33m\"\u001b[39m)], []\n\u001b[32m    157\u001b[39m )\n",
      "\u001b[31mInvalidStatus\u001b[39m: server rejected WebSocket connection: HTTP 502"
     ]
    }
   ],
   "source": [
    "async def run_relay(\n",
    "    acs_ws_url, openai_ws_url, openai_headers, model=\"gpt-4o-transcribe\"\n",
    "):\n",
    "    async with websockets.connect(acs_ws_url) as acs_ws, websockets.connect(\n",
    "        openai_ws_url, extra_headers=openai_headers\n",
    "    ) as openai_ws:\n",
    "        print(\n",
    "            \"‚úÖ Both WebSocket connections established. Relaying audio in real-time...\"\n",
    "        )\n",
    "        t1 = asyncio.create_task(relay_acs_to_openai(acs_ws, openai_ws, model))\n",
    "        t2 = asyncio.create_task(relay_openai_to_acs(openai_ws, acs_ws))\n",
    "        await asyncio.gather(t1, t2)\n",
    "\n",
    "\n",
    "await run_relay(\n",
    "    acs_ws_url=acs_ws_url,\n",
    "    openai_ws_url=openai_ws_url,\n",
    "    openai_headers=openai_headers,\n",
    "    model=\"gpt-4o-transcribe\",  # or \"gpt-4o-mini-transcribe\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448c69c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install azure-cognitiveservices-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883b9830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "\n",
    "def synthesize_text_to_pcm_bytes(\n",
    "    text: str,\n",
    "    subscription_key: str,\n",
    "    region: str,\n",
    "    voice: str = \"en-US-JennyNeural\",\n",
    "    sample_rate: int = 16000,\n",
    ") -> bytes:\n",
    "    \"\"\"Synthesize text to PCM bytes using Azure Speech SDK.\"\"\"\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=subscription_key, region=region)\n",
    "    speech_config.speech_synthesis_voice_name = voice\n",
    "    # PCM, 16-bit, 16 kHz, mono\n",
    "    speech_config.set_speech_synthesis_output_format(\n",
    "        speechsdk.SpeechSynthesisOutputFormat.Raw16Khz16BitMonoPcm\n",
    "    )\n",
    "\n",
    "    audio_stream = speechsdk.audio.PushAudioOutputStream()\n",
    "    audio_config = speechsdk.audio.AudioOutputConfig(stream=audio_stream)\n",
    "    synthesizer = speechsdk.SpeechSynthesizer(speech_config, audio_config)\n",
    "\n",
    "    # Synthesize, push PCM to the stream, then read it all back\n",
    "    result = synthesizer.speak_text_async(text).get()\n",
    "    if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "        pcm_bytes = result.audio_data\n",
    "        return pcm_bytes\n",
    "    else:\n",
    "        raise RuntimeError(f\"TTS failed: {result.reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf0d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "TTS_SUBSCRIPTION_KEY = (\n",
    "    AZURE_OPENAI_STT_TTS_KEY  # Usually the same as your OpenAI key for Speech resource\n",
    ")\n",
    "TTS_REGION = os.getenv(\"AZURE_OPENAI_REGION\") or \"eastus\"  # Change as appropriate\n",
    "\n",
    "\n",
    "async def relay_openai_to_acs_with_tts(openai_ws, acs_ws):\n",
    "    while True:\n",
    "        try:\n",
    "            msg = await openai_ws.recv()\n",
    "            data = json.loads(msg)\n",
    "            mapped = transform_openai_to_acs_format(data)\n",
    "            if mapped:\n",
    "                # Send transcript as text to ACS (could trigger TTS client-side)\n",
    "                await acs_ws.send_text(json.dumps(mapped))\n",
    "\n",
    "                # If it's a final transcript, synthesize and send audio\n",
    "                if mapped[\"kind\"] == \"TranscriptionCompleted\" and mapped.get(\"text\"):\n",
    "                    text = mapped[\"text\"]\n",
    "                    print(f\"üîä TTS Synthesizing: {text}\")\n",
    "                    pcm_bytes = synthesize_text_to_pcm_bytes(\n",
    "                        text=text,\n",
    "                        subscription_key=TTS_SUBSCRIPTION_KEY,\n",
    "                        region=TTS_REGION,\n",
    "                        voice=\"en-US-JennyNeural\",  # or your preferred voice\n",
    "                        sample_rate=16000,\n",
    "                    )\n",
    "                    # Split PCM into frames (20ms @ 16kHz mono = 640 bytes)\n",
    "                    for i in range(0, len(pcm_bytes), 640):\n",
    "                        frame = pcm_bytes[i : i + 640]\n",
    "                        # pad last frame if needed\n",
    "                        if len(frame) < 640:\n",
    "                            frame += b\"\\x00\" * (640 - len(frame))\n",
    "                        b64_audio = b64encode(frame).decode(\"ascii\")\n",
    "                        audio_payload = {\n",
    "                            \"kind\": \"AudioData\",\n",
    "                            \"audioData\": {\"data\": b64_audio},\n",
    "                            \"stopAudio\": None,\n",
    "                        }\n",
    "                        await acs_ws.send_text(json.dumps(audio_payload))\n",
    "                        await asyncio.sleep(0.02)  # Simulate real-time audio playback\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"OpenAI ‚Üí ACS relay error:\", e)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d7ed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_relay_with_tts(\n",
    "    acs_ws_url, openai_ws_url, openai_headers, model=\"gpt-4o-transcribe\"\n",
    "):\n",
    "    async with websockets.connect(acs_ws_url) as acs_ws, websockets.connect(\n",
    "        openai_ws_url, extra_headers=openai_headers\n",
    "    ) as openai_ws:\n",
    "        print(\n",
    "            \"‚úÖ Both WebSocket connections established. Relaying audio & TTS in real-time...\"\n",
    "        )\n",
    "        t1 = asyncio.create_task(relay_acs_to_openai(acs_ws, openai_ws, model))\n",
    "        t2 = asyncio.create_task(relay_openai_to_acs_with_tts(openai_ws, acs_ws))\n",
    "        await asyncio.gather(t1, t2)\n",
    "\n",
    "\n",
    "# And launch:\n",
    "await run_relay_with_tts(\n",
    "    acs_ws_url=acs_ws_url,\n",
    "    openai_ws_url=openai_ws_url,\n",
    "    openai_headers=openai_headers,\n",
    "    model=\"gpt-4o-transcribe\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audioagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
