{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc6b4e04",
   "metadata": {},
   "source": [
    "## **Building Your Voice-to-Voice Agent with Azure AI Speech and AOAI**\n",
    "\n",
    "This notebook provides a step-by-step guide to create a voice-to-voice agent using Azure AI Speech services and Azure OpenAI. It walks you through the process of configuring speech recognition, integrating external tools, and generating human-like responses for real-time interactions.\n",
    "\n",
    "![alt text](../utils/images/lab1.png) \n",
    "\n",
    "\n",
    "1. **Audio Ingestion**: Ensure the capability to record audio is set up.  \n",
    "2. **Azure Speech-to-Text (STT)**: Converts live audio into transcribed text for LLM processing.  \n",
    "3. **Azure OpenAI with Function Calling & Streaming**: Understands patient intent, routes queries, and dynamically calls backend tools in real time.  \n",
    "4. **Azure Text-to-Speech (TTS)**: Delivers natural, empathetic voice responses back to the user in chunks.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed102ccb",
   "metadata": {},
   "source": [
    "## **Audio Ingestion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available audio devices:\n",
      "0: Microsoft Sound Mapper - Input\n",
      "1: Surface Stereo Microphones (Sur\n",
      "2: Microsoft Sound Mapper - Output\n",
      "3: Surface Omnisonic Speakers (Sur\n",
      "4: Speakers (Dell USB Audio)\n",
      "5: Primary Sound Capture Driver\n",
      "6: Surface Stereo Microphones (Surface High Definition Audio)\n",
      "7: Primary Sound Driver\n",
      "8: Surface Omnisonic Speakers (Surface High Definition Audio)\n",
      "9: Speakers (Dell USB Audio)\n",
      "10: Speakers (Dell USB Audio)\n",
      "11: Surface Omnisonic Speakers (Surface High Definition Audio)\n",
      "12: Surface Stereo Microphones (Surface High Definition Audio)\n",
      "13: Headphones ()\n",
      "14: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Shiva’s AirPods Pro #2))\n",
      "15: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Shiva’s AirPods Pro #2))\n",
      "16: Speakers (Dell USB Audio)\n",
      "17: Microphone (Dell USB Audio)\n",
      "18: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Shiva’s AirPods Pro #2 - Find My))\n",
      "19: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Shiva’s AirPods Pro #2 - Find My))\n",
      "20: Headphones 1 (Realtek HD Audio 2nd output with SST)\n",
      "21: Headphones 2 (Realtek HD Audio 2nd output with SST)\n",
      "22: PC Speaker (Realtek HD Audio 2nd output with SST)\n",
      "23: Speakers 1 (Realtek HD Audio output with SST)\n",
      "24: Speakers 2 (Realtek HD Audio output with SST)\n",
      "25: PC Speaker (Realtek HD Audio output with SST)\n",
      "26: Microphone Array (Realtek HD Audio Mic input)\n",
      "27: Headset Microphone (Headset Microphone)\n",
      "28: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Pablo’s AirPods #3))\n",
      "29: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Pablo’s AirPods #3))\n",
      "30: Input ()\n",
      "31: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Pablo’s AirPods #4))\n",
      "32: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Pablo’s AirPods #4))\n",
      "33: Headphones ()\n",
      "34: Output (@System32\\drivers\\bthhfenum.sys,#4;%1 Hands-Free HF Audio%0\n",
      ";(iPhone de Pablo))\n",
      "35: Input (@System32\\drivers\\bthhfenum.sys,#4;%1 Hands-Free HF Audio%0\n",
      ";(iPhone de Pablo))\n",
      "36: Headphones ()\n",
      "37: Headphones ()\n",
      "38: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Pablo’s AirPods Pro - Find My))\n",
      "39: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Pablo’s AirPods Pro - Find My))\n",
      "40: Headphones ()\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "\n",
    "\n",
    "def list_audio_devices():\n",
    "    \"\"\"\n",
    "    List all available audio devices using PyAudio.\n",
    "\n",
    "    This function initializes PyAudio, retrieves the list of audio devices,\n",
    "    and prints their names. It also includes error handling to ensure proper\n",
    "    cleanup of resources.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        p = pyaudio.PyAudio()\n",
    "        print(\"Available audio devices:\")\n",
    "        for ii in range(p.get_device_count()):\n",
    "            device_name = p.get_device_info_by_index(ii).get(\"name\")\n",
    "            print(f\"{ii}: {device_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while listing audio devices: {e}\")\n",
    "    finally:\n",
    "        # Ensure PyAudio resources are released\n",
    "        if \"p\" in locals():\n",
    "            p.terminate()\n",
    "\n",
    "\n",
    "# Call the function to list audio devices\n",
    "list_audio_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e9ade96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording complete. Saving audio...\n",
      "Audio saved to test_audio.wav. Playing back...\n",
      "Playback complete.\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "def test_microphone():\n",
    "    \"\"\"\n",
    "    Test the microphone by recording audio and playing it back.\n",
    "\n",
    "    This function captures audio from the default input device (microphone),\n",
    "    saves it to a temporary WAV file, and plays it back to ensure the microphone\n",
    "    is working correctly.\n",
    "    \"\"\"\n",
    "    # Audio configuration\n",
    "    chunk = 1024  # Number of frames per buffer\n",
    "    format = pyaudio.paInt16  # 16-bit audio format\n",
    "    channels = 1  # Mono audio\n",
    "    rate = 44100  # Sampling rate (44.1 kHz)\n",
    "    record_seconds = 5  # Duration of the recording\n",
    "    output_filename = \"test_audio.wav\"\n",
    "\n",
    "    # Initialize PyAudio\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    try:\n",
    "        # Open the microphone stream\n",
    "        print(\"Recording...\")\n",
    "        stream = p.open(\n",
    "            format=format,\n",
    "            channels=channels,\n",
    "            rate=rate,\n",
    "            input=True,\n",
    "            frames_per_buffer=chunk,\n",
    "        )\n",
    "\n",
    "        frames = []\n",
    "\n",
    "        # Record audio in chunks\n",
    "        for _ in range(0, int(rate / chunk * record_seconds)):\n",
    "            data = stream.read(chunk)\n",
    "            frames.append(data)\n",
    "\n",
    "        print(\"Recording complete. Saving audio...\")\n",
    "\n",
    "        # Save the recorded audio to a WAV file\n",
    "        with wave.open(output_filename, \"wb\") as wf:\n",
    "            wf.setnchannels(channels)\n",
    "            wf.setsampwidth(p.get_sample_size(format))\n",
    "            wf.setframerate(rate)\n",
    "            wf.writeframes(b\"\".join(frames))\n",
    "\n",
    "        print(f\"Audio saved to {output_filename}. Playing back...\")\n",
    "\n",
    "        # Play back the recorded audio\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "\n",
    "        # Open the WAV file for playback\n",
    "        wf = wave.open(output_filename, \"rb\")\n",
    "        playback_stream = p.open(\n",
    "            format=p.get_format_from_width(wf.getsampwidth()),\n",
    "            channels=wf.getnchannels(),\n",
    "            rate=wf.getframerate(),\n",
    "            output=True,\n",
    "        )\n",
    "\n",
    "        # Read and play audio data\n",
    "        data = wf.readframes(chunk)\n",
    "        while data:\n",
    "            playback_stream.write(data)\n",
    "            data = wf.readframes(chunk)\n",
    "\n",
    "        playback_stream.stop_stream()\n",
    "        playback_stream.close()\n",
    "\n",
    "        print(\"Playback complete.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Terminate PyAudio\n",
    "        p.terminate()\n",
    "\n",
    "\n",
    "# Run the microphone test\n",
    "test_microphone()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e9da16",
   "metadata": {},
   "source": [
    "## **Define Clients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3f52ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'audioagent (Python 3.11.11)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pprint\n",
    "\n",
    "# Define the target directory\n",
    "target_directory = (\n",
    "    r\"C:\\Users\\pablosal\\Desktop\\gbb-ai-audio-agent\"  # change your directory here\n",
    ")\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(target_directory):\n",
    "    # Change the current working directory\n",
    "    os.chdir(target_directory)\n",
    "    print(f\"Directory changed to {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"Directory {target_directory} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a11c15",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'audioagent (Python 3.11.11)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from src.speech.speech_recognizer import SpeechRecognizer, StreamingSpeechRecognizer\n",
    "from src.speech.text_to_speech import SpeechSynthesizer\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Ensure clients are initialized only if not already defined\n",
    "if \"az_speech_recognizer_client\" not in locals():\n",
    "    az_speech_recognizer_client = SpeechRecognizer()\n",
    "\n",
    "if \"az_speech_recognizer_stream_client\" not in locals():\n",
    "    az_speech_recognizer_stream_client = StreamingSpeechRecognizer(\n",
    "        vad_silence_timeout_ms=5000\n",
    "    )\n",
    "\n",
    "if \"az_speach_synthesizer_client\" not in locals():\n",
    "    az_speach_synthesizer_client = SpeechSynthesizer()\n",
    "\n",
    "# Ensure Azure OpenAI client is initialized only if not already defined\n",
    "if \"client\" not in locals():\n",
    "    client = AzureOpenAI(\n",
    "        api_version=\"2025-02-01-preview\",\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4ee563",
   "metadata": {},
   "source": [
    "## **Azure Speech-to-Text (STT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96a82c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 20:27:10,910 - micro - MainProcess - INFO     Starting continuous recognition … (speech_recognizer.py:start:134)\n",
      "2025-06-16 20:27:10,948 - micro - MainProcess - INFO     Recognition started with languages=['en-US', 'es-ES', 'fr-FR'] (fallback=en-US) (speech_recognizer.py:start:171)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial (en-US): can you please\n",
      "Partial (en-US): can you please describe\n",
      "Partial (en-US): can you please describe madrid\n",
      "Partial (en-US): can you please describe madrid in\n",
      "Partial (en-US): can you please describe madrid in 100 words\n",
      "Partial (en-US): can you please describe madrid in 100 words you need to\n",
      "Partial (en-US): can you please describe madrid in 100 words you need to start\n",
      "Partial (en-US): can you please describe madrid in 100 words you need to start first\n",
      "Partial (en-US): can you please describe madrid in 100 words you need to start first in\n",
      "Partial (en-US): can you please describe madrid in 100 words you need to start first in spanish\n",
      "Partial (en-US): can you please describe madrid in 100 words you need to start first in spanish then\n",
      "Partial (en-US): can you please describe madrid in 100 words you need to start first in spanish then switch to\n",
      "Partial (en-US): can you please describe madrid in 100 words you need to start first in spanish then switch to italian\n",
      "Partial (en-US): can you please describe madrid in 100 words you need to start first in spanish then switch to italian then\n",
      "Partial (en-US): can you please describe madrid in 100 words you need to start first in spanish then switch to italian then to\n",
      "Partial (en-US): can you please describe madrid in 100 words you need to start first in spanish then switch to italian then to english\n",
      "Partial (en-US): can you please describe madrid in 100 words you need to start first in spanish then switch to italian then to english and then\n",
      "Partial (en-US): can you please describe madrid in 100 words you need to start first in spanish then switch to italian then to english and then finish with\n",
      "Partial (en-US): can you please describe madrid in 100 words you need to start first in spanish then switch to italian then to english and then finish with french\n",
      "Final (en-US): Can you please describe Madrid in 100 words? You need to start first in Spanish, then switch to Italian, then to English and then finish with French.\n"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "recognizer = StreamingSpeechRecognizer(vad_silence_timeout_ms=3000)\n",
    "\n",
    "# Buffers\n",
    "all_text_live = \"\"\n",
    "final_transcripts = []\n",
    "\n",
    "# Internal tracker to prevent double-processing\n",
    "last_final_text = None\n",
    "\n",
    "\n",
    "def on_partial(text: str, lang: str):\n",
    "    global all_text_live\n",
    "    print(f\"Partial ({lang}): {text}\")\n",
    "    all_text_live = text\n",
    "\n",
    "def on_final(text: str, lang: str):\n",
    "    global all_text_live, last_final_text\n",
    "    if text == last_final_text:\n",
    "        return\n",
    "    last_final_text = text\n",
    "\n",
    "    print(f\"Final ({lang}): {text}\")\n",
    "    final_transcripts.append(text)\n",
    "    all_text_live = \"\"\n",
    "\n",
    "# Attach\n",
    "recognizer.set_partial_result_callback(on_partial)\n",
    "recognizer.set_final_result_callback(on_final)\n",
    "\n",
    "# Start recognizing\n",
    "recognizer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04ebc8a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Can you please describe Madrid in 100 words? You need to start first in Spanish, then switch to Italian, then to English and then finish with French. '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_conversation = \" \".join(final_transcripts) + \" \" + all_text_live\n",
    "full_conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "698b9a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 20:27:30,654 - micro - MainProcess - INFO     Stopping recognition … (speech_recognizer.py:stop:180)\n",
      "2025-06-16 20:27:30,736 - micro - MainProcess - INFO     Session stopped: SessionEventArgs(session_id=3ba6dd0a566543a69adeae71014e6941) (speech_recognizer.py:_on_session_stopped:222)\n",
      "2025-06-16 20:27:30,739 - micro - MainProcess - INFO     Recognition stopped. (speech_recognizer.py:stop:182)\n"
     ]
    }
   ],
   "source": [
    "# Start recognizing\n",
    "recognizer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96eabf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 20:26:02,124 - micro - MainProcess - INFO     Starting continuous recognition … (speech_recognizer.py:start:134)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting microphone recognition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 20:26:02,511 - micro - MainProcess - INFO     Recognition started with languages=['en-US', 'es-ES', 'fr-FR'] (fallback=en-US) (speech_recognizer.py:start:171)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎤 Listening... (speak now)\n",
      "Partial (en-US): can you please describe\n",
      "Partial (en-US): can you please describe madrid\n",
      "Partial (en-US): can you please describe madrid in\n",
      "Partial (en-US): can you please describe madrid in 100\n",
      "Partial (en-US): can you please describe madrid in 100 words\n",
      "Partial (en-US): can you please describe madrid in 100 words in\n",
      "Partial (en-US): can you please describe madrid in 100 words in half\n",
      "Partial (en-US): can you please describe madrid in 100 words in half english\n",
      "Partial (en-US): can you please describe madrid in 100 words in half english half\n",
      "Partial (en-US): can you please describe madrid in 100 words in half english half spanish\n",
      "Partial (en-US): can you please describe madrid in 100 words in half english half spanish half\n",
      "Partial (en-US): can you please describe madrid in 100 words in half english half spanish half italian\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 20:26:15,375 - micro - MainProcess - INFO     Stopping recognition … (speech_recognizer.py:stop:180)\n",
      "2025-06-16 20:26:15,450 - micro - MainProcess - INFO     Session stopped: SessionEventArgs(session_id=2822193b3f5644c796f485ae5953742d) (speech_recognizer.py:_on_session_stopped:222)\n",
      "2025-06-16 20:26:15,503 - micro - MainProcess - INFO     Recognition stopped. (speech_recognizer.py:stop:182)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final (en-US): Can you please describe Madrid in 100 words in half English, half Spanish, half Italian?\n",
      "🛑 Recognition stopped.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "SILENCE_THRESHOLD = 20  # seconds of silence to stop recognition\n",
    "\n",
    "def handle_speech_recognition() -> str:\n",
    "    global all_text_live, last_final_text\n",
    "\n",
    "    final_transcripts = []\n",
    "\n",
    "    print(\"Starting microphone recognition...\")\n",
    "    final_transcripts.clear()\n",
    "    all_text_live = \"\"\n",
    "    last_final_text = None\n",
    "\n",
    "    def on_partial(text: str, lang: str):\n",
    "        global all_text_live\n",
    "        print(f\"Partial ({lang}): {text}\")\n",
    "        all_text_live = text\n",
    "\n",
    "    def on_final(text: str, lang: str):\n",
    "        global all_text_live, last_final_text\n",
    "        if text == last_final_text:\n",
    "            return\n",
    "        last_final_text = text\n",
    "\n",
    "        print(f\"Final ({lang}): {text}\")\n",
    "        final_transcripts.append(text)\n",
    "        all_text_live = \"\"\n",
    "\n",
    "    recognizer.set_partial_result_callback(on_partial)\n",
    "    recognizer.set_final_result_callback(on_final)\n",
    "\n",
    "    recognizer.start()\n",
    "    print(\"🎤 Listening... (speak now)\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    while not final_transcripts and (time.time() - start_time < SILENCE_THRESHOLD):\n",
    "        time.sleep(0.05)\n",
    "\n",
    "    recognizer.stop()\n",
    "    print(\"🛑 Recognition stopped.\")\n",
    "\n",
    "    return \" \".join(final_transcripts) + \" \" + all_text_live\n",
    "\n",
    "\n",
    "recognition = handle_speech_recognition()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21400ef9",
   "metadata": {},
   "source": [
    "## **Azure OpenAI Streaming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d92bad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claro, aquí tienes una definición del concepto de inteligencia artificial en coreano:\n",
      "\n",
      "인공지능(AI)은 컴퓨터 시스템이 인간의 지능을 모방하여 작업을 수행하는 기술입니다. 이는 학습, 추론, 문제 해결, 이해 및 자연어 처리와 같은 능력을 포함합니다. 인공지능은 머신러닝, 딥러닝과 같은 알고리즘을 사용하여 데이터를 분석하고 패턴을 인식합니다. 이를 통해 자율적 의사결정이 가능해져 다양한 분야에서 혁신을 이루고 있습니다. 의료, 금융, 자동차, 로봇 공학 등의 산업 분야에서 인공지능은 효율성을 높이고 새로운 가능성을 제공합니다. 인공지능은 인간의 삶을 변화시킬 수 있는 잠재력을 지니고 있습니다."
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    stream=True,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"puedes definir el concepto de inteligencia artificial en korean en 100 palabras?\",\n",
    "        },\n",
    "    ],\n",
    "    max_tokens=4096,\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    model=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    ")\n",
    "full_response = \"\"\n",
    "for update in response:\n",
    "    if update.choices:\n",
    "        chunk = update.choices[0].delta.content or \"\"\n",
    "        full_response += chunk\n",
    "        print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1486daef",
   "metadata": {},
   "source": [
    "## **Azure Text-to-Speech (TTS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82328088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "az_speach_synthesizer_client.start_speaking_text(\n",
    "    text=full_response,\n",
    ")\n",
    "time.sleep(2)\n",
    "az_speach_synthesizer_client.stop_speaking()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894c6a37",
   "metadata": {},
   "source": [
    "## **Streaming Audio Back to the User**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d764bd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "tts_sentence_end = [\".\", \"!\", \"?\", \";\", \"。\", \"！\", \"？\", \"；\", \"\\n\"]\n",
    "completion = client.chat.completions.create(\n",
    "    stream=True,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"{full_conversation}\",\n",
    "        },\n",
    "    ],\n",
    "    max_tokens=4096,\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    model=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    ")\n",
    "\n",
    "collected_messages = []\n",
    "last_tts_request = None\n",
    "\n",
    "for chunk in completion:\n",
    "    if len(chunk.choices) > 0:\n",
    "        chunk_text = chunk.choices[0].delta.content\n",
    "        if chunk_text:\n",
    "            collected_messages.append(chunk_text)\n",
    "            if chunk_text in tts_sentence_end:\n",
    "                text = \"\".join(collected_messages).strip()\n",
    "                last_tts_request = az_speach_synthesizer_client.start_speaking_text(\n",
    "                    text\n",
    "                )\n",
    "                collected_messages.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fffdc7b",
   "metadata": {},
   "source": [
    "## **Adding Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a19e4ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Templates found: ['voice_agent_system.jinja', 'voice_agent_user.jinja']\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "from app.backend.prompt_manager import PromptManager\n",
    "\n",
    "prompt_manager = PromptManager()\n",
    "systemp_prompt = prompt_manager.get_prompt(\"voice_agent_system.jinja\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0c217c",
   "metadata": {},
   "source": [
    "## **Adding Tools**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e66946b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.backend.tools import available_tools\n",
    "from app.backend.functions import (\n",
    "    schedule_appointment,\n",
    "    refill_prescription,\n",
    "    lookup_medication_info,\n",
    "    evaluate_prior_authorization,\n",
    "    escalate_emergency,\n",
    "    authenticate_user,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0da14961",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history: List[Dict[str, str]] = [\n",
    "    {\"role\": \"system\", \"content\": systemp_prompt},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello, my name is Pablo Salvador. \"\n",
    "        \"this is an emergency. I need to schedule an appointment with my doctor ASAP.\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f201ef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping tool names to actual Python async functions\n",
    "function_mapping = {\n",
    "    \"schedule_appointment\": schedule_appointment,\n",
    "    \"refill_prescription\": refill_prescription,\n",
    "    \"lookup_medication_info\": lookup_medication_info,\n",
    "    \"evaluate_prior_authorization\": evaluate_prior_authorization,\n",
    "    \"escalate_emergency\": escalate_emergency,\n",
    "    \"authenticate_user\": authenticate_user,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2716c04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's response:\n",
      "ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_mIxsi3OwoVbUtqSgjTxDFtTK', function=Function(arguments='{\"reason\":\"Need to schedule an appointment urgently.\"}', name='escalate_emergency'), type='function')])\n",
      "Function result: 🚨 Emergency escalation triggered: Need to schedule an appointment urgently.. A human healthcare agent is now being connected.\n",
      "Tool Name: escalate_emergency\n",
      "Final response:\n",
      "I’ve escalated your request to a human healthcare agent who will assist you with scheduling your appointment immediately. Please hold on for a moment.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=conversation_history,\n",
    "    tools=available_tools,\n",
    "    tool_choice=\"auto\",\n",
    "    max_tokens=4096,\n",
    "    temperature=0.5,\n",
    "    top_p=1.0,\n",
    "    model=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    ")\n",
    "\n",
    "# Process the model's response\n",
    "response_message = response.choices[0].message\n",
    "conversation_history.append(response_message)\n",
    "\n",
    "print(\"Model's response:\")\n",
    "print(response_message)\n",
    "\n",
    "if response_message.tool_calls:\n",
    "    for tool_call in response_message.tool_calls:\n",
    "        function_name = tool_call.function.name\n",
    "        function_args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "        # Check if the function name is in the mapping\n",
    "        if function_name in function_mapping:\n",
    "            # Call the corresponding Python async function\n",
    "            result = await function_mapping[function_name](**function_args)\n",
    "            print(f\"Function result: {result}\")\n",
    "            print(f\"Tool Name: {function_name}\")\n",
    "            conversation_history.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": result,\n",
    "                }\n",
    "            )\n",
    "else:\n",
    "    print(\"No tool calls were made by the model.\")\n",
    "\n",
    "# Second API call: Get the final response from the model\n",
    "final_response = client.chat.completions.create(\n",
    "    messages=conversation_history,\n",
    "    max_tokens=4096,\n",
    "    temperature=0.5,\n",
    "    top_p=1.0,\n",
    "    model=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    ")\n",
    "\n",
    "print(\"Final response:\")\n",
    "print(final_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70a5c34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c4fbffb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SpeechSynthesisOutputFormat.Riff24Khz16BitMonoPcm: 13>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speechsdk.SpeechSynthesisOutputFormat.Riff24Khz16BitMonoPcm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734714e0",
   "metadata": {},
   "source": [
    "## **Making Tools work in Streaming Fashion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfe00db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history: List[Dict[str, str]] = [\n",
    "    {\"role\": \"system\", \"content\": systemp_prompt},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello, my name is Pablo Salvador. \"\n",
    "        \"this is an emergency. I need to schedule an appointment with my doctor ASAP.\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f55da9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=0, id='call_VWhpt20nqoslFiup9FVvb9Dd', function=ChoiceDeltaToolCallFunction(arguments='', name='escalate_emergency'), type='function')])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='{\"', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='reason', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\":\"', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='Need', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' to', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' schedule', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' an', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' emergency', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' appointment', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' with', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' doctor', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' ASAP', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='.\"', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='}', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=None)\n",
      "tool_name:escalate_emergency\n",
      "tool_id:call_VWhpt20nqoslFiup9FVvb9Dd\n",
      "tool_call_accumulator:{\"reason\":\"Need to schedule an emergency appointment with doctor ASAP.\"}\n",
      "Function result: 🚨 Emergency escalation triggered: Request for immediate appointment scheduling due to emergency.. A human healthcare agent is now being connected.\n",
      "=============================================\n",
      "I've triggered an emergency escalation for your request, and a human healthcare agent is now being connected to assist you. Please hold on for a moment while they join the conversation. Your safety is our priority."
     ]
    }
   ],
   "source": [
    "# initial user message\n",
    "tool_call_accumulator = \"\"\n",
    "\n",
    "# First API call: Ask the model to use the function\n",
    "response = client.chat.completions.create(\n",
    "    model=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    "    messages=conversation_history,\n",
    "    tools=available_tools,\n",
    "    tool_choice=\"auto\",\n",
    "    stream=True,  # this time, we set stream=True\n",
    ")\n",
    "\n",
    "# process the model\n",
    "for chunk in response:\n",
    "    if len(chunk.choices) > 0:\n",
    "        delta = chunk.choices[0].delta\n",
    "        print(f\"delta: {delta}\")  # print delta from the chunk for learning\n",
    "\n",
    "        if delta.tool_calls:\n",
    "            if delta.tool_calls[0].function.name:\n",
    "                tool_name = chunk.choices[0].delta.tool_calls[0].function.name\n",
    "                tool_id = chunk.choices[0].delta.tool_calls[0].id\n",
    "                conversation_history.append(delta)\n",
    "\n",
    "            if chunk.choices[0].delta.tool_calls[0].function.arguments:\n",
    "                tool_call_accumulator += delta.tool_calls[0].function.arguments\n",
    "\n",
    "# print function related outputs\n",
    "print(f\"tool_name:{tool_name}\")\n",
    "print(f\"tool_id:{tool_id}\")\n",
    "print(f\"tool_call_accumulator:{tool_call_accumulator}\")\n",
    "# Check if the function name is in the mapping\n",
    "if tool_name in function_mapping:\n",
    "    # Call the corresponding Python async function\n",
    "    result = await function_mapping[function_name](**function_args)\n",
    "    print(f\"Function result: {result}\")\n",
    "    conversation_history.append(\n",
    "        {\n",
    "            \"tool_call_id\": tool_id,\n",
    "            \"role\": \"tool\",\n",
    "            \"name\": tool_name,\n",
    "            \"content\": result,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# print(f\"messages: {messages}\")\n",
    "print(\"=============================================\")\n",
    "\n",
    "#  second API call: Get the final response with stream\n",
    "response = client.chat.completions.create(\n",
    "    model=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    "    messages=conversation_history,\n",
    "    temperature=0.7,\n",
    "    stream=True,  # this time, we set stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    if len(chunk.choices) > 0:\n",
    "        delta = chunk.choices[0].delta\n",
    "        # print(f\"delta: {delta}\")\n",
    "        if delta.content:\n",
    "            print(delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d636a8d5",
   "metadata": {},
   "source": [
    "## **Run the Conversation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0de55282",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history: List[Dict[str, str]] = [\n",
    "    {\"role\": \"system\", \"content\": systemp_prompt},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello, my name is Pablo Salvador. \"\n",
    "        \"this is an emergency. I need to schedule an appointment with my doctor ASAP.\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f5c9d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history: List[Dict[str, str]] = [\n",
    "    {\"role\": \"system\", \"content\": systemp_prompt},\n",
    "    {\"role\": \"user\", \"content\": \"Hello, my name is Pablo Salvador. \"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea4f10c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "tts_sentence_end = [\".\", \"!\", \"?\", \";\", \"。\", \"！\", \"？\", \"；\", \"\\n\"]\n",
    "\n",
    "\n",
    "async def handle_chat(conversation_history: List[Dict[str, Any]]) -> None:\n",
    "    \"\"\"\n",
    "    Handles a full streaming chat round with Azure OpenAI GPT-4o,\n",
    "    correctly executes tool calls, synthesizes responses, and continues reasoning.\n",
    "    \"\"\"\n",
    "    tool_name = None\n",
    "    function_call_arguments = \"\"\n",
    "    tool_call_id = None\n",
    "    last_tts_request = None\n",
    "    collected_messages: List[str] = []\n",
    "\n",
    "    # 🔁 FIRST STREAMING RESPONSE (may include tool call)\n",
    "    response = client.chat.completions.create(\n",
    "        stream=True,\n",
    "        messages=conversation_history,\n",
    "        tools=available_tools,\n",
    "        tool_choice=\"auto\",\n",
    "        max_tokens=4096,\n",
    "        temperature=0.5,\n",
    "        top_p=1.0,\n",
    "        model=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    "    )\n",
    "\n",
    "    for chunk in response:\n",
    "        if chunk.choices:\n",
    "            delta = chunk.choices[0].delta\n",
    "\n",
    "            if delta.tool_calls:\n",
    "                if delta.tool_calls[0].function.name:\n",
    "                    tool_name = chunk.choices[0].delta.tool_calls[0].function.name\n",
    "                    tool_id = chunk.choices[0].delta.tool_calls[0].id\n",
    "                    conversation_history.append(delta)\n",
    "\n",
    "                if chunk.choices[0].delta.tool_calls[0].function.arguments:\n",
    "                    function_call_arguments += delta.tool_calls[0].function.arguments\n",
    "\n",
    "            elif delta.content:\n",
    "                chunk_text = chunk.choices[0].delta.content\n",
    "                if chunk_text:\n",
    "                    collected_messages.append(chunk_text)\n",
    "                    if chunk_text in tts_sentence_end:\n",
    "                        text = \"\".join(collected_messages).strip()\n",
    "                        last_tts_request = (\n",
    "                            az_speach_synthesizer_client.start_speaking_text(text)\n",
    "                        )\n",
    "                        collected_messages.clear()\n",
    "\n",
    "    # 🧠 If tool call was detected, execute it\n",
    "    if tool_name:\n",
    "        print(f\"tool_name:{tool_name}\")\n",
    "        print(f\"tool_id:{tool_id}\")\n",
    "        print(f\"function_call_arguments:{function_call_arguments}\")\n",
    "        try:\n",
    "            parsed_args = json.loads(function_call_arguments.strip())\n",
    "            function_to_call = function_mapping.get(tool_name)\n",
    "\n",
    "            if function_to_call:\n",
    "                result = await function_to_call(parsed_args)\n",
    "\n",
    "                print(f\"✅ Function `{tool_name}` executed. Result: {result}\")\n",
    "\n",
    "                conversation_history.append(\n",
    "                    {\n",
    "                        \"tool_call_id\": tool_id,\n",
    "                        \"role\": \"tool\",\n",
    "                        \"name\": tool_name,\n",
    "                        \"content\": result,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # 🧠 SECOND STREAMING CALL AFTER TOOL EXECUTION\n",
    "                second_response = client.chat.completions.create(\n",
    "                    stream=True,\n",
    "                    messages=conversation_history,\n",
    "                    temperature=0.5,\n",
    "                    top_p=1.0,\n",
    "                    max_tokens=4096,\n",
    "                    model=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    "                )\n",
    "\n",
    "                collected_messages = []\n",
    "\n",
    "                for chunk in second_response:\n",
    "                    if chunk.choices:\n",
    "                        delta = chunk.choices[0].delta\n",
    "                        if hasattr(delta, \"content\") and delta.content:\n",
    "                            chunk_message = delta.content\n",
    "                            collected_messages.append(chunk_message)\n",
    "                            if chunk_message.strip() in tts_sentence_end:\n",
    "                                text = \"\".join(collected_messages).strip()\n",
    "                                if text:\n",
    "                                    az_speach_synthesizer_client.start_speaking_text(\n",
    "                                        text\n",
    "                                    )\n",
    "                                    collected_messages.clear()\n",
    "\n",
    "                final_text = \"\".join(collected_messages).strip()\n",
    "                if final_text:\n",
    "                    conversation_history.append(\n",
    "                        {\"role\": \"assistant\", \"content\": final_text}\n",
    "                    )\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"❌ Error parsing function arguments: {e}\")\n",
    "\n",
    "    else:\n",
    "        # Append the assistant message if no function call was made\n",
    "        final_text = \"\".join(collected_messages).strip()\n",
    "        if final_text:\n",
    "            conversation_history.append({\"role\": \"assistant\", \"content\": final_text})\n",
    "            print(f\"✅ Final assistant message: {final_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6111fdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool_name:escalate_emergency\n",
      "tool_id:call_hbMdkWeNNrbjDUEhUih6vQEK\n",
      "function_call_arguments:{\"reason\":\"urgent appointment needed\"}\n",
      "✅ Function `escalate_emergency` executed. Result: 🚨 Emergency escalation triggered: {'reason': 'urgent appointment needed'}. A human healthcare agent is now being connected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 00:42:22,451 - micro - MainProcess - INFO     [🔊] Starting streaming speech synthesis for text: I understand that this is an e... (text_to_speech.py:start_speaking_text:44)\n",
      "INFO:micro:[🔊] Starting streaming speech synthesis for text: I understand that this is an e...\n",
      "2025-04-15 00:42:22,529 - micro - MainProcess - INFO     [🔊] Starting streaming speech synthesis for text: I've escalated your request, a... (text_to_speech.py:start_speaking_text:44)\n",
      "INFO:micro:[🔊] Starting streaming speech synthesis for text: I've escalated your request, a...\n",
      "2025-04-15 00:42:22,535 - micro - MainProcess - INFO     [🔊] Starting streaming speech synthesis for text: Please hold on for a moment.... (text_to_speech.py:start_speaking_text:44)\n",
      "INFO:micro:[🔊] Starting streaming speech synthesis for text: Please hold on for a moment....\n"
     ]
    }
   ],
   "source": [
    "await handle_chat(conversation_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7cc26a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 17:05:05,507 - micro - MainProcess - INFO     [🛑] Stopping speech synthesis... (text_to_speech.py:stop_speaking:55)\n",
      "INFO:micro:[🛑] Stopping speech synthesis...\n"
     ]
    }
   ],
   "source": [
    "az_speach_synthesizer_client.stop_speaking()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audioagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
