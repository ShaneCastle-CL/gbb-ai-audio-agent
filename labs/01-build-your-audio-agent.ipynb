{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc6b4e04",
   "metadata": {},
   "source": [
    "## **Building Your Voice-to-Voice Agent with Azure AI Speech and AOAI**\n",
    "\n",
    "This notebook provides a step-by-step guide to create a voice-to-voice agent using Azure AI Speech services and Azure OpenAI. It walks you through the process of configuring speech recognition, integrating external tools, and generating human-like responses for real-time interactions.\n",
    "\n",
    "![alt text](../utils/images/lab1.png) \n",
    "\n",
    "\n",
    "1. **Audio Ingestion**: Ensure the capability to record audio is set up.  \n",
    "2. **Azure Speech-to-Text (STT)**: Converts live audio into transcribed text for LLM processing.  \n",
    "3. **Azure OpenAI with Function Calling & Streaming**: Understands patient intent, routes queries, and dynamically calls backend tools in real time.  \n",
    "4. **Azure Text-to-Speech (TTS)**: Delivers natural, empathetic voice responses back to the user in chunks.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed102ccb",
   "metadata": {},
   "source": [
    "## **Audio Ingestion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available audio devices:\n",
      "0: Microsoft Sound Mapper - Input\n",
      "1: Surface Stereo Microphones (Sur\n",
      "2: Microphone (Lumina Camera - Raw\n",
      "3: Echo Cancelling Speakerphone (M\n",
      "4: Microsoft Sound Mapper - Output\n",
      "5: Echo Cancelling Speakerphone (M\n",
      "6: Speakers (Dell USB Audio)\n",
      "7: Surface Omnisonic Speakers (Sur\n",
      "8: Primary Sound Capture Driver\n",
      "9: Surface Stereo Microphones (Surface High Definition Audio)\n",
      "10: Microphone (Lumina Camera - Raw)\n",
      "11: Echo Cancelling Speakerphone (Microsoft Audio Dock)\n",
      "12: Primary Sound Driver\n",
      "13: Echo Cancelling Speakerphone (Microsoft Audio Dock)\n",
      "14: Speakers (Dell USB Audio)\n",
      "15: Surface Omnisonic Speakers (Surface High Definition Audio)\n",
      "16: Speakers (Dell USB Audio)\n",
      "17: Surface Omnisonic Speakers (Surface High Definition Audio)\n",
      "18: Echo Cancelling Speakerphone (Microsoft Audio Dock)\n",
      "19: Surface Stereo Microphones (Surface High Definition Audio)\n",
      "20: Microphone (Lumina Camera - Raw)\n",
      "21: Echo Cancelling Speakerphone (Microsoft Audio Dock)\n",
      "22: Headphones ()\n",
      "23: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Shiva’s AirPods Pro #2))\n",
      "24: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Shiva’s AirPods Pro #2))\n",
      "25: Speakers (Dell USB Audio)\n",
      "26: Microphone (Dell USB Audio)\n",
      "27: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Shiva’s AirPods Pro #2 - Find My))\n",
      "28: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Shiva’s AirPods Pro #2 - Find My))\n",
      "29: Headphones 1 (Realtek HD Audio 2nd output with SST)\n",
      "30: Headphones 2 (Realtek HD Audio 2nd output with SST)\n",
      "31: PC Speaker (Realtek HD Audio 2nd output with SST)\n",
      "32: Speakers 1 (Realtek HD Audio output with SST)\n",
      "33: Speakers 2 (Realtek HD Audio output with SST)\n",
      "34: PC Speaker (Realtek HD Audio output with SST)\n",
      "35: Microphone Array (Realtek HD Audio Mic input)\n",
      "36: Headset Microphone (Headset Microphone)\n",
      "37: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Pablo’s AirPods #3))\n",
      "38: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Pablo’s AirPods #3))\n",
      "39: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Pablo’s AirPods #4))\n",
      "40: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Pablo’s AirPods #4))\n",
      "41: Headphones ()\n",
      "42: Headphones ()\n",
      "43: Microphone (Lumina Camera - Raw)\n",
      "44: Echo Cancelling Speakerphone (Microsoft Audio Dock)\n",
      "45: Echo Cancelling Speakerphone (Microsoft Audio Dock)\n",
      "46: Headphones ()\n",
      "47: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Pablo’s AirPods Pro - Find My))\n",
      "48: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Pablo’s AirPods Pro - Find My))\n",
      "49: Headphones ()\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "\n",
    "def list_audio_devices():\n",
    "    \"\"\"\n",
    "    List all available audio devices using PyAudio.\n",
    "\n",
    "    This function initializes PyAudio, retrieves the list of audio devices,\n",
    "    and prints their names. It also includes error handling to ensure proper\n",
    "    cleanup of resources.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        p = pyaudio.PyAudio()\n",
    "        print(\"Available audio devices:\")\n",
    "        for ii in range(p.get_device_count()):\n",
    "            device_name = p.get_device_info_by_index(ii).get('name')\n",
    "            print(f\"{ii}: {device_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while listing audio devices: {e}\")\n",
    "    finally:\n",
    "        # Ensure PyAudio resources are released\n",
    "        if 'p' in locals():\n",
    "            p.terminate()\n",
    "\n",
    "# Call the function to list audio devices\n",
    "list_audio_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e9ade96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording complete. Saving audio...\n",
      "Audio saved to test_audio.wav. Playing back...\n",
      "Playback complete.\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "def test_microphone():\n",
    "    \"\"\"\n",
    "    Test the microphone by recording audio and playing it back.\n",
    "\n",
    "    This function captures audio from the default input device (microphone),\n",
    "    saves it to a temporary WAV file, and plays it back to ensure the microphone\n",
    "    is working correctly.\n",
    "    \"\"\"\n",
    "    # Audio configuration\n",
    "    chunk = 1024  # Number of frames per buffer\n",
    "    format = pyaudio.paInt16  # 16-bit audio format\n",
    "    channels = 1  # Mono audio\n",
    "    rate = 44100  # Sampling rate (44.1 kHz)\n",
    "    record_seconds = 5  # Duration of the recording\n",
    "    output_filename = \"test_audio.wav\"\n",
    "\n",
    "    # Initialize PyAudio\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    try:\n",
    "        # Open the microphone stream\n",
    "        print(\"Recording...\")\n",
    "        stream = p.open(format=format,\n",
    "                        channels=channels,\n",
    "                        rate=rate,\n",
    "                        input=True,\n",
    "                        frames_per_buffer=chunk)\n",
    "\n",
    "        frames = []\n",
    "\n",
    "        # Record audio in chunks\n",
    "        for _ in range(0, int(rate / chunk * record_seconds)):\n",
    "            data = stream.read(chunk)\n",
    "            frames.append(data)\n",
    "\n",
    "        print(\"Recording complete. Saving audio...\")\n",
    "\n",
    "        # Save the recorded audio to a WAV file\n",
    "        with wave.open(output_filename, 'wb') as wf:\n",
    "            wf.setnchannels(channels)\n",
    "            wf.setsampwidth(p.get_sample_size(format))\n",
    "            wf.setframerate(rate)\n",
    "            wf.writeframes(b''.join(frames))\n",
    "\n",
    "        print(f\"Audio saved to {output_filename}. Playing back...\")\n",
    "\n",
    "        # Play back the recorded audio\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "\n",
    "        # Open the WAV file for playback\n",
    "        wf = wave.open(output_filename, 'rb')\n",
    "        playback_stream = p.open(format=p.get_format_from_width(wf.getsampwidth()),\n",
    "                                 channels=wf.getnchannels(),\n",
    "                                 rate=wf.getframerate(),\n",
    "                                 output=True)\n",
    "\n",
    "        # Read and play audio data\n",
    "        data = wf.readframes(chunk)\n",
    "        while data:\n",
    "            playback_stream.write(data)\n",
    "            data = wf.readframes(chunk)\n",
    "\n",
    "        playback_stream.stop_stream()\n",
    "        playback_stream.close()\n",
    "\n",
    "        print(\"Playback complete.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Terminate PyAudio\n",
    "        p.terminate()\n",
    "\n",
    "# Run the microphone test\n",
    "test_microphone()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e9da16",
   "metadata": {},
   "source": [
    "## **Define Clients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c3f52ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory changed to C:\\Users\\pablosal\\Desktop\\gbb-ai-audio-agent\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pprint\n",
    "\n",
    "# Define the target directory\n",
    "target_directory = r\"C:\\Users\\pablosal\\Desktop\\gbb-ai-audio-agent\"  # change your directory here\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(target_directory):\n",
    "    # Change the current working directory\n",
    "    os.chdir(target_directory)\n",
    "    print(f\"Directory changed to {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"Directory {target_directory} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8a11c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.speech.speech_recognizer import SpeechRecognizer, StreamingSpeechRecognizer\n",
    "from src.speech.text_to_speech import SpeechSynthesizer\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Ensure clients are initialized only if not already defined\n",
    "if 'az_speech_recognizer_client' not in locals():\n",
    "    az_speech_recognizer_client = SpeechRecognizer()\n",
    "\n",
    "if 'az_speech_recognizer_stream_client' not in locals():\n",
    "    az_speech_recognizer_stream_client = StreamingSpeechRecognizer(vad_silence_timeout_ms=5000)\n",
    "\n",
    "if 'az_speach_synthesizer_client' not in locals():\n",
    "    az_speach_synthesizer_client = SpeechSynthesizer()\n",
    "\n",
    "# Ensure Azure OpenAI client is initialized only if not already defined\n",
    "if 'client' not in locals():\n",
    "    client = AzureOpenAI(\n",
    "        api_version=\"2025-02-01-preview\",\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4ee563",
   "metadata": {},
   "source": [
    "## **Azure Speech-to-Text (STT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a82c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 10:14:30,825 - micro - MainProcess - INFO     Starting continuous speech recognition with VAD using asynchronous call... (speech_recognizer.py:start:134)\n",
      "INFO:micro:Starting continuous speech recognition with VAD using asynchronous call...\n",
      "2025-04-16 10:14:30,911 - micro - MainProcess - INFO     Continuous speech recognition started. (speech_recognizer.py:start:170)\n",
      "INFO:micro:Continuous speech recognition started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial: play and\n",
      "Partial: play and you will see\n",
      "Partial: play and you will see that\n",
      "Partial: play and you will see that hey how\n",
      "Partial: play and you will see that hey how are you doing\n",
      "Partial: play and you will see that hey how are you doing all good\n",
      "Partial: play and you will see that hey how are you doing all good yeah very nice\n",
      "Partial: play and you will see that hey how are you doing all good yeah very nice very nice\n",
      "Partial: play and you will see that hey how are you doing all good yeah very nice very nice very nice\n",
      "Partial: play and you will see that hey how are you doing all good yeah very nice very nice very nice bye\n",
      "Final: Play and you will see that. Hey, how are you doing? All good. Yeah, very nice. Very nice. Very nice. Bye. Bye.\n"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "recognizer = StreamingSpeechRecognizer(\n",
    "    vad_silence_timeout_ms=3000\n",
    ")\n",
    "\n",
    "# Buffers\n",
    "all_text_live = \"\"\n",
    "final_transcripts = []\n",
    "\n",
    "# Internal tracker to prevent double-processing\n",
    "last_final_text = None\n",
    "\n",
    "# Callbacks\n",
    "def on_partial(text: str):\n",
    "    global all_text_live\n",
    "    print(f\"Partial: {text}\")\n",
    "    all_text_live = text\n",
    "\n",
    "def on_final(text: str):\n",
    "    global all_text_live, last_final_text\n",
    "\n",
    "    # Skip duplicate finalizations\n",
    "    if text == last_final_text:\n",
    "        return\n",
    "\n",
    "    last_final_text = text\n",
    "\n",
    "    print(f\"Final: {text}\")\n",
    "    final_transcripts.append(text)\n",
    "    all_text_live = \"\"  # Clear the live buffer\n",
    "\n",
    "# Attach\n",
    "recognizer.set_partial_result_callback(on_partial)\n",
    "recognizer.set_final_result_callback(on_final)\n",
    "\n",
    "# Start recognizing\n",
    "recognizer.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04ebc8a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Play and you will see that. Hey, how are you doing? All good. Yeah, very nice. Very nice. Very nice. Bye. Bye. '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_conversation = \" \".join(final_transcripts) + \" \" + all_text_live\n",
    "full_conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "698b9a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 10:14:45,896 - micro - MainProcess - INFO     Stopping continuous speech recognition using asynchronous call... (speech_recognizer.py:stop:177)\n",
      "INFO:micro:Stopping continuous speech recognition using asynchronous call...\n",
      "2025-04-16 10:14:45,943 - micro - MainProcess - INFO     Speech recognition session stopped: SessionEventArgs(session_id=628b724f7c9646dcb622821ba7c3e2e7) (speech_recognizer.py:_on_session_stopped:214)\n",
      "INFO:micro:Speech recognition session stopped: SessionEventArgs(session_id=628b724f7c9646dcb622821ba7c3e2e7)\n",
      "2025-04-16 10:14:45,954 - micro - MainProcess - INFO     Continuous speech recognition stopped. (speech_recognizer.py:stop:180)\n",
      "INFO:micro:Continuous speech recognition stopped.\n"
     ]
    }
   ],
   "source": [
    "# Start recognizing\n",
    "recognizer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eabf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 21:14:49,793 - micro - MainProcess - INFO     Starting continuous speech recognition with VAD using asynchronous call... (speech_recognizer.py:start:134)\n",
      "2025-04-15 21:14:49,901 - micro - MainProcess - INFO     Continuous speech recognition started. (speech_recognizer.py:start:170)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting microphone recognition...\n",
      "🎤 Listening... (speak now)\n",
      "Partial: hello\n",
      "Partial: hello how are\n",
      "Partial: hello how are you doing\n",
      "Partial: hello how are you doing are you doing well\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 21:14:56,730 - micro - MainProcess - INFO     Stopping continuous speech recognition using asynchronous call... (speech_recognizer.py:stop:177)\n",
      "2025-04-15 21:14:56,786 - micro - MainProcess - INFO     Speech recognition session stopped: SessionEventArgs(session_id=cd90a3ecd6f14f269c93077234b350a5) (speech_recognizer.py:_on_session_stopped:214)\n",
      "2025-04-15 21:14:56,793 - micro - MainProcess - INFO     Continuous speech recognition stopped. (speech_recognizer.py:stop:180)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final: Hello, how are you doing? Are you doing well?\n",
      "🛑 Recognition stopped.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 21:47:00,858 - micro - MainProcess - WARNING  Speech recognition canceled: SpeechRecognitionCanceledEventArgs(session_id=cd90a3ecd6f14f269c93077234b350a5, result=SpeechRecognitionResult(result_id=aa9120a414b64b4a8bf862f879294e5d, text=\"\", reason=ResultReason.Canceled)) (speech_recognizer.py:_on_canceled:203)\n",
      "2025-04-15 21:47:00,869 - micro - MainProcess - WARNING  Cancellation reason: CancellationReason.Error (speech_recognizer.py:_on_canceled:206)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "SILENCE_THRESHOLD = 20  # seconds of silence to stop recognition\n",
    "\n",
    "def handle_speech_recognition() -> str:\n",
    "    global all_text_live, last_final_text\n",
    "\n",
    "    final_transcripts = []\n",
    "\n",
    "    print(\"Starting microphone recognition...\")\n",
    "    final_transcripts.clear()\n",
    "    all_text_live = \"\"\n",
    "    last_final_text = None\n",
    "\n",
    "    def on_partial(text: str):\n",
    "        global all_text_live\n",
    "        print(f\"Partial: {text}\")\n",
    "        all_text_live = text\n",
    "\n",
    "    def on_final(text: str):\n",
    "        global all_text_live, last_final_text\n",
    "        if text == last_final_text:\n",
    "            return\n",
    "        last_final_text = text\n",
    "        print(f\"Final: {text}\")\n",
    "        final_transcripts.append(text)\n",
    "        all_text_live = \"\"  # Clear the live buffer\n",
    "\n",
    "    recognizer.set_partial_result_callback(on_partial)\n",
    "    recognizer.set_final_result_callback(on_final)\n",
    "\n",
    "    recognizer.start()\n",
    "    print(\"🎤 Listening... (speak now)\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    while not final_transcripts and (time.time() - start_time < SILENCE_THRESHOLD):\n",
    "        time.sleep(0.05)\n",
    "\n",
    "    recognizer.stop()\n",
    "    print(\"🛑 Recognition stopped.\")\n",
    "\n",
    "    return \" \".join(final_transcripts) + \" \" + all_text_live\n",
    "\n",
    "recognition = handle_speech_recognition()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21400ef9",
   "metadata": {},
   "source": [
    "## **Azure OpenAI Streaming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d92bad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Madrid, the vibrant capital of Spain, is a city that blends historical charm with modern dynamism. Nestled in the heart of the Iberian Peninsula, the city is steeped in rich cultural heritage, bursting with energy, and boasts a unique atmosphere that attracts millions of visitors every year. Let’s take a closer look at the aspects that make Madrid a remarkable destination.\n",
      "\n",
      "**Historical Roots and Architecture**\n",
      "Madrid’s history dates back to the 9th century when Mohamed I built a fortress on the site where the Royal Palace stands today. It grew into prominence in 1561 when King Philip II transferred his court there, and it became the capital of Spain. Its history is reflected in its stunning architecture, with a blend of medieval, Renaissance, Baroque, and modern styles. The Royal Palace of Madrid, the official residence of the Spanish royal family, is an architectural marvel with over 3,000 rooms, vast gardens, and stunning art collections.\n",
      "\n",
      "The Plaza Mayor, a grand arcade square built in the 17th century, stands as a centerpiece of Madrid’s historical side. It has witnessed countless historical events, from royal festivities to markets and bullfights. The nearby Puerta del Sol, considered the heart of Madrid, is famed for the clock tower, where Spaniards traditionally welcome the New Year.\n",
      "\n",
      "Madrid’s architecture isn't limited to historical monuments. The city also embraces contemporary designs, such as the striking Cuatro Torres Business Area, where modern skyscrapers punctuate the skyline, displaying Madrid’s capacity to blend tradition with modernity.\n",
      "\n",
      "**Museums and Art**\n",
      "Madrid is a haven for art lovers, housing some of the world's most extraordinary museums. The Prado Museum is an essential visit, hosting masterpieces by Velázquez, Goya, Rubens, and Bosch. Its vast collection covers Renaissance, Baroque, and Romantic art, offering an immersive journey through European art history.\n",
      "\n",
      "The Reina Sofia Museum is renowned for contemporary and modern art, with Picasso’s iconic masterpiece \"Guernica\" drawing visitors from around the globe. The Thyssen-Bornemisza Museum completes Madrid’s golden triangle of art institutions, providing an eclectic collection that spans Impressionism, Expressionism, and Pop Art.\n",
      "\n",
      "Beyond these major museums, Madrid is sprinkled with smaller art galleries and cultural centers showcasing the works of emerging artists, making the city a fertile ground for both art appreciation and innovation.\n",
      "\n",
      "**Culture and Lifestyle**\n",
      "Madrid’s cultural vibrancy is palpable. The city prides itself on its theaters, cultural events, and live performances. The Gran Via, often dubbed Madrid’s Broadway, is lined with theaters showcasing musicals, dramas, and comedies. The Teatro Real, an opera house built in the 19th century, offers a luxurious setting for opera and ballet enthusiasts.\n",
      "\n",
      "Cultural festivities are a cornerstone of Madrid’s identity. The city joyfully embraces traditional festivals such as San Isidro in May, where locals don traditional attire, sing, dance, and honor the patron saint of Madrid. Then there's the Flamenco, Spain’s passionate dance and music tradition, which finds its fervent expression in numerous tablaos across Madrid, keeping the cultural heartbeat of the city alive.\n",
      "\n",
      "The lifestyle in Madrid is laid-back yet bustling. The concept of \"tapeo\", the casual grazing of tapas, is an ingrained lifestyle ritual. Tapas bars abound, each offering delicious small plates, from patatas bravas to jamón ibérico, providing a delightful taste of Spanish cuisine without the need for a full meal. The city's markets, like Mercado de San Miguel, are a feast for the senses, combining grocery shopping with gourmet sampling.\n",
      "\n",
      "The nightlife in Madrid is legendary, ranging from quiet taverns to vibrant nightclubs. The district of Malasaña is known for its indie scene and alternative music, while neighborhoods like Chueca and La Latina provide diverse venues for night owls.\n",
      "\n",
      "**Parks and Recreation**\n",
      "Madrid is a green city where parks and gardens offer respite from the urban pace. El Retiro Park, once the royal retreat, is a sprawling haven with manicured gardens, fountains, and lakes. It's a popular spot for locals and tourists alike to unwind, enjoy boat rides, or simply soak up the sun.\n",
      "\n",
      "Casa de Campo is Madrid’s largest park, a vast woodland offering hiking, cycling, and the city zoo. Its natural beauty provides plenty of activities, from family picnics to adventurous trails.\n",
      "\n",
      "**Cuisine**\n",
      "Madrid is a culinary capital where food is celebrated with fervor. Beyond tapas, the city offers traditional dishes like Cocido Madrileño, a hearty stew, and churros with chocolate, perfect for indulging. The culinary scene also embraces global influences, providing a diverse dining landscape with Michelin-starred restaurants and innovative fusion eateries.\n",
      "\n",
      "**People and Language**\n",
      "The people of Madrid, known as Madrileños, are welcoming and convivial. Madrid's cosmopolitan nature is reflected in the diversity seen across the city, both culturally and linguistically. Spanish is the official language, and learning a few phrases in Spanish can enrich the experience, as locals appreciate the effort to communicate in their language.\n",
      "\n",
      "**Shopping and Fashion**\n",
      "Madrid is a fashion-forward city with shopping experiences to match every taste. The Salamanca district is home to luxury boutiques and designer brands, while districts like Malasaña and Chueca offer eclectic shops and thrifting opportunities. The El Rastro market, a famous flea market open on Sundays, is vibrant and colorful, providing antiques, trinkets, and souvenirs.\n",
      "\n",
      "**Transportation and Accessibility**\n",
      "Madrid boasts an efficient public transportation system, including metro, buses, and suburban trains, making it easy to navigate. The Adolfo Suárez Madrid-Barajas Airport connects the city globally, while high-speed trains facilitate travel to other parts of Spain.\n",
      "\n",
      "**Education and Innovation**\n",
      "Madrid is home to prestigious universities and institutes that drive educational excellence and innovation. Complutense University, one of Europe’s oldest universities, alongside newer institutions, fosters academic and cultural growth. Moreover, the city’s innovation scene is thriving, with tech startups and creative hubs contributing to Madrid's economic vigor.\n",
      "\n",
      "**In Summary**\n",
      "Madrid, with its incredible blend of history, art, culture, and modern innovations, stands as a testament to Spain’s vibrant spirit. From its majestic architectural landmarks and world-class museums to its lively festivals and culinary delights, the city offers a rich tapestry that celebrates the past while embracing the future. The warmth and friendliness of Madrileños, combined with an enthralling lifestyle, render Madrid a must-visit destination and a city beloved by those who have the pleasure of experiencing its remarkable charm."
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    stream=True,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Describe Madrid in 1000 words\",\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=4096,\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    model=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    ")\n",
    "full_response = \"\"\n",
    "for update in response:\n",
    "    if update.choices:\n",
    "        chunk = update.choices[0].delta.content or \"\"\n",
    "        full_response += chunk\n",
    "        print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1486daef",
   "metadata": {},
   "source": [
    "## **Azure Text-to-Speech (TTS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82328088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 15:36:26,113 - micro - MainProcess - INFO     [🔊] Starting streaming speech synthesis for text: Hello! I'm just a computer pro... (text_to_speech.py:start_speaking_text:36)\n",
      "INFO:micro:[🔊] Starting streaming speech synthesis for text: Hello! I'm just a computer pro...\n",
      "2025-04-15 15:36:30,122 - micro - MainProcess - INFO     [🛑] Stopping speech synthesis... (text_to_speech.py:stop_speaking:43)\n",
      "INFO:micro:[🛑] Stopping speech synthesis...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "az_speach_synthesizer_client.start_speaking_text(\n",
    "                    text=full_response,\n",
    "                )\n",
    "time.sleep(4)\n",
    "az_speach_synthesizer_client.stop_speaking()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894c6a37",
   "metadata": {},
   "source": [
    "## **Streaming Audio Back to the User**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d764bd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 10:15:39,495 - micro - MainProcess - INFO     [🔊] Starting streaming speech synthesis for text: It sounds like you're quoting ... (text_to_speech.py:start_speaking_text:36)\n",
      "INFO:micro:[🔊] Starting streaming speech synthesis for text: It sounds like you're quoting ...\n",
      "2025-04-16 10:15:39,514 - micro - MainProcess - INFO     [🔊] Starting streaming speech synthesis for text: I'm here to help if you want t... (text_to_speech.py:start_speaking_text:36)\n",
      "INFO:micro:[🔊] Starting streaming speech synthesis for text: I'm here to help if you want t...\n",
      "2025-04-16 10:15:39,527 - micro - MainProcess - INFO     [🔊] Starting streaming speech synthesis for text: How can I assist you today?... (text_to_speech.py:start_speaking_text:36)\n",
      "INFO:micro:[🔊] Starting streaming speech synthesis for text: How can I assist you today?...\n"
     ]
    }
   ],
   "source": [
    "tts_sentence_end = [ \".\", \"!\", \"?\", \";\", \"。\", \"！\", \"？\", \"；\", \"\\n\" ]\n",
    "completion = client.chat.completions.create(\n",
    "    stream=True,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f'{full_conversation}',\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=4096,\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    model=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    ")\n",
    "\n",
    "collected_messages = []\n",
    "last_tts_request = None\n",
    "\n",
    "for chunk in completion:\n",
    "    if len(chunk.choices) > 0:\n",
    "        chunk_text = chunk.choices[0].delta.content\n",
    "        if chunk_text:\n",
    "            collected_messages.append(chunk_text)\n",
    "            if chunk_text in tts_sentence_end:\n",
    "                text = \"\".join(collected_messages).strip()\n",
    "                last_tts_request = az_speach_synthesizer_client.start_speaking_text(text)\n",
    "                collected_messages.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fffdc7b",
   "metadata": {},
   "source": [
    "## **Adding Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a19e4ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Templates found: ['voice_agent_system.jinja', 'voice_agent_user.jinja']\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "from app.backend.prompt_manager import PromptManager\n",
    "\n",
    "prompt_manager = PromptManager()\n",
    "systemp_prompt = prompt_manager.get_prompt(\"voice_agent_system.jinja\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0c217c",
   "metadata": {},
   "source": [
    "## **Adding Tools**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e66946b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.backend.tools import available_tools\n",
    "from app.backend.functions import (\n",
    "    schedule_appointment,\n",
    "    refill_prescription,\n",
    "    lookup_medication_info,\n",
    "    evaluate_prior_authorization,\n",
    "    escalate_emergency,\n",
    "    authenticate_user\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0da14961",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history: List[Dict[str, str]] = [\n",
    "    {\"role\": \"system\", \"content\": systemp_prompt},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello, my name is Pablo Salvador. \"\n",
    "        \"this is an emergency. I need to schedule an appointment with my doctor ASAP.\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f201ef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping tool names to actual Python async functions\n",
    "function_mapping = {\n",
    "    \"schedule_appointment\": schedule_appointment,\n",
    "    \"refill_prescription\": refill_prescription,\n",
    "    \"lookup_medication_info\": lookup_medication_info,\n",
    "    \"evaluate_prior_authorization\": evaluate_prior_authorization,\n",
    "    \"escalate_emergency\": escalate_emergency,\n",
    "    \"authenticate_user\": authenticate_user,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2716c04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's response:\n",
      "ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_mIxsi3OwoVbUtqSgjTxDFtTK', function=Function(arguments='{\"reason\":\"Need to schedule an appointment urgently.\"}', name='escalate_emergency'), type='function')])\n",
      "Function result: 🚨 Emergency escalation triggered: Need to schedule an appointment urgently.. A human healthcare agent is now being connected.\n",
      "Tool Name: escalate_emergency\n",
      "Final response:\n",
      "I’ve escalated your request to a human healthcare agent who will assist you with scheduling your appointment immediately. Please hold on for a moment.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "        messages=conversation_history,\n",
    "        tools=available_tools,\n",
    "        tool_choice=\"auto\",\n",
    "        max_tokens=4096,\n",
    "        temperature=0.5,\n",
    "        top_p=1.0,\n",
    "        model=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    "    )\n",
    "\n",
    "# Process the model's response\n",
    "response_message = response.choices[0].message\n",
    "conversation_history.append(response_message)\n",
    "\n",
    "print(\"Model's response:\")  \n",
    "print(response_message)  \n",
    "\n",
    "if response_message.tool_calls:\n",
    "    for tool_call in response_message.tool_calls:\n",
    "        function_name = tool_call.function.name\n",
    "        function_args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "        # Check if the function name is in the mapping\n",
    "        if function_name in function_mapping:\n",
    "            # Call the corresponding Python async function\n",
    "            result = await function_mapping[function_name](**function_args)\n",
    "            print(f\"Function result: {result}\")  \n",
    "            print(f\"Tool Name: {function_name}\")  \n",
    "            conversation_history.append({\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"role\": \"tool\",\n",
    "                \"name\": function_name,\n",
    "                \"content\": result,\n",
    "            })\n",
    "else:\n",
    "    print(\"No tool calls were made by the model.\")\n",
    "\n",
    "# Second API call: Get the final response from the model\n",
    "final_response = client.chat.completions.create(\n",
    "    messages=conversation_history,\n",
    "    max_tokens=4096,\n",
    "    temperature=0.5,\n",
    "    top_p=1.0,\n",
    "    model=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    ")   \n",
    "\n",
    "print(\"Final response:\")\n",
    "print(final_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70a5c34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c4fbffb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SpeechSynthesisOutputFormat.Riff24Khz16BitMonoPcm: 13>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speechsdk.SpeechSynthesisOutputFormat.Riff24Khz16BitMonoPcm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734714e0",
   "metadata": {},
   "source": [
    "## **Making Tools work in Streaming Fashion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfe00db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history: List[Dict[str, str]] = [\n",
    "    {\"role\": \"system\", \"content\": systemp_prompt},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello, my name is Pablo Salvador. \"\n",
    "        \"this is an emergency. I need to schedule an appointment with my doctor ASAP.\",\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f55da9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=0, id='call_VWhpt20nqoslFiup9FVvb9Dd', function=ChoiceDeltaToolCallFunction(arguments='', name='escalate_emergency'), type='function')])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='{\"', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='reason', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\":\"', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='Need', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' to', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' schedule', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' an', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' emergency', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' appointment', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' with', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' doctor', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' ASAP', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='.\"', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='}', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=None)\n",
      "tool_name:escalate_emergency\n",
      "tool_id:call_VWhpt20nqoslFiup9FVvb9Dd\n",
      "tool_call_accumulator:{\"reason\":\"Need to schedule an emergency appointment with doctor ASAP.\"}\n",
      "Function result: 🚨 Emergency escalation triggered: Request for immediate appointment scheduling due to emergency.. A human healthcare agent is now being connected.\n",
      "=============================================\n",
      "I've triggered an emergency escalation for your request, and a human healthcare agent is now being connected to assist you. Please hold on for a moment while they join the conversation. Your safety is our priority."
     ]
    }
   ],
   "source": [
    "# initial user message    \n",
    "tool_call_accumulator = \"\"\n",
    "\n",
    "# First API call: Ask the model to use the function\n",
    "response = client.chat.completions.create(\n",
    "    model= os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    "    messages=conversation_history,\n",
    "    tools=available_tools,\n",
    "    tool_choice=\"auto\",\n",
    "    stream=True  # this time, we set stream=True\n",
    ")\n",
    "\n",
    "# process the model \n",
    "for chunk in response:\n",
    "    if len(chunk.choices) > 0:\n",
    "        delta = chunk.choices[0].delta\n",
    "        print(f\"delta: {delta}\") # print delta from the chunk for learning\n",
    "        \n",
    "        if delta.tool_calls:\n",
    "                if(delta.tool_calls[0].function.name):\n",
    "                    tool_name = chunk.choices[0].delta.tool_calls[0].function.name\n",
    "                    tool_id = chunk.choices[0].delta.tool_calls[0].id\n",
    "                    conversation_history.append(delta)\n",
    "                \n",
    "                if(chunk.choices[0].delta.tool_calls[0].function.arguments):    \n",
    "                    tool_call_accumulator+=delta.tool_calls[0].function.arguments\n",
    "\n",
    "# print function related outputs                    \n",
    "print(f\"tool_name:{tool_name}\")   \n",
    "print(f\"tool_id:{tool_id}\")    \n",
    "print(f\"tool_call_accumulator:{tool_call_accumulator}\")   \n",
    "# Check if the function name is in the mapping\n",
    "if tool_name in function_mapping:\n",
    "    # Call the corresponding Python async function\n",
    "    result = await function_mapping[function_name](**function_args)\n",
    "    print(f\"Function result: {result}\")  \n",
    "    conversation_history.append({\n",
    "        \"tool_call_id\": tool_id,\n",
    "        \"role\": \"tool\",\n",
    "        \"name\": tool_name,\n",
    "        \"content\": result,\n",
    "    })\n",
    "\n",
    "#print(f\"messages: {messages}\") \n",
    "print(\"=============================================\")  \n",
    "    \n",
    "#  second API call: Get the final response with stream\n",
    "response = client.chat.completions.create(\n",
    "    model= os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    "    messages=conversation_history,\n",
    "    temperature=0.7,\n",
    "    stream=True  # this time, we set stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    if len(chunk.choices) > 0:\n",
    "        delta = chunk.choices[0].delta\n",
    "        #print(f\"delta: {delta}\")\n",
    "        if delta.content:\n",
    "            print(delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d636a8d5",
   "metadata": {},
   "source": [
    "## **Run the Conversation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0de55282",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history: List[Dict[str, str]] = [\n",
    "    {\"role\": \"system\", \"content\": systemp_prompt},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello, my name is Pablo Salvador. \"\n",
    "        \"this is an emergency. I need to schedule an appointment with my doctor ASAP.\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f5c9d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history: List[Dict[str, str]] = [\n",
    "    {\"role\": \"system\", \"content\": systemp_prompt},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello, my name is Pablo Salvador. \"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea4f10c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "tts_sentence_end = [\".\", \"!\", \"?\", \";\", \"。\", \"！\", \"？\", \"；\", \"\\n\"]\n",
    "\n",
    "async def handle_chat(conversation_history: List[Dict[str, Any]]) -> None:\n",
    "    \"\"\"\n",
    "    Handles a full streaming chat round with Azure OpenAI GPT-4o,\n",
    "    correctly executes tool calls, synthesizes responses, and continues reasoning.\n",
    "    \"\"\"\n",
    "    tool_name = None\n",
    "    function_call_arguments = \"\"\n",
    "    tool_call_id = None\n",
    "    last_tts_request = None\n",
    "    collected_messages: List[str] = []\n",
    "\n",
    "    # 🔁 FIRST STREAMING RESPONSE (may include tool call)\n",
    "    response = client.chat.completions.create(\n",
    "        stream=True,\n",
    "        messages=conversation_history,\n",
    "        tools=available_tools,\n",
    "        tool_choice=\"auto\",\n",
    "        max_tokens=4096,\n",
    "        temperature=0.5,\n",
    "        top_p=1.0,\n",
    "        model=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    "    )\n",
    "\n",
    "    for chunk in response:\n",
    "        if chunk.choices:\n",
    "            delta = chunk.choices[0].delta\n",
    "\n",
    "            if delta.tool_calls:\n",
    "                if(delta.tool_calls[0].function.name):\n",
    "                    tool_name = chunk.choices[0].delta.tool_calls[0].function.name\n",
    "                    tool_id = chunk.choices[0].delta.tool_calls[0].id\n",
    "                    conversation_history.append(delta)\n",
    "                \n",
    "                if(chunk.choices[0].delta.tool_calls[0].function.arguments):    \n",
    "                    function_call_arguments+=delta.tool_calls[0].function.arguments\n",
    "            \n",
    "            elif delta.content:\n",
    "                chunk_text = chunk.choices[0].delta.content\n",
    "                if chunk_text:\n",
    "                    collected_messages.append(chunk_text)\n",
    "                    if chunk_text in tts_sentence_end:\n",
    "                        text = \"\".join(collected_messages).strip()\n",
    "                        last_tts_request = az_speach_synthesizer_client.start_speaking_text(text)\n",
    "                        collected_messages.clear()\n",
    "\n",
    "    # 🧠 If tool call was detected, execute it\n",
    "    if tool_name:\n",
    "        print(f\"tool_name:{tool_name}\")   \n",
    "        print(f\"tool_id:{tool_id}\")    \n",
    "        print(f\"function_call_arguments:{function_call_arguments}\")  \n",
    "        try:\n",
    "            parsed_args = json.loads(function_call_arguments.strip())\n",
    "            function_to_call = function_mapping.get(tool_name)\n",
    "\n",
    "            if function_to_call:\n",
    "                result = await function_to_call(parsed_args)\n",
    "\n",
    "                print(f\"✅ Function `{tool_name}` executed. Result: {result}\")\n",
    "\n",
    "                conversation_history.append({\n",
    "                    \"tool_call_id\": tool_id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": tool_name,\n",
    "                    \"content\": result,\n",
    "                })\n",
    "\n",
    "                # 🧠 SECOND STREAMING CALL AFTER TOOL EXECUTION\n",
    "                second_response = client.chat.completions.create(\n",
    "                    stream=True,\n",
    "                    messages=conversation_history,\n",
    "                    temperature=0.5,\n",
    "                    top_p=1.0,\n",
    "                    max_tokens=4096,\n",
    "                    model=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    "                )\n",
    "\n",
    "                collected_messages = []\n",
    "\n",
    "                for chunk in second_response:\n",
    "                    if chunk.choices:\n",
    "                        delta = chunk.choices[0].delta\n",
    "                        if hasattr(delta, \"content\") and delta.content:\n",
    "                            chunk_message = delta.content\n",
    "                            collected_messages.append(chunk_message)\n",
    "                            if chunk_message.strip() in tts_sentence_end:\n",
    "                                text = ''.join(collected_messages).strip()\n",
    "                                if text:\n",
    "                                    az_speach_synthesizer_client.start_speaking_text(text)\n",
    "                                    collected_messages.clear()\n",
    "\n",
    "                final_text = ''.join(collected_messages).strip()\n",
    "                if final_text:\n",
    "                    conversation_history.append({\"role\": \"assistant\", \"content\": final_text})\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"❌ Error parsing function arguments: {e}\")\n",
    "\n",
    "    else:\n",
    "        # Append the assistant message if no function call was made\n",
    "        final_text = ''.join(collected_messages).strip()\n",
    "        if final_text:\n",
    "            conversation_history.append({\"role\": \"assistant\", \"content\": final_text})\n",
    "            print(f\"✅ Final assistant message: {final_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6111fdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool_name:escalate_emergency\n",
      "tool_id:call_hbMdkWeNNrbjDUEhUih6vQEK\n",
      "function_call_arguments:{\"reason\":\"urgent appointment needed\"}\n",
      "✅ Function `escalate_emergency` executed. Result: 🚨 Emergency escalation triggered: {'reason': 'urgent appointment needed'}. A human healthcare agent is now being connected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 00:42:22,451 - micro - MainProcess - INFO     [🔊] Starting streaming speech synthesis for text: I understand that this is an e... (text_to_speech.py:start_speaking_text:44)\n",
      "INFO:micro:[🔊] Starting streaming speech synthesis for text: I understand that this is an e...\n",
      "2025-04-15 00:42:22,529 - micro - MainProcess - INFO     [🔊] Starting streaming speech synthesis for text: I've escalated your request, a... (text_to_speech.py:start_speaking_text:44)\n",
      "INFO:micro:[🔊] Starting streaming speech synthesis for text: I've escalated your request, a...\n",
      "2025-04-15 00:42:22,535 - micro - MainProcess - INFO     [🔊] Starting streaming speech synthesis for text: Please hold on for a moment.... (text_to_speech.py:start_speaking_text:44)\n",
      "INFO:micro:[🔊] Starting streaming speech synthesis for text: Please hold on for a moment....\n"
     ]
    }
   ],
   "source": [
    "await handle_chat(conversation_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7cc26a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 17:05:05,507 - micro - MainProcess - INFO     [🛑] Stopping speech synthesis... (text_to_speech.py:stop_speaking:55)\n",
      "INFO:micro:[🛑] Stopping speech synthesis...\n"
     ]
    }
   ],
   "source": [
    "az_speach_synthesizer_client.stop_speaking()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audioagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
