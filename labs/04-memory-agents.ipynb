{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1eac7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed directory to: c:\\Users\\pablosal\\Desktop\\gbb-ai-audio-agent\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "# set the directory to the location of the script\n",
    "try:\n",
    "    os.chdir(\"..\")\n",
    "    target_directory = os.getenv(\n",
    "        \"TARGET_DIRECTORY\", os.getcwd()\n",
    "    )  # Use environment variable if available\n",
    "    if os.path.exists(target_directory):\n",
    "        os.chdir(target_directory)\n",
    "        print(f\"Changed directory to: {os.getcwd()}\")\n",
    "        logging.info(f\"Successfully changed directory to: {os.getcwd()}\")\n",
    "    else:\n",
    "        logging.error(f\"Directory does not exist: {target_directory}\")\n",
    "except Exception as e:\n",
    "    logging.exception(f\"An error occurred while changing directory: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f4ded9",
   "metadata": {},
   "source": [
    "## **🧠 Agent Memory in Our RTAgent Architecture**\n",
    "\n",
    "*Making conversations smarter, faster, and more personal*\n",
    "\n",
    "#### **Why Memory Matters**\n",
    "\n",
    "When agents forget recent or past user interactions, the user experience suffers. To build real-time, context-aware, and personalized AI agents, memory is separated into two types:\n",
    "\n",
    "- **Short-term memory:** Tracks current session context.\n",
    "- **Long-term memory:** Remembers previous sessions and user history.\n",
    "\n",
    "Both are essential for coherent, informed, and scalable agents.\n",
    "\n",
    "#### **Short-Term (Session) Memory**\n",
    "\n",
    "- **Purpose:** Tracks what’s happening now.\n",
    "- **Storage:** Redis (session-scoped, fast).\n",
    "- **Managed by:** ConversationManager or memory orchestrator.\n",
    "- **Stores:**  \n",
    "    - Recent conversation turns  \n",
    "    - Partial tool outputs  \n",
    "    - Temporary state (e.g., queue, auth status)\n",
    "- **Key Features:**  \n",
    "    - Expiry: TTL (e.g., 1–2 hours)  \n",
    "    - Isolation: Per session_id  \n",
    "    - Speed: Fastest lookup\n",
    "\n",
    "#### **Long-Term (User-Level) Memory**\n",
    "\n",
    "- **Purpose:** Remembers user history across sessions.\n",
    "- **Storage:** Azure Cosmos DB for MongoDB, vector-indexed.\n",
    "- **Stores:**  \n",
    "    - Conversation summaries  \n",
    "    - Extracted knowledge  \n",
    "    - User insights (preferences, topics)\n",
    "- **How:**  \n",
    "    - End-of-session: Summarize chat, generate embedding  \n",
    "    - Store in Cosmos DB with metadata (user_id, summary, vector, tags, timestamp)\n",
    "- **Usage:**  \n",
    "    - At session start: Embed user query, search Cosmos DB, return top-k relevant memories  \n",
    "    - Filter by metadata (e.g., category, status)\n",
    "\n",
    "#### **Memory Workflow**\n",
    "\n",
    "1. **During Session:**  \n",
    "     - Redis stores ongoing dialog and transient state.\n",
    "2. **End of Session:**  \n",
    "     - Summarize, embed, and store in CosmosDB.\n",
    "3. **Next Session Start:**  \n",
    "     - Embed user query, search summaries, inject top results into context.\n",
    "\n",
    "#### **Summary Table**\n",
    "\n",
    "| Memory Type | Location       |      Scope         | Lifetime         | Example Use                        |\n",
    "|-------------|----------------|--------------------|------------------|-------------------------------------|\n",
    "| Short-Term  | Redis          | Session-only  | TTL (e.g., 1hr)  | Track auth flow, tool state         |\n",
    "| Long-Term   | CosmosDB + Vectors  | Cross-session | Persistent       | Recall patient conditions, history  |\n",
    "\n",
    "#### **Engineering Best Practices**\n",
    "\n",
    "- Store user_id and session_id as metadata for secure, scoped retrieval.\n",
    "- Summarize long chats before embedding; avoid raw chat logs.\n",
    "- Use vector search with category filters for context-aware recall.\n",
    "- Prune or cluster older memories to maintain relevance.\n",
    "\n",
    "**This design ensures:**\n",
    "\n",
    "- Agents act coherently within sessions (Redis).\n",
    "- Agents stay informed across sessions (Cosmos vector memory).\n",
    "- Fast, scalable performance for production.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b1063f",
   "metadata": {},
   "source": [
    "###  **At Conversation End — Summarize & Store Memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6bba839",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 13:52:47,852 - micro - MainProcess - INFO     Azure Redis connection initialized with access key. (manager.py:_create_client:82)\n",
      "INFO:micro:Azure Redis connection initialized with access key.\n"
     ]
    }
   ],
   "source": [
    "from rtagents.RTMedAgent.backend.orchestration.conversation_state import (\n",
    "    ConversationManager,\n",
    ")\n",
    "from src.aoai.manager import AzureOpenAIManager\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# -----------------------------\n",
    "# Load Environment Variables\n",
    "# -----------------------------\n",
    "load_dotenv()\n",
    "\n",
    "# --- Azure OpenAI Setup ---\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_KEY = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "AZURE_OPENAI_DEPLOYMENT = \"text-embedding-3-small\"\n",
    "EMBED_MODEL = AZURE_OPENAI_DEPLOYMENT\n",
    "EMB_FIELD = \"contentVector\"\n",
    "MAX_DIM = 1536\n",
    "\n",
    "aoai_client = AzureOpenAIManager(\n",
    "    api_key=AZURE_OPENAI_KEY,\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    embedding_model_name=EMBED_MODEL,\n",
    ")\n",
    "from src.redis.manager import AzureRedisManager\n",
    "\n",
    "redis_manager = AzureRedisManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bfed482",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 13:52:50,942 - micro - MainProcess - INFO     Restored session 94eb58c3: 22 msgs total, ctx keys=['authenticated', 'active_agent', 'latency_roundtrip', 'tool_outputs', 'caller_name', 'policy_id', 'intake_completed'] (conversation_state.py:from_redis:48)\n",
      "INFO:micro:Restored session 94eb58c3: 22 msgs total, ctx keys=['authenticated', 'active_agent', 'latency_roundtrip', 'tool_outputs', 'caller_name', 'policy_id', 'intake_completed']\n"
     ]
    }
   ],
   "source": [
    "# Example retrieval from redis after session end\n",
    "SESSION_ID = \"94eb58c3\"\n",
    "cm = ConversationManager.from_redis(\"94eb58c3\", redis_manager)\n",
    "history = cm.full_history()\n",
    "context = cm.context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "709b6f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are an AI call wrap-up assistant.\n",
    "Your job is to produce a MEMORY record **as a single JSON block**.\n",
    "\n",
    "### INPUT\n",
    "- Full dialog turns (chronological):\n",
    "{history}\n",
    "- Session context:\n",
    "{context}\n",
    "\n",
    "### RULES\n",
    "1. **summary** → Maximum 3 plain-English sentences; capture only the final outcome and key details. Do not include details about successful authentication—only mention authentication if there are multiple failures.\n",
    "2. **sentiment** → \"positive\", \"neutral\", or \"negative\" based on the caller's mood and the agent's tone.\n",
    "3. **intent** → One of: \"authentication\", \"claim_filed\", \"claim_inquiry\", \"other\".\n",
    "4. **entities** → Extract if present: caller_name, policy_id, claim_id.\n",
    "5. Output **one valid JSON object** with keys:\n",
    "{{\n",
    "  \"summary\": \"\",\n",
    "  \"sentiment\": \"\",\n",
    "  \"intent\": \"\",\n",
    "  \"entities\": {{\n",
    "    \"caller_name\": \"\",\n",
    "    \"policy_id\": \"\",\n",
    "    \"claim_id\": \"\"\n",
    "  }}\n",
    "}}\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "839fc7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, openai, json, re\n",
    "\n",
    "def _strip_fence(txt: str) -> str:\n",
    "    \"\"\"Remove ```json … ``` fences if present.\"\"\"\n",
    "    return re.sub(r\"^```(?:json)?\\s*|\\s*```$\", \"\", txt.strip(), flags=re.I)\n",
    "\n",
    "async def build_memory(history, context, openai_client):\n",
    "    # Pretty-print history to avoid brace collisions\n",
    "    hist_str = json.dumps(history, indent=2, ensure_ascii=False)\n",
    "    ctx_str  = json.dumps(context, indent=2, ensure_ascii=False)\n",
    "\n",
    "    prompt = PROMPT_TEMPLATE.format(history=hist_str, context=ctx_str)\n",
    "\n",
    "    resp = await openai_client.generate_chat_response(\n",
    "        query=prompt,\n",
    "        conversation_history=[],\n",
    "        temperature=0.2\n",
    "    )\n",
    "\n",
    "    raw = resp[\"response\"]\n",
    "    try:\n",
    "        memory_json = json.loads(_strip_fence(raw))\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Model did not return valid JSON: {e}\\n---RAW---\\n{raw}\")\n",
    "\n",
    "    # Add technical metadata\n",
    "    memory_json.update(\n",
    "        user_id=context.get(\"caller_name\", \"unknown\"),\n",
    "        timestamp=datetime.datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\"\n",
    "    )\n",
    "    return memory_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b1c0b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 13:53:02,880 - micro - MainProcess - INFO     Function generate_chat_response started at 2025-06-16 13:53:02 (manager.py:generate_chat_response:536)\n",
      "INFO:micro:Function generate_chat_response started at 2025-06-16 13:53:02\n",
      "2025-06-16 13:53:02,882 - micro - MainProcess - INFO     Sending request to Azure OpenAI at 2025-06-16 13:53:02 (manager.py:generate_chat_response:593)\n",
      "INFO:micro:Sending request to Azure OpenAI at 2025-06-16 13:53:02\n",
      "2025-06-16 13:53:04,770 - micro - MainProcess - INFO     Function generate_chat_response finished at 2025-06-16 13:53:04 (Duration: 1.89 seconds) (manager.py:generate_chat_response:647)\n",
      "INFO:micro:Function generate_chat_response finished at 2025-06-16 13:53:04 (Duration: 1.89 seconds)\n"
     ]
    }
   ],
   "source": [
    "mem = await build_memory(history, context, aoai_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd9901ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'Alice Brown successfully filed a claim for a collision involving her blue Honda Civic. The incident occurred at an intersection near downtown Chicago, IL, with no injuries or additional property damage. The claim was recorded with ID CLA-2025-LZKBOY.',\n",
       " 'sentiment': 'positive',\n",
       " 'intent': 'claim_filed',\n",
       " 'entities': {'caller_name': 'Alice Brown',\n",
       "  'policy_id': 'POL-A10001',\n",
       "  'claim_id': 'CLA-2025-LZKBOY'},\n",
       " 'user_id': 'Alice Brown',\n",
       " 'timestamp': '2025-06-16T18:53:04Z'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb13065b",
   "metadata": {},
   "source": [
    "### **📌 Step 1: Setup and Insert Embedded Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afb46350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to MongoDB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pablosal\\AppData\\Local\\Temp\\ipykernel_31076\\1646640554.py:20: UserWarning: You appear to be connected to a CosmosDB cluster. For more information regarding feature compatibility and support please visit https://www.mongodb.com/supportability/cosmosdb\n",
      "  mongo_client = MongoClient(mongo_conn)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.parse\n",
    "from pymongo import MongoClient, errors\n",
    "\n",
    "# --- MongoDB (Cosmos) Setup ---\n",
    "COSMOS_MONGO_USER = os.getenv(\"COSMOS_MONGO_USER\")\n",
    "COSMOS_MONGO_PWD = os.getenv(\"COSMOS_MONGO_PWD\")\n",
    "COSMOS_MONGO_SERVER = os.getenv(\"COSMOS_MONGO_SERVER\")\n",
    "DB_NAME  = \"memorydb\"\n",
    "COLL    = \"long_term_memory\"\n",
    "\n",
    "# Format SRV URI\n",
    "mongo_conn = (\n",
    "    f\"mongodb+srv://{urllib.parse.quote(COSMOS_MONGO_USER)}:\"\n",
    "    f\"{urllib.parse.quote(COSMOS_MONGO_PWD)}@{COSMOS_MONGO_SERVER}\"\n",
    "    \"?tls=true&authMechanism=SCRAM-SHA-256&retrywrites=false&maxIdleTimeMS=120000\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    mongo_client = MongoClient(mongo_conn)\n",
    "    db = mongo_client[DB_NAME]\n",
    "    collection = db[COLL]\n",
    "    print(\"✅ Connected to MongoDB.\")\n",
    "except errors.ConnectionError as e:\n",
    "    raise RuntimeError(f\"❌ MongoDB connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60232dc",
   "metadata": {},
   "source": [
    "### **📌 Step 2: Create DiskANN Vector Index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d155a77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vector & helper indexes created.\n"
     ]
    }
   ],
   "source": [
    "# ── constants ───────────────────────────────────────────\n",
    "EMB_FIELD   = \"memoryVector\"     # vector field in new docs\n",
    "MAX_DIM     = 1536               # text-embedding-3-small\n",
    "COLL_NAME   = collection.name    # same handle you already opened\n",
    "\n",
    "try:\n",
    "    db.command({\n",
    "        \"createIndexes\": COLL_NAME,\n",
    "        \"indexes\": [\n",
    "            {\n",
    "                \"name\": \"diskann_memory_vec\",\n",
    "                \"key\":  { EMB_FIELD: \"cosmosSearch\" },\n",
    "                \"cosmosSearchOptions\": {\n",
    "                    \"kind\":        \"vector-diskann\",\n",
    "                    \"dimensions\":  MAX_DIM,\n",
    "                    \"similarity\":  \"COS\",\n",
    "                    \"maxDegree\":   32,\n",
    "                    \"lBuild\":      64\n",
    "                }\n",
    "            },\n",
    "            # ▸ lookup / filtering\n",
    "            { \"name\": \"uid_idx\",        \"key\": { \"user_id\": 1 } },          # filter per user\n",
    "            { \"name\": \"intent_idx\",     \"key\": { \"intent\": 1 } },           # optional filter\n",
    "            { \"name\": \"ts_desc_idx\",    \"key\": { \"timestamp\": -1 } }        # recent-first sort\n",
    "        ]\n",
    "    })\n",
    "    print(\"✅ Vector & helper indexes created.\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Failed to create indexes:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af42a94d",
   "metadata": {},
   "source": [
    "### **Step 3: Insert Embedded Docs with Metadata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9d7c856c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ memory stored: 685047335b1975313bb03aca\n"
     ]
    }
   ],
   "source": [
    "emb_resp = aoai_client.generate_embedding(\n",
    "    input_text = mem[\"summary\"],\n",
    ")\n",
    "vec = emb_resp.data[0].embedding\n",
    "if len(vec) != MAX_DIM:\n",
    "    raise ValueError(f\"Embedding dim mismatch: {len(vec)} (expected {MAX_DIM})\")\n",
    "\n",
    "# ── 2. attach vector + insert ───────────────────────────────────────\n",
    "mem[\"memoryVector\"] = vec           # field name matches the new index\n",
    "insert_result = collection.insert_one(mem)\n",
    "\n",
    "print(\"✅ memory stored:\", insert_result.inserted_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebcfb98",
   "metadata": {},
   "source": [
    "### **Step 4: Perform a Vector Search (Retrieval)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5043ab8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Relevant memories:\n",
      "- (claim_filed, positive, score 0.101) → Alice Brown successfully filed a claim for a collision involving her blue Honda Civic, which occurred at an intersection near downtown Chicago, IL. No injuries or additional property damage were reported. The claim was filed under policy ID POL-A10001 with claim ID CLA-2025-LZKBOY.\n"
     ]
    }
   ],
   "source": [
    "# ── make query vector ───────────────────────────────────────────────\n",
    "query   = \"Food recommendations for heart health\"\n",
    "resp    = aoai_client.generate_embedding(input_text=query)\n",
    "q_emb   = resp.data[0].embedding\n",
    "\n",
    "USER_ID = \"Alice Brown\"        # the current caller / session user\n",
    "TOP_K   = 3                    # how many memories to bring back\n",
    "\n",
    "# ── build aggregation pipeline ──────────────────────────────────────\n",
    "pipeline = [\n",
    "    {\n",
    "        \"$search\": {\n",
    "            \"cosmosSearch\": {\n",
    "                \"path\": \"memoryVector\",      # vector field in new docs\n",
    "                \"vector\": q_emb,\n",
    "                \"k\": TOP_K,\n",
    "                # return only this user’s memories\n",
    "                \"filter\": { \"user_id\": { \"$eq\": USER_ID } }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    # keep only the fields you need downstream\n",
    "    {\n",
    "        \"$project\": {\n",
    "            \"_id\": 0,\n",
    "            \"summary\":     1,\n",
    "            \"intent\":      1,\n",
    "            \"sentiment\":   1,\n",
    "            \"score\": { \"$meta\": \"searchScore\" },\n",
    "            \"timestamp\":   1\n",
    "        }\n",
    "    },\n",
    "    # (optional) prefer recent memories if scores tie\n",
    "    { \"$sort\": { \"score\": -1, \"timestamp\": -1 } }\n",
    "]\n",
    "\n",
    "results = list(collection.aggregate(pipeline))\n",
    "\n",
    "print(\"🔍 Relevant memories:\")\n",
    "for r in results:\n",
    "    print(f\"- ({r['intent']}, {r['sentiment']}, score {r['score']:.3f}) → {r['summary']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5360cb55",
   "metadata": {},
   "source": [
    "### **📌 Step 5: Inspect Indexes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b963a1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Current Indexes:\n",
      "SON([('v', 2), ('key', SON([('_id', 1)])), ('name', '_id_')])\n",
      "SON([('v', 2), ('key', SON([('memoryVector', 'cosmosSearch')])), ('name', 'diskann_memory_vec'), ('cosmosSearchOptions', SON([('kind', 'vector-diskann'), ('dimensions', 1536), ('similarity', 'COS'), ('maxDegree', 32), ('lBuild', 64)]))])\n",
      "SON([('v', 2), ('key', SON([('user_id', 1)])), ('name', 'uid_idx')])\n",
      "SON([('v', 2), ('key', SON([('intent', 1)])), ('name', 'intent_idx')])\n",
      "SON([('v', 2), ('key', SON([('timestamp', -1)])), ('name', 'ts_desc_idx')])\n"
     ]
    }
   ],
   "source": [
    "print(\"📄 Current Indexes:\")\n",
    "for idx in collection.list_indexes():\n",
    "    print(idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d021f39",
   "metadata": {},
   "source": [
    "### **📌 Step 6: Use Client**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0483ccde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pablosal\\Desktop\\gbb-ai-audio-agent\\src\\cosmosdb\\manager.py:32: UserWarning: You appear to be connected to a CosmosDB cluster. For more information regarding feature compatibility and support please visit https://www.mongodb.com/supportability/cosmosdb\n",
      "  self.client = pymongo.MongoClient(connection_string)\n"
     ]
    }
   ],
   "source": [
    "from src.cosmosdb.manager import CosmosDBMongoCoreManager\n",
    "import urllib.parse\n",
    "\n",
    "# --- MongoDB (Cosmos) Setup ---\n",
    "COSMOS_MONGO_USER = os.getenv(\"COSMOS_MONGO_USER\")\n",
    "COSMOS_MONGO_PWD = os.getenv(\"COSMOS_MONGO_PWD\")\n",
    "COSMOS_MONGO_SERVER = os.getenv(\"COSMOS_MONGO_SERVER\")\n",
    "DB_NAME  = \"memorydb\"\n",
    "COLL    = \"long_term_memory\"\n",
    "\n",
    "# Format SRV URI\n",
    "mongo_conn = (\n",
    "    f\"mongodb+srv://{urllib.parse.quote(COSMOS_MONGO_USER)}:\"\n",
    "    f\"{urllib.parse.quote(COSMOS_MONGO_PWD)}@{COSMOS_MONGO_SERVER}\"\n",
    "    \"?tls=true&authMechanism=SCRAM-SHA-256&retrywrites=false&maxIdleTimeMS=120000\"\n",
    ")\n",
    "\n",
    "mgr = CosmosDBMongoCoreManager(connection_string=mongo_conn, database_name=DB_NAME, collection_name=COLL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a7d5376",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:src.cosmosdb.manager:Duplicate key error while inserting document: Duplicate key violation on the requested collection: Index '_id_', full error: {'index': 0, 'code': 11000, 'errmsg': \"Duplicate key violation on the requested collection: Index '_id_'\"}\n"
     ]
    }
   ],
   "source": [
    "# 1️⃣ one-time index bootstrap\n",
    "mgr.ensure_index_from_yaml(\"rtagents/RTInsuranceAgent/backend/agents/memory_store/vector_index.yaml\")\n",
    "\n",
    "# 2️⃣ insert your memory doc (uses existing insert_document)\n",
    "mgr.insert_document(mem)         # mem includes memoryVector already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d0caad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3️⃣ later: retrieve\n",
    "results = mgr.semantic_search(\n",
    "    query_text  = \"collision involving her blue Honda Civic\",\n",
    "    user_id     = \"Alice Brown\",\n",
    "    aoai_client = aoai_client,\n",
    "    top_k       = 5\n",
    ")\n",
    "\n",
    "for r in results:\n",
    "    print(json.dumps(r, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audioagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
