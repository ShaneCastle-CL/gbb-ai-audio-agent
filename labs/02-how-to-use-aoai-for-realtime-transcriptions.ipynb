{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab0cebd6",
   "metadata": {},
   "source": [
    "## **Real-time Speech Transcription with GPT-4o-transcribe**\n",
    "\n",
    "Azure OpenAI now supports blazing fast, real-time speech recognition with two models:\n",
    "\n",
    "- **GPT-4o-transcribe**: High accuracy, full power  \n",
    "- **GPT-4o-mini-transcribe**: Super fast, lighter, lower latency\n",
    "\n",
    "Both use WebSocket connections for live, streaming transcription‚Äîperfect for any app that needs instant speech-to-text.\n",
    "\n",
    "### **How It Works**\n",
    "\n",
    "1. **Audio Ingestion**\n",
    "    - Capture audio from your device mic, phone, or browser\n",
    "    - Or stream audio from another service (e.g., Azure Communication Services - ACS)\n",
    "\n",
    "2. **Stream Audio to Azure OpenAI**\n",
    "    - Open a WebSocket to the GPT-4o-transcribe endpoint\n",
    "    - Push audio chunks as you record‚Äîno need to wait for the whole file\n",
    "\n",
    "3. **Get Real-Time Transcripts**\n",
    "    - Receive live text output‚Äîsee it as you speak\n",
    "    - Handle partial (interim) and final (confirmed) results\n",
    "\n",
    "4. **Do More with Your Transcripts**\n",
    "    - Feed transcripts to your LLM for intent, function calling, or downstream processing\n",
    "    - Send back to the user as captions, messages, or voice\n",
    "\n",
    "This Notebooks will guide you to laverage *Real-time Speech Transcription*\n",
    "\n",
    "- **From Your Phone(Local)**: Record and stream your voice straight to Azure for live transcription\n",
    "- **From Other Apps (e.g., ACS)**: Pipe in audio streams from calls, meetings, or bots‚Äîtranscribe them in real-time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d474318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "import asyncio\n",
    "import wave\n",
    "from datetime import datetime\n",
    "from typing import Optional, Callable, Dict, Any\n",
    "\n",
    "import pyaudio\n",
    "import websockets\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396e3e32",
   "metadata": {},
   "source": [
    "## **Record and stream your voice straight to Azure for live transcription**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1db1c4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_audio_input_devices() -> None:\n",
    "    \"\"\"\n",
    "    Print all available input devices (microphones) for user selection.\n",
    "    \"\"\"\n",
    "    p = pyaudio.PyAudio()\n",
    "    print(\"\\nAvailable audio input devices:\")\n",
    "    for i in range(p.get_device_count()):\n",
    "        dev = p.get_device_info_by_index(i)\n",
    "        if dev[\"maxInputChannels\"] > 0:\n",
    "            print(f\"{i}: {dev['name']}\")\n",
    "    p.terminate()\n",
    "\n",
    "\n",
    "def choose_audio_device(predefined_index: int = None) -> int:\n",
    "    \"\"\"\n",
    "    Return the index of the selected audio input device.\n",
    "    If predefined_index is provided and valid, use it.\n",
    "    Otherwise, prompt user if multiple devices are available.\n",
    "    \"\"\"\n",
    "    p = pyaudio.PyAudio()\n",
    "    try:\n",
    "        mic_indices = [\n",
    "            i\n",
    "            for i in range(p.get_device_count())\n",
    "            if p.get_device_info_by_index(i)[\"maxInputChannels\"] > 0\n",
    "        ]\n",
    "        if not mic_indices:\n",
    "            raise RuntimeError(\"‚ùå No audio input (microphone) devices found.\")\n",
    "\n",
    "        if predefined_index is not None:\n",
    "            if predefined_index in mic_indices:\n",
    "                print(f\"üé§ Using predefined audio input device: {predefined_index}\")\n",
    "                return predefined_index\n",
    "            else:\n",
    "                print(f\"Provided index {predefined_index} is not a valid input device.\")\n",
    "\n",
    "        if len(mic_indices) == 1:\n",
    "            print(f\"üé§ Only one audio input device found: {mic_indices[0]}\")\n",
    "            return mic_indices[0]\n",
    "\n",
    "        print(\"Available audio input devices:\")\n",
    "        for idx in mic_indices:\n",
    "            info = p.get_device_info_by_index(idx)\n",
    "            print(f\"  [{idx}]: {info['name']}\")\n",
    "        while True:\n",
    "            try:\n",
    "                selection = input(\n",
    "                    f\"Select audio input device index [{mic_indices[0]}]: \"\n",
    "                ).strip()\n",
    "                if selection == \"\":\n",
    "                    return mic_indices[0]\n",
    "                selected_index = int(selection)\n",
    "                if selected_index in mic_indices:\n",
    "                    return selected_index\n",
    "                print(\n",
    "                    f\"Index {selected_index} is not valid. Please choose from {mic_indices}.\"\n",
    "                )\n",
    "            except ValueError:\n",
    "                print(\"Invalid input. Please enter a valid integer index.\")\n",
    "\n",
    "    finally:\n",
    "        p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4edd8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioRecorder:\n",
    "    \"\"\"\n",
    "    Async audio recorder using PyAudio.\n",
    "    Allows independent recording (to memory and .wav) and streaming (for STT).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        rate: int,\n",
    "        channels: int,\n",
    "        format_: int,\n",
    "        chunk: int,\n",
    "        device_index: Optional[int] = None,\n",
    "    ):\n",
    "        self.rate = rate\n",
    "        self.channels = channels\n",
    "        self.format = format_\n",
    "        self.chunk = chunk\n",
    "        self.device_index = (\n",
    "            device_index if device_index is not None else choose_audio_device()\n",
    "        )\n",
    "        self.p = pyaudio.PyAudio()\n",
    "        self.stream = None\n",
    "        self.frames = []\n",
    "        self.audio_queue: asyncio.Queue[bytes] = asyncio.Queue()\n",
    "        self._loop = asyncio.get_event_loop()\n",
    "        self._running = False\n",
    "\n",
    "    def start(self) -> None:\n",
    "        \"\"\"\n",
    "        Start the audio stream and begin capturing to the queue.\n",
    "        \"\"\"\n",
    "\n",
    "        def callback(in_data, frame_count, time_info, status):\n",
    "            self.frames.append(in_data)\n",
    "            self._loop.call_soon_threadsafe(self.audio_queue.put_nowait, in_data)\n",
    "            return (None, pyaudio.paContinue)\n",
    "\n",
    "        self.stream = self.p.open(\n",
    "            format=self.format,\n",
    "            channels=self.channels,\n",
    "            rate=self.rate,\n",
    "            input=True,\n",
    "            input_device_index=self.device_index,\n",
    "            frames_per_buffer=self.chunk,\n",
    "            stream_callback=callback,\n",
    "        )\n",
    "        self._running = True\n",
    "        self.stream.start_stream()\n",
    "\n",
    "    def stop(self) -> None:\n",
    "        \"\"\"\n",
    "        Stop and close the stream, release audio resources.\n",
    "        \"\"\"\n",
    "        self._running = False\n",
    "        if self.stream is not None:\n",
    "            self.stream.stop_stream()\n",
    "            self.stream.close()\n",
    "        self.p.terminate()\n",
    "\n",
    "    def save_wav(self, filename: str) -> None:\n",
    "        \"\"\"\n",
    "        Save the recorded audio to a .wav file.\n",
    "        Ensures there is audio data before saving.\n",
    "        Creates the output directory if it does not exist.\n",
    "        \"\"\"\n",
    "        if not self.frames:\n",
    "            print(\"‚ö†Ô∏è No audio recorded. Nothing to save.\")\n",
    "            return\n",
    "        directory = os.path.dirname(filename)\n",
    "        if directory and not os.path.exists(directory):\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "        wf = wave.open(filename, \"wb\")\n",
    "        wf.setnchannels(self.channels)\n",
    "        wf.setsampwidth(self.p.get_sample_size(self.format))\n",
    "        wf.setframerate(self.rate)\n",
    "        wf.writeframes(b\"\".join(self.frames))\n",
    "        wf.close()\n",
    "        print(f\"üéôÔ∏è Audio saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "633c4bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranscriptionClient:\n",
    "    \"\"\"\n",
    "    Handles async websocket transcription session to Azure OpenAI STT.\n",
    "    Can be used independently: just supply an async generator of audio chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        url: str,\n",
    "        headers: dict,\n",
    "        session_config: Dict[str, Any],\n",
    "        on_delta: Optional[Callable[[str], None]] = None,\n",
    "        on_transcript: Optional[Callable[[str], None]] = None,\n",
    "    ):\n",
    "        self.url = url\n",
    "        self.headers = headers\n",
    "        self.session_config = session_config\n",
    "        self.ws: Optional[websockets.WebSocketClientProtocol] = None\n",
    "        self._on_delta = on_delta\n",
    "        self._on_transcript = on_transcript\n",
    "        self._running = False\n",
    "        self._send_task = None\n",
    "        self._recv_task = None\n",
    "\n",
    "    async def __aenter__(self):\n",
    "        try:\n",
    "            self.ws = await websockets.connect(\n",
    "                self.url, additional_headers=self.headers\n",
    "            )\n",
    "        except TypeError:\n",
    "            self.ws = await websockets.connect(self.url, extra_headers=self.headers)\n",
    "        self._running = True\n",
    "        return self\n",
    "\n",
    "    async def __aexit__(self, exc_type, exc, tb):\n",
    "        self._running = False\n",
    "        if self.ws:\n",
    "            await self.ws.close()\n",
    "        if self._send_task:\n",
    "            self._send_task.cancel()\n",
    "        if self._recv_task:\n",
    "            self._recv_task.cancel()\n",
    "\n",
    "    async def send_json(self, data: dict) -> None:\n",
    "        if self.ws:\n",
    "            await self.ws.send(json.dumps(data))\n",
    "\n",
    "    async def send_audio_chunk(self, audio_data: bytes) -> None:\n",
    "        audio_base64 = base64.b64encode(audio_data).decode(\"utf-8\")\n",
    "        await self.send_json(\n",
    "            {\"type\": \"input_audio_buffer.append\", \"audio\": audio_base64}\n",
    "        )\n",
    "\n",
    "    async def start_session(self, rate: int, channels: int) -> None:\n",
    "        session_config = {\n",
    "            \"type\": \"transcription_session.update\",\n",
    "            \"session\": self.session_config,\n",
    "        }\n",
    "        await self.send_json(session_config)\n",
    "        await self.send_json(\n",
    "            {\n",
    "                \"type\": \"audio_start\",\n",
    "                \"data\": {\"encoding\": \"pcm\", \"sample_rate\": rate, \"channels\": channels},\n",
    "            }\n",
    "        )\n",
    "\n",
    "    async def receive_loop(self) -> None:\n",
    "        async for message in self.ws:\n",
    "            try:\n",
    "                data = json.loads(message)\n",
    "                event_type = data.get(\"type\", \"\")\n",
    "                if event_type == \"conversation.item.input_audio_transcription.delta\":\n",
    "                    delta = data.get(\"delta\", \"\")\n",
    "                    if delta and self._on_delta:\n",
    "                        self._on_delta(delta)\n",
    "                elif (\n",
    "                    event_type\n",
    "                    == \"conversation.item.input_audio_transcription.completed\"\n",
    "                ):\n",
    "                    transcript = data.get(\"transcript\", \"\")\n",
    "                    if transcript and self._on_transcript:\n",
    "                        self._on_transcript(transcript)\n",
    "                elif event_type == \"conversation.item.created\":\n",
    "                    transcript = data.get(\"item\", \"\")\n",
    "                    if (\n",
    "                        isinstance(transcript, dict)\n",
    "                        and \"content\" in transcript\n",
    "                        and transcript[\"content\"]\n",
    "                    ):\n",
    "                        t = transcript[\"content\"][0].get(\"transcript\")\n",
    "                        if t and self._on_transcript:\n",
    "                            self._on_transcript(t)\n",
    "                    elif transcript and self._on_transcript:\n",
    "                        self._on_transcript(str(transcript))\n",
    "            except Exception as e:\n",
    "                print(\"‚ùå Error parsing message:\", e)\n",
    "\n",
    "    async def run(self, audio_chunk_iter: asyncio.Queue, rate: int, channels: int):\n",
    "        \"\"\"\n",
    "        Main loop: configure session, send audio from queue, receive results.\n",
    "        \"\"\"\n",
    "        await self.start_session(rate, channels)\n",
    "        self._send_task = asyncio.create_task(self._send_audio_loop(audio_chunk_iter))\n",
    "        self._recv_task = asyncio.create_task(self.receive_loop())\n",
    "        done, pending = await asyncio.wait(\n",
    "            [self._send_task, self._recv_task], return_when=asyncio.FIRST_COMPLETED\n",
    "        )\n",
    "        for task in pending:\n",
    "            task.cancel()\n",
    "\n",
    "    async def _send_audio_loop(self, audio_queue: asyncio.Queue):\n",
    "        while self._running:\n",
    "            try:\n",
    "                audio_data = await audio_queue.get()\n",
    "                if audio_data is None:\n",
    "                    break\n",
    "                await self.send_audio_chunk(audio_data)\n",
    "            except asyncio.CancelledError:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3724fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioTranscriber:\n",
    "    \"\"\"\n",
    "    High-level orchestrator for audio recording and real-time transcription.\n",
    "    Use as: record only, transcribe only, or chain both (record+transcribe).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        url: str,\n",
    "        headers: dict,\n",
    "        rate: int,\n",
    "        channels: int,\n",
    "        format_: int,\n",
    "        chunk: int,\n",
    "        device_index: Optional[int] = None,\n",
    "    ):\n",
    "        self.url = url\n",
    "        self.headers = headers\n",
    "        self.rate = rate\n",
    "        self.channels = channels\n",
    "        self.format = format_\n",
    "        self.chunk = chunk\n",
    "        self.device_index = device_index\n",
    "\n",
    "    async def record(\n",
    "        self, duration: Optional[float] = None, output_file: Optional[str] = None\n",
    "    ) -> AudioRecorder:\n",
    "        \"\"\"\n",
    "        Record audio from mic. Returns AudioRecorder.\n",
    "        Optionally, specify duration (seconds). Use output_file to auto-save.\n",
    "        \"\"\"\n",
    "        recorder = AudioRecorder(\n",
    "            rate=self.rate,\n",
    "            channels=self.channels,\n",
    "            format_=self.format,\n",
    "            chunk=self.chunk,\n",
    "            device_index=self.device_index,\n",
    "        )\n",
    "        recorder.start()\n",
    "        print(\n",
    "            f\"Recording{' for ' + str(duration) + ' seconds' if duration else ' (Ctrl+C to stop)'}...\"\n",
    "        )\n",
    "        try:\n",
    "            if duration:\n",
    "                await asyncio.sleep(duration)\n",
    "            else:\n",
    "                while True:\n",
    "                    await asyncio.sleep(0.5)\n",
    "        except (KeyboardInterrupt, asyncio.CancelledError):\n",
    "            pass\n",
    "        finally:\n",
    "            recorder.stop()\n",
    "            if output_file:\n",
    "                recorder.save_wav(output_file)\n",
    "        return recorder\n",
    "\n",
    "    async def transcribe(\n",
    "        self,\n",
    "        audio_queue: Optional[asyncio.Queue] = None,\n",
    "        model: str = \"gpt-4o-transcribe\",\n",
    "        prompt: Optional[str] = \"Respond in English.\",\n",
    "        language: Optional[str] = None,\n",
    "        noise_reduction: str = \"near_field\",\n",
    "        vad_type: str = \"server_vad\",\n",
    "        vad_config: Optional[dict] = None,\n",
    "        on_delta: Optional[Callable[[str], None]] = None,\n",
    "        on_transcript: Optional[Callable[[str], None]] = None,\n",
    "        output_wav_file: Optional[str] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Run a transcription session with full model/config control.\n",
    "\n",
    "        If audio_queue is None, creates and uses a live AudioRecorder.\n",
    "\n",
    "        Args:\n",
    "            audio_queue: Asyncio queue containing audio chunks to transcribe.\n",
    "            model: Transcription model to use.\n",
    "            prompt: Custom prompt for the model.\n",
    "            language: Language hint for recognition.\n",
    "            noise_reduction: Type of noise reduction.\n",
    "            vad_type: Voice activity detection type.\n",
    "            vad_config: Config dict for VAD.\n",
    "            on_delta: Callback for interim results.\n",
    "            on_transcript: Callback for final results.\n",
    "            output_wav_file: Filename for saving raw microphone audio (if recording).\n",
    "        \"\"\"\n",
    "        recorder = None\n",
    "        if audio_queue is None:\n",
    "            recorder = AudioRecorder(\n",
    "                rate=self.rate,\n",
    "                channels=self.channels,\n",
    "                format_=self.format,\n",
    "                chunk=self.chunk,\n",
    "                device_index=self.device_index,\n",
    "            )\n",
    "            recorder.start()\n",
    "            audio_queue = recorder.audio_queue\n",
    "\n",
    "        session_config = {\n",
    "            \"input_audio_format\": \"pcm16\",\n",
    "            \"input_audio_transcription\": {\n",
    "                \"model\": model,\n",
    "                \"prompt\": prompt,\n",
    "            },\n",
    "            \"input_audio_noise_reduction\": {\"type\": noise_reduction},\n",
    "            \"turn_detection\": {\"type\": vad_type} if vad_type else None,\n",
    "        }\n",
    "        if vad_config:\n",
    "            session_config[\"turn_detection\"].update(vad_config)\n",
    "        if language:\n",
    "            session_config[\"input_audio_transcription\"][\"language\"] = language\n",
    "\n",
    "            \n",
    "\n",
    "        async with TranscriptionClient(\n",
    "            self.url, self.headers, session_config, on_delta, on_transcript\n",
    "        ) as client:\n",
    "            try:\n",
    "                await client.run(audio_queue, self.rate, self.channels)\n",
    "            except asyncio.CancelledError:\n",
    "                print(\"Transcription cancelled.\")\n",
    "            finally:\n",
    "                if recorder:\n",
    "                    recorder.stop()\n",
    "                    if output_wav_file is None:\n",
    "                        # Default to timestamped file if not provided\n",
    "                        output_wav_file = (\n",
    "                            f\"microphone_capture_{datetime.now():%Y%m%d_%H%M%S}.wav\"\n",
    "                        )\n",
    "                    recorder.save_wav(output_wav_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca05061d",
   "metadata": {},
   "source": [
    "#### **üé§ Real-time Audio Transcription with Azure OpenAI & Microphone Input**\n",
    "\n",
    "Stream your voice, get instant captions, and interact with state-of-the-art transcription AI‚Äîall in real time!\n",
    "\n",
    "**How Does It Work?**\n",
    "\n",
    "- **AudioRecorder**  \n",
    "    Captures audio from your microphone, streams it chunk-by-chunk to a queue, and can save the recording as a `.wav` file.\n",
    "- **TranscriptionClient**  \n",
    "    Connects to Azure OpenAI via WebSocket, manages the transcription session, streams audio, and receives live transcription results. Calls your handler functions for real-time feedback.\n",
    "- **AudioTranscriber**  \n",
    "    The high-level orchestrator. Chains recording and transcription together, so you can focus on your workflow‚Äînot the plumbing.\n",
    "\n",
    "**Result:**  \n",
    "Speak into your mic and watch as your words are transcribed live, with both interim and final results‚Äîperfect for captions, notes, or downstream AI processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8f798bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import pyaudio\n",
    "from dotenv import load_dotenv\n",
    "from typing import Optional\n",
    "\n",
    "# Audio configuration constants\n",
    "RATE = 24000\n",
    "CHANNELS = 1\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHUNK = 1024\n",
    "AUDIO_INDEX = 0\n",
    "OUTPUT_AUDIO_FILE = \"recordings/test/microphone_output.wav\"\n",
    "\n",
    "\n",
    "def get_env_variable(name: str) -> str:\n",
    "    \"\"\"Get environment variable or raise RuntimeError if missing.\"\"\"\n",
    "    value = os.environ.get(name)\n",
    "    if not value:\n",
    "        raise RuntimeError(f\"‚ùå Required environment variable '{name}' is missing.\")\n",
    "    return value\n",
    "\n",
    "\n",
    "async def main() -> None:\n",
    "    \"\"\"\n",
    "    Main entry point for real-time transcription.\n",
    "    Loads environment, configures audio, and starts transcription session.\n",
    "    \"\"\"\n",
    "    load_dotenv()\n",
    "    try:\n",
    "        OPENAI_API_KEY = get_env_variable(\"AZURE_OPENAI_STT_TTS_KEY\")\n",
    "        AZURE_OPENAI_ENDPOINT = get_env_variable(\"AZURE_OPENAI_STT_TTS_ENDPOINT\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "        return\n",
    "\n",
    "    url = f\"{AZURE_OPENAI_ENDPOINT.replace('https', 'wss')}/openai/realtime?api-version=2025-04-01-preview&intent=transcription\"\n",
    "    headers = {\"api-key\": OPENAI_API_KEY}\n",
    "    device_index = choose_audio_device(AUDIO_INDEX)\n",
    "\n",
    "    transcriber = AudioTranscriber(\n",
    "        url=url,\n",
    "        headers=headers,\n",
    "        rate=RATE,\n",
    "        channels=CHANNELS,\n",
    "        format_=FORMAT,\n",
    "        chunk=CHUNK,\n",
    "        device_index=device_index,\n",
    "    )\n",
    "\n",
    "    def print_delta(delta: str):\n",
    "        \"\"\"Prints incremental transcription results.\"\"\"\n",
    "        print(delta, end=\" \", flush=True)\n",
    "\n",
    "    def print_transcript(transcript: str):\n",
    "        \"\"\"Prints the final transcript.\"\"\"\n",
    "        print(f\"\\n‚úÖ Transcript: {transcript}\")\n",
    "\n",
    "    print(\">>> Starting real-time transcription session. Press Ctrl+C to stop.\")\n",
    "    try:\n",
    "        await transcriber.transcribe(\n",
    "            model=\"gpt-4o-transcribe\",\n",
    "            prompt=\"Respond in English. This is a medical environment.\",\n",
    "            noise_reduction=\"near_field\",\n",
    "            vad_type=\"server_vad\",\n",
    "            vad_config={\n",
    "                \"threshold\": 0.5,\n",
    "                \"prefix_padding_ms\": 300,\n",
    "                \"silence_duration_ms\": 1000,\n",
    "            },\n",
    "            on_delta=print_delta,\n",
    "            on_transcript=print_transcript,\n",
    "            output_wav_file=OUTPUT_AUDIO_FILE,\n",
    "        )\n",
    "    except (KeyboardInterrupt, asyncio.CancelledError):\n",
    "        print(\"\\nüõë Interrupted by user. Exiting...\")\n",
    "    except Exception as ex:\n",
    "        print(f\"\\n‚ùå Error: {ex}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855ecc7f",
   "metadata": {},
   "source": [
    "#### **How the code above works?**\n",
    "\n",
    "**Configure Audio:**  \n",
    "Select your microphone (default or prompt).  \n",
    "Settings: `RATE=24000`, `CHANNELS=1`, `FORMAT=pyaudio.paInt16`, `CHUNK=1024`.\n",
    "\n",
    "**WebSocket URL:**  \n",
    "Convert endpoint to `wss://.../openai/realtime?...` for Azure real-time transcription.  \n",
    "Add `intent=transcription` and API version in the URL.\n",
    "\n",
    "**AudioTranscriber:**  \n",
    "Acts as the high-level orchestrator:  \n",
    "- Records from the mic  \n",
    "- Streams audio chunks  \n",
    "- Pipes data to Azure OpenAI in real time\n",
    "\n",
    "**Handlers:**  \n",
    "- `print_delta`: Prints live streaming text as you speak  \n",
    "- `print_transcript`: Prints final transcript (with ‚úÖ)\n",
    "\n",
    "**Run Session:**  \n",
    "- Notify user that transcription is starting (`Ctrl+C` to exit)\n",
    "- Start the transcriber with:\n",
    "    - Model: `gpt-4o-transcribe`\n",
    "    - Prompt: (e.g., \"Respond in English. This is a medical environment.\")\n",
    "    - Noise reduction: enabled\n",
    "    - Voice Activity Detection (VAD): enabled, with custom settings\n",
    "    - Pass in the handlers for real-time feedback\n",
    "\n",
    "**Streaming Process:**  \n",
    "- Microphone records your voice  \n",
    "- Audio is streamed in small chunks instantly to Azure OpenAI via WebSocket  \n",
    "- Azure transcribes in real time, sending:\n",
    "    - Deltas (partial text as you speak)\n",
    "    - Final transcript (when the speech is complete)\n",
    "- Handlers print both live and final results to your terminal\n",
    "\n",
    "**Exit Gracefully:**  \n",
    "- On `Ctrl+C`, the session stops and resources are released cleanly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cf60104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé§ Using predefined audio input device: 0\n",
      ">>> Starting real-time transcription session. Press Ctrl+C to stop.\n",
      "\n",
      "‚úÖ Transcript: Hey Heather, this is Pablo, are you working in my region?\n",
      "\n",
      "‚úÖ Transcript: Oh yeah, it seems they are working.\n",
      "Transcription cancelled.\n",
      "üéôÔ∏è Audio saved to recordings/test/microphone_output.wav\n"
     ]
    }
   ],
   "source": [
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc49fe0b",
   "metadata": {},
   "source": [
    "### **From Other Apps (e.g., ACS)**: Pipe in audio streams from calls, meetings, or bots‚Äîtranscribe them in real-time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301babd8",
   "metadata": {},
   "source": [
    "```python\n",
    "@app.websocket(ACS_WEBSOCKET_PATH)\n",
    "async def acs_websocket_endpoint(websocket: WebSocket):\n",
    "    \"\"\"Handles the bidirectional audio stream for an ACS call, using AOAI streaming STT, and records audio as a WAV file.\"\"\"\n",
    "    acs_caller_instance = app.state.acs_caller\n",
    "\n",
    "    if not acs_caller_instance:\n",
    "        logger.error(\"ACS Caller not available. Cannot process ACS audio.\")\n",
    "        return\n",
    "\n",
    "    await websocket.accept()\n",
    "    call_connection_id = websocket.headers.get(\"x-ms-call-connection-id\", \"UnknownCall\")\n",
    "    logger.info(f\"‚ñ∂ ACS media WebSocket accepted for call {call_connection_id}\")\n",
    "\n",
    "    cm = ConversationManager(auth=False)\n",
    "    cm.cid = call_connection_id\n",
    "\n",
    "    # --- AOAI Streaming Setup ---\n",
    "    AOAI_STT_KEY = os.environ.get(\"AZURE_OPENAI_STT_TTS_KEY\")\n",
    "    AOAI_STT_ENDPOINT = os.environ.get(\"AZURE_OPENAI_STT_TTS_ENDPOINT\")\n",
    "    aoai_url = f\"{AOAI_STT_ENDPOINT.replace('https', 'wss')}/openai/realtime?api-version=2025-04-01-preview&intent=transcription\"\n",
    "    aoai_headers = {\"api-key\": AOAI_STT_KEY}\n",
    "    RATE = 16000\n",
    "    CHANNELS = 1\n",
    "    FORMAT = 16  # PCM16\n",
    "\n",
    "    audio_queue = asyncio.Queue()\n",
    "\n",
    "    async def on_delta(delta: str):\n",
    "        await broadcast_message(delta, \"User\")\n",
    "\n",
    "    async def on_transcript(transcript: str):\n",
    "        logger.info(f\"[AOAI-Transcript] üé§üé∂üéßüìº {transcript}\")\n",
    "        await broadcast_message(transcript, \"User\")\n",
    "        await process_gpt_response(cm, transcript, websocket, is_acs=True)\n",
    "\n",
    "    # --- Open WAV file for writing ---\n",
    "    wav_filename = f\"acs_audio_{call_connection_id}.wav\"\n",
    "    wav_file = wave.open(wav_filename, \"wb\")\n",
    "    wav_file.setnchannels(CHANNELS)\n",
    "    wav_file.setsampwidth(2)  # 16-bit PCM = 2 bytes\n",
    "    wav_file.setframerate(RATE)\n",
    "\n",
    "    async def record_audio_chunk(audio_bytes: bytes):\n",
    "        wav_file.writeframes(audio_bytes)\n",
    "\n",
    "    transcriber = AudioTranscriber(\n",
    "        url=aoai_url,\n",
    "        headers=aoai_headers,\n",
    "        rate=RATE,\n",
    "        channels=CHANNELS,\n",
    "        format_=FORMAT,\n",
    "        chunk=1024,\n",
    "        device_index=None,\n",
    "    )\n",
    "    transcribe_task = asyncio.create_task(\n",
    "        transcriber.transcribe(\n",
    "            audio_queue=audio_queue,\n",
    "            model=\"gpt-4o-transcribe\",\n",
    "            prompt=\"Respond in English. This is a medical environment.\",\n",
    "            noise_reduction=\"near_field\",\n",
    "            vad_type=\"server_vad\",\n",
    "            vad_config={\n",
    "                \"threshold\": 0.5,\n",
    "                \"prefix_padding_ms\": 300,\n",
    "                \"silence_duration_ms\": 2000,\n",
    "            },\n",
    "            on_delta=lambda delta: asyncio.create_task(on_delta(delta)),\n",
    "            on_transcript=lambda t: asyncio.create_task(on_transcript(t)),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    greeted_call_ids = app.state.greeted_call_ids\n",
    "    if (\n",
    "        call_connection_id != \"UnknownCall\"\n",
    "        and call_connection_id not in greeted_call_ids\n",
    "    ):\n",
    "        initial_greeting = \"Hello from XMYX Healthcare Company! Before I can assist you, let‚Äôs verify your identity. How may I address you?\"\n",
    "        logger.info(f\"Playing initial greeting for call {call_connection_id}\")\n",
    "        await broadcast_message(initial_greeting, \"Assistant\")\n",
    "        await send_response_to_acs(websocket, initial_greeting)\n",
    "        cm.hist.append({\"role\": \"assistant\", \"content\": initial_greeting})\n",
    "        greeted_call_ids.add(call_connection_id)\n",
    "    else:\n",
    "        logger.info(\n",
    "            f\"Skipping initial greeting for already greeted call {call_connection_id}\"\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            try:\n",
    "                raw_data = await asyncio.wait_for(websocket.receive_text(), timeout=5.0)\n",
    "                data = json.loads(raw_data)\n",
    "            except asyncio.TimeoutError:\n",
    "                if websocket.client_state != WebSocketState.CONNECTED:\n",
    "                    logger.warning(\n",
    "                        f\"ACS WebSocket {call_connection_id} disconnected while waiting for data.\"\n",
    "                    )\n",
    "                    break\n",
    "                continue\n",
    "            except WebSocketDisconnect:\n",
    "                logger.info(f\"ACS WebSocket disconnected for call {call_connection_id}\")\n",
    "                break\n",
    "            except json.JSONDecodeError:\n",
    "                logger.warning(\n",
    "                    f\"Received invalid JSON from ACS for call {call_connection_id}\"\n",
    "                )\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                logger.error(\n",
    "                    f\"Error receiving from ACS WebSocket {call_connection_id}: {e}\",\n",
    "                    exc_info=True,\n",
    "                )\n",
    "                break\n",
    "\n",
    "            kind = data.get(\"kind\")\n",
    "            if kind == \"AudioData\":\n",
    "                b64 = data.get(\"audioData\", {}).get(\"data\")\n",
    "                if b64:\n",
    "                    audio_bytes = base64.b64decode(b64)\n",
    "                    await audio_queue.put(audio_bytes)  # AOAI streaming\n",
    "                    await record_audio_chunk(audio_bytes)  # Write to .wav file\n",
    "            elif kind == \"CallConnected\":\n",
    "                connected_participant_id = (\n",
    "                    data.get(\"callConnected\", {}).get(\"participant\", {}).get(\"rawID\")\n",
    "                )\n",
    "                if (\n",
    "                    connected_participant_id\n",
    "                    and call_connection_id not in call_user_raw_ids\n",
    "                ):\n",
    "                    call_user_raw_ids[call_connection_id] = connected_participant_id\n",
    "            elif kind in (\"PlayCompleted\", \"PlayFailed\", \"PlayCanceled\"):\n",
    "                logger.info(\n",
    "                    f\"Received {kind} event via WebSocket for call {call_connection_id}\"\n",
    "                )\n",
    "\n",
    "    except WebSocketDisconnect:\n",
    "        logger.info(f\"ACS WebSocket {call_connection_id} disconnected.\")\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            f\"Unhandled error in ACS WebSocket handler for call {call_connection_id}: {e}\",\n",
    "            exc_info=True,\n",
    "        )\n",
    "    finally:\n",
    "        logger.info(\n",
    "            f\"üßπ Cleaning up ACS WebSocket handler for call {call_connection_id}.\"\n",
    "        )\n",
    "        await audio_queue.put(None)  # End audio for AOAI transcriber\n",
    "        await transcribe_task         # Flush all transcripts\n",
    "\n",
    "        try:\n",
    "            wav_file.close()  # <--- IMPORTANT: Close file so it's readable!\n",
    "            logger.info(f\"WAV file closed: {wav_filename}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to close WAV file: {e}\")\n",
    "\n",
    "        if websocket.client_state == WebSocketState.CONNECTED:\n",
    "            await websocket.close()\n",
    "            logger.info(\n",
    "                f\"ACS WebSocket connection closed for call {call_connection_id}\"\n",
    "            )\n",
    "        if call_connection_id in call_user_raw_ids:\n",
    "            try:\n",
    "                del call_user_raw_ids[call_connection_id]\n",
    "                logger.info(f\"Removed call ID mapping for {call_connection_id}\")\n",
    "            except KeyError:\n",
    "                logger.warning(\n",
    "                    f\"Call ID mapping for {call_connection_id} already removed.\"\n",
    "                )\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fb8c33",
   "metadata": {},
   "source": [
    "#### **Let's understand code above**\n",
    "\n",
    "**1. WebSocket Connection from ACS:**  \n",
    "ACS calls your `/websocket` endpoint and streams audio (base64 in JSON) and call events.\n",
    "\n",
    "```python\n",
    "@app.websocket(ACS_WEBSOCKET_PATH)\n",
    "async def acs_websocket_endpoint(websocket: WebSocket):\n",
    "    await websocket.accept()\n",
    "    # ... initialization ...\n",
    "```\n",
    "**2. Audio & Session Setup**\n",
    "\n",
    "Create an async queue for streaming audio chunks.\n",
    "Prepare a .wav file for raw audio storage.\n",
    "Set up callbacks for delta/final transcript results.\n",
    "\n",
    "```python\n",
    "audio_queue = asyncio.Queue()\n",
    "wav_file = wave.open(wav_filename, \"wb\")\n",
    "wav_file.setnchannels(CHANNELS)\n",
    "wav_file.setsampwidth(2)  # PCM16 = 2 bytes\n",
    "wav_file.setframerate(RATE)\n",
    "```\n",
    "\n",
    "**3. AOAI Transcriber Setup**\n",
    "Instantiate AudioTranscriber for AOAI real-time endpoint over WebSocket.\n",
    "Start as async task, streaming from audio_queue.\n",
    "\n",
    "```python\n",
    "transcriber = AudioTranscriber(\n",
    "    url=aoai_url,\n",
    "    headers=aoai_headers,\n",
    "    rate=RATE,\n",
    "    channels=CHANNELS,\n",
    "    format_=FORMAT,\n",
    "    chunk=1024,\n",
    "    device_index=None,\n",
    ")\n",
    "transcribe_task = asyncio.create_task(\n",
    "    transcriber.transcribe(\n",
    "        audio_queue=audio_queue,\n",
    "        model=\"gpt-4o-transcribe\",\n",
    "        prompt=\"Respond in English. This is a medical environment.\",\n",
    "        noise_reduction=\"near_field\",\n",
    "        vad_type=\"server_vad\",\n",
    "        vad_config={\"threshold\": 0.5, \"prefix_padding_ms\": 300, \"silence_duration_ms\": 2000},\n",
    "        on_delta=lambda delta: asyncio.create_task(on_delta(delta)),\n",
    "        on_transcript=lambda t: asyncio.create_task(on_transcript(t)),\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "**4.Receiving Audio from ACS:**\n",
    "Main loop parses incoming ACS JSON:\n",
    "If \"AudioData\", decode, push to audio_queue, and write to .wav.\n",
    "If event, handle accordingly.\n",
    "\n",
    "```python\n",
    "if kind == \"AudioData\":\n",
    "    b64 = data.get(\"audioData\", {}).get(\"data\")\n",
    "    if b64:\n",
    "        audio_bytes = base64.b64decode(b64)\n",
    "        await audio_queue.put(audio_bytes)    # Stream to AOAI\n",
    "        await record_audio_chunk(audio_bytes) # Write to .wav\n",
    "```\n",
    "\n",
    "**5.Real-Time Streaming to AOAI:**\n",
    "Audio in audio_queue is streamed to AOAI.\n",
    "AOAI returns:\n",
    "- Deltas: Partial text updates\n",
    "+ Final transcripts: Confirmed utterances - Handlers forward both to the client/chat.\n",
    "\n",
    "```python\n",
    "async def on_delta(delta: str):\n",
    "    await broadcast_message(delta, \"User\")\n",
    "\n",
    "async def on_transcript(transcript: str):\n",
    "    await broadcast_message(transcript, \"User\")\n",
    "    await process_gpt_response(cm, transcript, websocket, is_acs=True)\n",
    "```\n",
    "\n",
    "**6.Session Cleanup**\n",
    "On disconnect/error, end AOAI stream, wait for task to finish, and close .wav file.\n",
    "\n",
    "```python\n",
    "finally:\n",
    "    await audio_queue.put(None)   # Signal end to AOAI\n",
    "    await transcribe_task         # Wait for completion\n",
    "    wav_file.close()   \n",
    "```           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24f9be97",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pyaudio.PyAudio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e37d4667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.get_sample_size(pyaudio.paInt16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48f8a939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "import threading\n",
    "import pyaudio\n",
    "import websocket\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\")  # Load environment variables from .env\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get(\"AZURE_OPENAI_STT_TTS_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise RuntimeError(\"‚ùå OPENAI_API_KEY is missing!\")\n",
    "\n",
    "# WebSocket endpoint for OpenAI Realtime API (transcription model)\n",
    "url = f\"{os.environ.get('AZURE_OPENAI_STT_TTS_ENDPOINT').replace('https', 'wss')}/openai/realtime?api-version=2025-04-01-preview&intent=transcription\"\n",
    "headers = { \"api-key\": OPENAI_API_KEY}\n",
    "# Audio stream parameters (16-bit PCM, 16kHz mono)\n",
    "RATE = 24000\n",
    "CHANNELS = 1\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHUNK = 1024\n",
    "\n",
    "audio_interface = pyaudio.PyAudio()\n",
    "stream = audio_interface.open(format=FORMAT,\n",
    "                              channels=CHANNELS,\n",
    "                              rate=RATE,\n",
    "                              input=True,\n",
    "                              frames_per_buffer=CHUNK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "568cf338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_open(ws):\n",
    "    print(\"Connected! Start speaking...\")\n",
    "    session_config = {\n",
    "        \"type\": \"transcription_session.update\",\n",
    "        \"session\": {\n",
    "            \"input_audio_format\": \"pcm16\",\n",
    "            \"input_audio_transcription\": {\n",
    "                \"model\": \"gpt-4o-mini-transcribe\",\n",
    "                \"prompt\": \"Respond in English.\"\n",
    "            },\n",
    "            #\"input_audio_noise_reduction\": {\"type\": \"near_field\"},\n",
    "            \"turn_detection\": {\"type\": \"server_vad\", \"threshold\": 0.5, \"prefix_padding_ms\": 300, \"silence_duration_ms\": 200}\n",
    "        }\n",
    "    }\n",
    "    ws.send(json.dumps(session_config))\n",
    "\n",
    "    def stream_microphone():\n",
    "        try:\n",
    "            while ws.keep_running:\n",
    "                audio_data = stream.read(CHUNK, exception_on_overflow=False)\n",
    "                audio_base64 = base64.b64encode(audio_data).decode('utf-8')\n",
    "                ws.send(json.dumps({\n",
    "                    \"type\": \"input_audio_buffer.append\",\n",
    "                    \"audio\": audio_base64\n",
    "                }))\n",
    "        except Exception as e:\n",
    "            print(\"Audio streaming error:\", e)\n",
    "            ws.close()\n",
    "\n",
    "    threading.Thread(target=stream_microphone, daemon=True).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "099ea9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_message(ws, message):\n",
    "    try:\n",
    "        data = json.loads(message)\n",
    "        event_type = data.get(\"type\", \"\")\n",
    "        print(\"Event type:\", event_type)\n",
    "        #print(data)   \n",
    "        # Stream live incremental transcripts\n",
    "        if event_type == \"conversation.item.input_audio_transcription.delta\":\n",
    "            transcript_piece = data.get(\"delta\", \"\")\n",
    "            if transcript_piece:\n",
    "                print(transcript_piece, end=' ', flush=True)\n",
    "        if event_type == \"conversation.item.input_audio_transcription.completed\":\n",
    "            print(data[\"transcript\"])\n",
    "        if event_type == \"item\":\n",
    "            transcript = data.get(\"item\", \"\")\n",
    "            if transcript:\n",
    "                print(\"\\nFinal transcript:\", transcript)\n",
    "\n",
    "    except Exception:\n",
    "        pass  # Ignore unrelated events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f4b97f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_error(ws, error):\n",
    "    print(\"WebSocket error:\", error)\n",
    "\n",
    "def on_close(ws, close_status_code, close_msg):\n",
    "    print(\"Disconnected from server.\")\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio_interface.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0a369c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to OpenAI Realtime API...\n",
      "Connected! Start speaking...\n",
      "Event type: transcription_session.created\n",
      "Event type: transcription_session.updated\n",
      "Event type: input_audio_buffer.speech_started\n",
      "Event type: input_audio_buffer.speech_stopped\n",
      "Event type: input_audio_buffer.committed\n",
      "Event type: conversation.item.created\n",
      "Event type: conversation.item.input_audio_transcription.failed\n",
      "WebSocket error: \n",
      "Disconnected from server.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Connecting to OpenAI Realtime API...\")\n",
    "ws_app = websocket.WebSocketApp(\n",
    "    url,\n",
    "    header=headers,\n",
    "    on_open=on_open,\n",
    "    on_message=on_message,\n",
    "    on_error=on_error,\n",
    "    on_close=on_close\n",
    ")\n",
    "\n",
    "ws_app.run_forever()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audioagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
