{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed102ccb",
   "metadata": {},
   "source": [
    "## **Testing Audio Devices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available audio devices:\n",
      "0: Microsoft Sound Mapper - Input\n",
      "1: Surface Stereo Microphones (Sur\n",
      "2: Microphone (Lumina Camera - Raw\n",
      "3: Echo Cancelling Speakerphone (M\n",
      "4: Microsoft Sound Mapper - Output\n",
      "5: Surface Omnisonic Speakers (Sur\n",
      "6: Speakers (Dell USB Audio)\n",
      "7: Echo Cancelling Speakerphone (M\n",
      "8: Primary Sound Capture Driver\n",
      "9: Surface Stereo Microphones (Surface High Definition Audio)\n",
      "10: Microphone (Lumina Camera - Raw)\n",
      "11: Echo Cancelling Speakerphone (Microsoft Audio Dock)\n",
      "12: Primary Sound Driver\n",
      "13: Surface Omnisonic Speakers (Surface High Definition Audio)\n",
      "14: Speakers (Dell USB Audio)\n",
      "15: Echo Cancelling Speakerphone (Microsoft Audio Dock)\n",
      "16: Speakers (Dell USB Audio)\n",
      "17: Surface Omnisonic Speakers (Surface High Definition Audio)\n",
      "18: Echo Cancelling Speakerphone (Microsoft Audio Dock)\n",
      "19: Surface Stereo Microphones (Surface High Definition Audio)\n",
      "20: Microphone (Lumina Camera - Raw)\n",
      "21: Echo Cancelling Speakerphone (Microsoft Audio Dock)\n",
      "22: Headphones ()\n",
      "23: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Shiva’s AirPods Pro #2))\n",
      "24: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Shiva’s AirPods Pro #2))\n",
      "25: Speakers (Dell USB Audio)\n",
      "26: Microphone (Dell USB Audio)\n",
      "27: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Shiva’s AirPods Pro #2 - Find My))\n",
      "28: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Shiva’s AirPods Pro #2 - Find My))\n",
      "29: Headphones 1 (Realtek HD Audio 2nd output with SST)\n",
      "30: Headphones 2 (Realtek HD Audio 2nd output with SST)\n",
      "31: PC Speaker (Realtek HD Audio 2nd output with SST)\n",
      "32: Speakers 1 (Realtek HD Audio output with SST)\n",
      "33: Speakers 2 (Realtek HD Audio output with SST)\n",
      "34: PC Speaker (Realtek HD Audio output with SST)\n",
      "35: Microphone Array (Realtek HD Audio Mic input)\n",
      "36: Headset Microphone (Headset Microphone)\n",
      "37: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Pablo’s AirPods #3))\n",
      "38: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Pablo’s AirPods #3))\n",
      "39: Headphones ()\n",
      "40: Headphones ()\n",
      "41: Headphones ()\n",
      "42: Microphone (Lumina Camera - Raw)\n",
      "43: Echo Cancelling Speakerphone (Microsoft Audio Dock)\n",
      "44: Echo Cancelling Speakerphone (Microsoft Audio Dock)\n",
      "45: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Pablo’s AirPods Pro - Find My))\n",
      "46: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Pablo’s AirPods Pro - Find My))\n",
      "47: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Pablo’s AirPods #4 - Find My))\n",
      "48: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Pablo’s AirPods #4 - Find My))\n",
      "49: Headphones ()\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "\n",
    "def list_audio_devices():\n",
    "    \"\"\"\n",
    "    List all available audio devices using PyAudio.\n",
    "\n",
    "    This function initializes PyAudio, retrieves the list of audio devices,\n",
    "    and prints their names. It also includes error handling to ensure proper\n",
    "    cleanup of resources.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        p = pyaudio.PyAudio()\n",
    "        print(\"Available audio devices:\")\n",
    "        for ii in range(p.get_device_count()):\n",
    "            device_name = p.get_device_info_by_index(ii).get('name')\n",
    "            print(f\"{ii}: {device_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while listing audio devices: {e}\")\n",
    "    finally:\n",
    "        # Ensure PyAudio resources are released\n",
    "        if 'p' in locals():\n",
    "            p.terminate()\n",
    "\n",
    "# Call the function to list audio devices\n",
    "list_audio_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e9ade96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording complete. Saving audio...\n",
      "Audio saved to test_audio.wav. Playing back...\n",
      "Playback complete.\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "def test_microphone():\n",
    "    \"\"\"\n",
    "    Test the microphone by recording audio and playing it back.\n",
    "\n",
    "    This function captures audio from the default input device (microphone),\n",
    "    saves it to a temporary WAV file, and plays it back to ensure the microphone\n",
    "    is working correctly.\n",
    "    \"\"\"\n",
    "    # Audio configuration\n",
    "    chunk = 1024  # Number of frames per buffer\n",
    "    format = pyaudio.paInt16  # 16-bit audio format\n",
    "    channels = 1  # Mono audio\n",
    "    rate = 44100  # Sampling rate (44.1 kHz)\n",
    "    record_seconds = 5  # Duration of the recording\n",
    "    output_filename = \"test_audio.wav\"\n",
    "\n",
    "    # Initialize PyAudio\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    try:\n",
    "        # Open the microphone stream\n",
    "        print(\"Recording...\")\n",
    "        stream = p.open(format=format,\n",
    "                        channels=channels,\n",
    "                        rate=rate,\n",
    "                        input=True,\n",
    "                        frames_per_buffer=chunk)\n",
    "\n",
    "        frames = []\n",
    "\n",
    "        # Record audio in chunks\n",
    "        for _ in range(0, int(rate / chunk * record_seconds)):\n",
    "            data = stream.read(chunk)\n",
    "            frames.append(data)\n",
    "\n",
    "        print(\"Recording complete. Saving audio...\")\n",
    "\n",
    "        # Save the recorded audio to a WAV file\n",
    "        with wave.open(output_filename, 'wb') as wf:\n",
    "            wf.setnchannels(channels)\n",
    "            wf.setsampwidth(p.get_sample_size(format))\n",
    "            wf.setframerate(rate)\n",
    "            wf.writeframes(b''.join(frames))\n",
    "\n",
    "        print(f\"Audio saved to {output_filename}. Playing back...\")\n",
    "\n",
    "        # Play back the recorded audio\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "\n",
    "        # Open the WAV file for playback\n",
    "        wf = wave.open(output_filename, 'rb')\n",
    "        playback_stream = p.open(format=p.get_format_from_width(wf.getsampwidth()),\n",
    "                                 channels=wf.getnchannels(),\n",
    "                                 rate=wf.getframerate(),\n",
    "                                 output=True)\n",
    "\n",
    "        # Read and play audio data\n",
    "        data = wf.readframes(chunk)\n",
    "        while data:\n",
    "            playback_stream.write(data)\n",
    "            data = wf.readframes(chunk)\n",
    "\n",
    "        playback_stream.stop_stream()\n",
    "        playback_stream.close()\n",
    "\n",
    "        print(\"Playback complete.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Terminate PyAudio\n",
    "        p.terminate()\n",
    "\n",
    "# Run the microphone test\n",
    "test_microphone()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e9da16",
   "metadata": {},
   "source": [
    "## **Define Clients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c3f52ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "import os\n",
    "# set the directory to the location of the script\n",
    "try:\n",
    "    target_directory = os.getenv(\"TARGET_DIRECTORY\", os.getcwd())  # Use environment variable if available\n",
    "    if os.path.exists(target_directory):\n",
    "        os.chdir(target_directory)\n",
    "        logging.info(f\"Successfully changed directory to: {os.getcwd()}\")\n",
    "    else:\n",
    "        logging.error(f\"Directory does not exist: {target_directory}\")\n",
    "except Exception as e:\n",
    "    logging.exception(f\"An error occurred while changing directory: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8a11c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.speech.speech_recognizer import SpeechRecognizer, StreamingSpeechRecognizer\n",
    "from src.speech.text_to_speech import SpeechSynthesizer\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Ensure clients are initialized only if not already defined\n",
    "if 'az_speech_recognizer_client' not in locals():\n",
    "    az_speech_recognizer_client = SpeechRecognizer()\n",
    "\n",
    "if 'az_speech_recognizer_stream_client' not in locals():\n",
    "    az_speech_recognizer_stream_client = StreamingSpeechRecognizer()\n",
    "\n",
    "if 'az_speach_synthesizer_client' not in locals():\n",
    "    az_speach_synthesizer_client = SpeechSynthesizer()\n",
    "\n",
    "# Ensure Azure OpenAI client is initialized only if not already defined\n",
    "if 'client' not in locals():\n",
    "    client = AzureOpenAI(\n",
    "        api_version=\"2025-02-01-preview\",\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4ee563",
   "metadata": {},
   "source": [
    "## **Azure AI Speech**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96a82c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 13:34:56,351 - micro - MainProcess - INFO     Starting continuous speech recognition with VAD... (speech_recognizer.py:start:140)\n",
      "INFO:micro:Starting continuous speech recognition with VAD...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial: hey my name\n",
      "Partial: hey my name is\n",
      "Partial: hey my name is pab\n",
      "Partial: hey my name is pablo\n",
      "Partial: hey my name is pablo how\n",
      "Partial: hey my name is pablo how are you doing\n",
      "Partial: hey my name is pablo how are you doing i'm\n",
      "Partial: hey my name is pablo how are you doing i'm doing\n",
      "Partial: hey my name is pablo how are you doing i'm doing good\n",
      "Partial: hey my name is pablo how are you doing i'm doing good hey how\n",
      "Partial: hey my name is pablo how are you doing i'm doing good hey how are you doing\n",
      "Partial: hey my name is pablo how are you doing i'm doing good hey how are you doing pablo\n",
      "Partial: hey my name is pablo how are you doing i'm doing good hey how are you doing pablo are you\n",
      "Partial: hey my name is pablo how are you doing i'm doing good hey how are you doing pablo are you doing well\n",
      "Partial: hey my name is pablo how are you doing i'm doing good hey how are you doing pablo are you doing well very\n",
      "Partial: hey my name is pablo how are you doing i'm doing good hey how are you doing pablo are you doing well very good\n",
      "Partial: hey my name is pablo how are you doing i'm doing good hey how are you doing pablo are you doing well very good very\n",
      "Partial: hey my name is pablo how are you doing i'm doing good hey how are you doing pablo are you doing well very good very good very good\n",
      "Partial: hey my name is pablo how are you doing i'm doing good hey how are you doing pablo are you doing well very good very good very good very good\n",
      "Partial: hey my name is pablo how are you doing i'm doing good hey how are you doing pablo are you doing well very good very good very good very good pretty good\n",
      "Partial: hey my name is pablo how are you doing i'm doing good hey how are you doing pablo are you doing well very good very good very good very good pretty good can you\n",
      "Partial: hey my name is pablo how are you doing i'm doing good hey how are you doing pablo are you doing well very good very good very good very good pretty good can you just\n",
      "Partial: hey my name is pablo how are you doing i'm doing good hey how are you doing pablo are you doing well very good very good very good very good pretty good can you just gonna stop\n",
      "Final: Hey, my name is Pablo. How are you doing? I'm doing good. Hey, how are you doing? Pablo, Are you doing well? Very good. Very good. Very good. Very good. Pretty good. I'm just gonna stop.\n"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "recognizer = StreamingSpeechRecognizer(\n",
    "    vad_silence_timeout_ms=1000\n",
    ")\n",
    "\n",
    "# Buffers\n",
    "all_text_live = \"\"\n",
    "final_transcripts = []\n",
    "\n",
    "# Internal tracker to prevent double-processing\n",
    "last_final_text = None\n",
    "\n",
    "# Callbacks\n",
    "def on_partial(text: str):\n",
    "    global all_text_live\n",
    "    print(f\"Partial: {text}\")\n",
    "    all_text_live = text\n",
    "\n",
    "def on_final(text: str):\n",
    "    global all_text_live, last_final_text\n",
    "\n",
    "    # Skip duplicate finalizations\n",
    "    if text == last_final_text:\n",
    "        return\n",
    "\n",
    "    last_final_text = text\n",
    "\n",
    "    print(f\"Final: {text}\")\n",
    "    final_transcripts.append(text)\n",
    "    all_text_live = \"\"  # Clear the live buffer\n",
    "\n",
    "# Attach\n",
    "recognizer.set_partial_result_callback(on_partial)\n",
    "recognizer.set_final_result_callback(on_final)\n",
    "\n",
    "# Start recognizing\n",
    "recognizer.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04ebc8a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hey, my name is Pablo. How are you doing? I'm doing good. Hey, how are you doing? Pablo, Are you doing well? Very good. Very good. Very good. Very good. Pretty good. I'm just gonna stop. \""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_conversation = \" \".join(final_transcripts) + \" \" + all_text_live\n",
    "full_conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "698b9a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 13:35:13,200 - micro - MainProcess - INFO     Stopping continuous speech recognition... (speech_recognizer.py:stop:186)\n",
      "INFO:micro:Stopping continuous speech recognition...\n",
      "2025-04-11 13:35:13,242 - micro - MainProcess - INFO     Session stopped: SessionEventArgs(session_id=f87bce662afe4fd3a464c1c571bec2ff) (speech_recognizer.py:_on_session_stopped:216)\n",
      "INFO:micro:Session stopped: SessionEventArgs(session_id=f87bce662afe4fd3a464c1c571bec2ff)\n"
     ]
    }
   ],
   "source": [
    "# Start recognizing\n",
    "recognizer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21400ef9",
   "metadata": {},
   "source": [
    "## **Streaming Text with gpt4o-mini**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d92bad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Pablo! I'm glad to hear you're doing well. I'm here and ready to help with anything you need! What’s on your mind?"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    stream=True,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f'{full_conversation}',\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=4096,\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    model=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    ")\n",
    "full_response = \"\"\n",
    "for update in response:\n",
    "    if update.choices:\n",
    "        chunk = update.choices[0].delta.content or \"\"\n",
    "        full_response += chunk\n",
    "        print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1486daef",
   "metadata": {},
   "source": [
    "## **Speach to Text - Return the Audio to the User**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82328088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 10:40:27,455 - micro - MainProcess - INFO     [🔊] Starting streaming speech synthesis for text: I'm glad to hear you're doing ... (text_to_speech.py:start_speaking_text:44)\n",
      "2025-04-10 10:40:31,457 - micro - MainProcess - INFO     [🛑] Stopping speech synthesis... (text_to_speech.py:stop_speaking:55)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "az_speach_synthesizer_client.start_speaking_text(\n",
    "                    text=full_response,\n",
    "                )\n",
    "time.sleep(4)\n",
    "az_speach_synthesizer_client.stop_speaking()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894c6a37",
   "metadata": {},
   "source": [
    "## **Streaming Audio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d764bd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 10:42:11,624 - micro - MainProcess - INFO     [🔊] Starting streaming speech synthesis for text: I'm doing well, thank you!... (text_to_speech.py:start_speaking_text:44)\n",
      "2025-04-10 10:42:12,107 - micro - MainProcess - INFO     [🔊] Starting streaming speech synthesis for text: When you mention \"the best rou... (text_to_speech.py:start_speaking_text:44)\n",
      "2025-04-10 10:42:12,114 - micro - MainProcess - INFO     [🔊] Starting streaming speech synthesis for text: Could you provide a bit more c... (text_to_speech.py:start_speaking_text:44)\n",
      "2025-04-10 10:42:12,123 - micro - MainProcess - INFO     [🔊] Starting streaming speech synthesis for text: This way, I can give you more ... (text_to_speech.py:start_speaking_text:44)\n"
     ]
    }
   ],
   "source": [
    "tts_sentence_end = [ \".\", \"!\", \"?\", \";\", \"。\", \"！\", \"？\", \"；\", \"\\n\" ]\n",
    "completion = client.chat.completions.create(\n",
    "    stream=True,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f'{full_conversation}',\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=4096,\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    model=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    ")\n",
    "\n",
    "collected_messages = []\n",
    "last_tts_request = None\n",
    "\n",
    "for chunk in completion:\n",
    "    if len(chunk.choices) > 0:\n",
    "        chunk_text = chunk.choices[0].delta.content\n",
    "        if chunk_text:\n",
    "            collected_messages.append(chunk_text)\n",
    "            if chunk_text in tts_sentence_end:\n",
    "                text = \"\".join(collected_messages).strip()\n",
    "                last_tts_request = az_speach_synthesizer_client.start_speaking_text(text)\n",
    "                collected_messages.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fffdc7b",
   "metadata": {},
   "source": [
    "## **Adding Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a19e4ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Templates found: ['voice_agent_system.jinja', 'voice_agent_user.jinja']\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "from app.backend.prompt_manager import PromptManager\n",
    "\n",
    "prompt_manager = PromptManager()\n",
    "systemp_prompt = prompt_manager.get_prompt(\"voice_agent_system.jinja\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0c217c",
   "metadata": {},
   "source": [
    "## **Adding Tools**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e66946b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.backend.tools import available_tools\n",
    "from app.backend.functions import (\n",
    "    schedule_appointment,\n",
    "    refill_prescription,\n",
    "    lookup_medication_info,\n",
    "    evaluate_prior_authorization,\n",
    "    escalate_emergency,\n",
    "    authenticate_user\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0da14961",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history: List[Dict[str, str]] = [\n",
    "    {\"role\": \"system\", \"content\": systemp_prompt},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello, my name is Pablo Salvador. \"\n",
    "        \"this is an emergency. I need to schedule an appointment with my doctor ASAP.\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f201ef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping tool names to actual Python async functions\n",
    "function_mapping = {\n",
    "    \"schedule_appointment\": schedule_appointment,\n",
    "    \"refill_prescription\": refill_prescription,\n",
    "    \"lookup_medication_info\": lookup_medication_info,\n",
    "    \"evaluate_prior_authorization\": evaluate_prior_authorization,\n",
    "    \"escalate_emergency\": escalate_emergency,\n",
    "    \"authenticate_user\": authenticate_user,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2716c04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's response:\n",
      "ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_9gnGBAJNhJ0EwW6KEx5xK81K', function=Function(arguments='{\"reason\":\"urgent appointment needed\"}', name='escalate_emergency'), type='function')])\n",
      "Function result: 🚨 Emergency escalation triggered: urgent appointment needed. A human healthcare agent is now being connected.\n",
      "Final response:\n",
      "Hello Pablo, I understand this is an emergency, and I've escalated your request to a human healthcare agent who will assist you right away. Please hold on for a moment while they connect with you. Your health and safety are our top priority.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "        messages=conversation_history,\n",
    "        tools=available_tools,\n",
    "        tool_choice=\"auto\",\n",
    "        max_tokens=4096,\n",
    "        temperature=0.5,\n",
    "        top_p=1.0,\n",
    "        model=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    "    )\n",
    "\n",
    "# Process the model's response\n",
    "response_message = response.choices[0].message\n",
    "conversation_history.append(response_message)\n",
    "\n",
    "print(\"Model's response:\")  \n",
    "print(response_message)  \n",
    "\n",
    "if response_message.tool_calls:\n",
    "    for tool_call in response_message.tool_calls:\n",
    "        function_name = tool_call.function.name\n",
    "        function_args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "        # Check if the function name is in the mapping\n",
    "        if function_name in function_mapping:\n",
    "            # Call the corresponding Python async function\n",
    "            result = await function_mapping[function_name](**function_args)\n",
    "            print(f\"Function result: {result}\")  \n",
    "            conversation_history.append({\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"role\": \"tool\",\n",
    "                \"name\": function_name,\n",
    "                \"content\": result,\n",
    "            })\n",
    "else:\n",
    "    print(\"No tool calls were made by the model.\")\n",
    "\n",
    "# Second API call: Get the final response from the model\n",
    "final_response = client.chat.completions.create(\n",
    "    messages=conversation_history,\n",
    "    max_tokens=4096,\n",
    "    temperature=0.5,\n",
    "    top_p=1.0,\n",
    "    model=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    ")   \n",
    "\n",
    "print(\"Final response:\")\n",
    "print(final_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734714e0",
   "metadata": {},
   "source": [
    "## **Making Tools work in Streaming Fashion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfe00db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history: List[Dict[str, str]] = [\n",
    "    {\"role\": \"system\", \"content\": systemp_prompt},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello, my name is Pablo Salvador. \"\n",
    "        \"this is an emergency. I need to schedule an appointment with my doctor ASAP.\",\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f55da9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=0, id='call_xkkR6tXUIgsxnHIz7RwaCN28', function=ChoiceDeltaToolCallFunction(arguments='', name='escalate_emergency'), type='function')])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='{\"', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='reason', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\":\"', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='Urg', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='ent', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' appointment', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' request', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' from', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' the', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' user', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='.\"', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='}', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=None)\n",
      "tool_name:escalate_emergency\n",
      "tool_id:call_xkkR6tXUIgsxnHIz7RwaCN28\n",
      "tool_call_accumulator:{\"reason\":\"Urgent appointment request from the user.\"}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'function_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Check if the function name is in the mapping\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tool_name \u001b[38;5;129;01min\u001b[39;00m function_mapping:\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# Call the corresponding Python async function\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m function_mapping[\u001b[43mfunction_name\u001b[49m](**function_args)\n\u001b[32m     36\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFunction result: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)  \n\u001b[32m     37\u001b[39m     conversation_history.append({\n\u001b[32m     38\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtool_call_id\u001b[39m\u001b[33m\"\u001b[39m: tool_id,\n\u001b[32m     39\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtool\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     40\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: tool_name,\n\u001b[32m     41\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: result,\n\u001b[32m     42\u001b[39m     })\n",
      "\u001b[31mNameError\u001b[39m: name 'function_name' is not defined"
     ]
    }
   ],
   "source": [
    "# initial user message    \n",
    "tool_call_accumulator = \"\"\n",
    "\n",
    "# First API call: Ask the model to use the function\n",
    "response = client.chat.completions.create(\n",
    "    model= os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    "    messages=conversation_history,\n",
    "    tools=available_tools,\n",
    "    tool_choice=\"auto\",\n",
    "    stream=True  # this time, we set stream=True\n",
    ")\n",
    "\n",
    "# process the model \n",
    "for chunk in response:\n",
    "    if len(chunk.choices) > 0:\n",
    "        delta = chunk.choices[0].delta\n",
    "        print(f\"delta: {delta}\") # print delta from the chunk for learning\n",
    "        \n",
    "        if delta.tool_calls:\n",
    "                if(delta.tool_calls[0].function.name):\n",
    "                    tool_name = chunk.choices[0].delta.tool_calls[0].function.name\n",
    "                    tool_id = chunk.choices[0].delta.tool_calls[0].id\n",
    "                    conversation_history.append(delta)\n",
    "                \n",
    "                if(chunk.choices[0].delta.tool_calls[0].function.arguments):    \n",
    "                    tool_call_accumulator+=delta.tool_calls[0].function.arguments\n",
    "\n",
    "# print function related outputs                    \n",
    "print(f\"tool_name:{tool_name}\")   \n",
    "print(f\"tool_id:{tool_id}\")    \n",
    "print(f\"tool_call_accumulator:{tool_call_accumulator}\")   \n",
    "# Check if the function name is in the mapping\n",
    "if tool_name in function_mapping:\n",
    "    # Call the corresponding Python async function\n",
    "    result = await function_mapping[function_name](**function_args)\n",
    "    print(f\"Function result: {result}\")  \n",
    "    conversation_history.append({\n",
    "        \"tool_call_id\": tool_id,\n",
    "        \"role\": \"tool\",\n",
    "        \"name\": tool_name,\n",
    "        \"content\": result,\n",
    "    })\n",
    "\n",
    "#print(f\"messages: {messages}\") \n",
    "print(\"=============================================\")  \n",
    "    \n",
    "#  second API call: Get the final response with stream\n",
    "response = client.chat.completions.create(\n",
    "    model= os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    "    messages=conversation_history,\n",
    "    temperature=0.7,\n",
    "    stream=True  # this time, we set stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    if len(chunk.choices) > 0:\n",
    "        delta = chunk.choices[0].delta\n",
    "        #print(f\"delta: {delta}\")\n",
    "        if delta.content:\n",
    "            print(delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d636a8d5",
   "metadata": {},
   "source": [
    "## **Run the Conversation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0de55282",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history: List[Dict[str, str]] = [\n",
    "    {\"role\": \"system\", \"content\": systemp_prompt},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello, my name is Pablo Salvador. \"\n",
    "        \"this is an emergency. I need to schedule an appointment with my doctor ASAP.\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f5c9d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history: List[Dict[str, str]] = [\n",
    "    {\"role\": \"system\", \"content\": systemp_prompt},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello, my name is Pablo Salvador. \"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea4f10c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "tts_sentence_end = [\".\", \"!\", \"?\", \";\", \"。\", \"！\", \"？\", \"；\", \"\\n\"]\n",
    "\n",
    "async def handle_chat(conversation_history: List[Dict[str, Any]]) -> None:\n",
    "    \"\"\"\n",
    "    Handles a full streaming chat round with Azure OpenAI GPT-4o,\n",
    "    correctly executes tool calls, synthesizes responses, and continues reasoning.\n",
    "    \"\"\"\n",
    "    tool_name = None\n",
    "    function_call_arguments = \"\"\n",
    "    tool_call_id = None\n",
    "    last_tts_request = None\n",
    "    collected_messages: List[str] = []\n",
    "\n",
    "    # 🔁 FIRST STREAMING RESPONSE (may include tool call)\n",
    "    response = client.chat.completions.create(\n",
    "        stream=True,\n",
    "        messages=conversation_history,\n",
    "        tools=available_tools,\n",
    "        tool_choice=\"auto\",\n",
    "        max_tokens=4096,\n",
    "        temperature=0.5,\n",
    "        top_p=1.0,\n",
    "        model=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    "    )\n",
    "\n",
    "    for chunk in response:\n",
    "        if chunk.choices:\n",
    "            delta = chunk.choices[0].delta\n",
    "\n",
    "            if delta.tool_calls:\n",
    "                if(delta.tool_calls[0].function.name):\n",
    "                    tool_name = chunk.choices[0].delta.tool_calls[0].function.name\n",
    "                    tool_id = chunk.choices[0].delta.tool_calls[0].id\n",
    "                    conversation_history.append(delta)\n",
    "                \n",
    "                if(chunk.choices[0].delta.tool_calls[0].function.arguments):    \n",
    "                    function_call_arguments+=delta.tool_calls[0].function.arguments\n",
    "            \n",
    "            elif delta.content:\n",
    "                chunk_text = chunk.choices[0].delta.content\n",
    "                if chunk_text:\n",
    "                    collected_messages.append(chunk_text)\n",
    "                    if chunk_text in tts_sentence_end:\n",
    "                        text = \"\".join(collected_messages).strip()\n",
    "                        last_tts_request = az_speach_synthesizer_client.start_speaking_text(text)\n",
    "                        collected_messages.clear()\n",
    "\n",
    "    # 🧠 If tool call was detected, execute it\n",
    "    if tool_name:\n",
    "        print(f\"tool_name:{tool_name}\")   \n",
    "        print(f\"tool_id:{tool_id}\")    \n",
    "        print(f\"function_call_arguments:{function_call_arguments}\")  \n",
    "        try:\n",
    "            parsed_args = json.loads(function_call_arguments.strip())\n",
    "            function_to_call = function_mapping.get(tool_name)\n",
    "\n",
    "            if function_to_call:\n",
    "                result = await function_to_call(parsed_args)\n",
    "\n",
    "                print(f\"✅ Function `{tool_name}` executed. Result: {result}\")\n",
    "\n",
    "                conversation_history.append({\n",
    "                    \"tool_call_id\": tool_id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": tool_name,\n",
    "                    \"content\": result,\n",
    "                })\n",
    "\n",
    "                # 🧠 SECOND STREAMING CALL AFTER TOOL EXECUTION\n",
    "                second_response = client.chat.completions.create(\n",
    "                    stream=True,\n",
    "                    messages=conversation_history,\n",
    "                    temperature=0.5,\n",
    "                    top_p=1.0,\n",
    "                    max_tokens=4096,\n",
    "                    model=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    "                )\n",
    "\n",
    "                collected_messages = []\n",
    "\n",
    "                for chunk in second_response:\n",
    "                    if chunk.choices:\n",
    "                        delta = chunk.choices[0].delta\n",
    "                        if hasattr(delta, \"content\") and delta.content:\n",
    "                            chunk_message = delta.content\n",
    "                            collected_messages.append(chunk_message)\n",
    "                            if chunk_message.strip() in tts_sentence_end:\n",
    "                                text = ''.join(collected_messages).strip()\n",
    "                                if text:\n",
    "                                    az_speach_synthesizer_client.start_speaking_text(text)\n",
    "                                    collected_messages.clear()\n",
    "\n",
    "                final_text = ''.join(collected_messages).strip()\n",
    "                if final_text:\n",
    "                    conversation_history.append({\"role\": \"assistant\", \"content\": final_text})\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"❌ Error parsing function arguments: {e}\")\n",
    "\n",
    "    else:\n",
    "        # Append the assistant message if no function call was made\n",
    "        final_text = ''.join(collected_messages).strip()\n",
    "        if final_text:\n",
    "            conversation_history.append({\"role\": \"assistant\", \"content\": final_text})\n",
    "            print(f\"✅ Final assistant message: {final_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6111fdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool_name:escalate_emergency\n",
      "tool_id:call_hbMdkWeNNrbjDUEhUih6vQEK\n",
      "function_call_arguments:{\"reason\":\"urgent appointment needed\"}\n",
      "✅ Function `escalate_emergency` executed. Result: 🚨 Emergency escalation triggered: {'reason': 'urgent appointment needed'}. A human healthcare agent is now being connected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 00:42:22,451 - micro - MainProcess - INFO     [🔊] Starting streaming speech synthesis for text: I understand that this is an e... (text_to_speech.py:start_speaking_text:44)\n",
      "INFO:micro:[🔊] Starting streaming speech synthesis for text: I understand that this is an e...\n",
      "2025-04-15 00:42:22,529 - micro - MainProcess - INFO     [🔊] Starting streaming speech synthesis for text: I've escalated your request, a... (text_to_speech.py:start_speaking_text:44)\n",
      "INFO:micro:[🔊] Starting streaming speech synthesis for text: I've escalated your request, a...\n",
      "2025-04-15 00:42:22,535 - micro - MainProcess - INFO     [🔊] Starting streaming speech synthesis for text: Please hold on for a moment.... (text_to_speech.py:start_speaking_text:44)\n",
      "INFO:micro:[🔊] Starting streaming speech synthesis for text: Please hold on for a moment....\n"
     ]
    }
   ],
   "source": [
    "await handle_chat(conversation_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7cc26a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 17:05:05,507 - micro - MainProcess - INFO     [🛑] Stopping speech synthesis... (text_to_speech.py:stop_speaking:55)\n",
      "INFO:micro:[🛑] Stopping speech synthesis...\n"
     ]
    }
   ],
   "source": [
    "az_speach_synthesizer_client.stop_speaking()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audioagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
