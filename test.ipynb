{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e24e206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "# set the directory to the location of the script\n",
    "try:\n",
    "    target_directory = os.getenv(\n",
    "        \"TARGET_DIRECTORY\", os.getcwd()\n",
    "    )  # Use environment variable if available\n",
    "    if os.path.exists(target_directory):\n",
    "        os.chdir(target_directory)\n",
    "        logging.info(f\"Successfully changed directory to: {os.getcwd()}\")\n",
    "    else:\n",
    "        logging.error(f\"Directory does not exist: {target_directory}\")\n",
    "except Exception as e:\n",
    "    logging.exception(f\"An error occurred while changing directory: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc0813d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.redis.redis_client import AzureRedisManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7740ed0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 22:57:55,442 - micro - MainProcess - INFO     Azure Redis connection initialized. (redis_client.py:__init__:42)\n",
      "INFO:micro:Azure Redis connection initialized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user': 'Pablo', 'authenticated': 'True'}\n"
     ]
    }
   ],
   "source": [
    "redis_manager = AzureRedisManager()\n",
    "\n",
    "redis_manager.ping()  # Check connectivity\n",
    "redis_manager.store_session_data(\n",
    "    \"session123\", {\"user\": \"Pablo\", \"authenticated\": \"True\"}\n",
    ")\n",
    "print(redis_manager.get_session_data(\"session123\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13272911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.speech.speech_to_text import SpeechCoreTranslator  # existing class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45177166",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech.create_realtime_recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79d35909",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'speechsdk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mspeechsdk\u001b[49m.PropertyId.Speech_SegmentationStrategy\n",
      "\u001b[31mNameError\u001b[39m: name 'speechsdk' is not defined"
     ]
    }
   ],
   "source": [
    "speechsdk.PropertyId.Speech_SegmentationStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "571a6b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from typing import Callable, Optional, Tuple\n",
    "\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "from azure.cognitiveservices.speech import SpeechRecognitionResult\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from utils.ml_logging import get_logger\n",
    "import asyncio\n",
    "\n",
    "# Set up logger\n",
    "logger = get_logger()\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad9c0ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PropertyId.Speech_SegmentationStrategy: 9004>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speechsdk.PropertyId.Speech_SegmentationStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b669357",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_key = os.getenv(\"AZURE_SPEECH_KEY\")\n",
    "speech_region = os.getenv(\"AZURE_SPEECH_REGION\")\n",
    "connection_string = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "speech_config = speechsdk.SpeechConfig(\n",
    "    subscription=speech_key, region=speech_region\n",
    ")\n",
    "speech_config.set_property(\n",
    "    speechsdk.PropertyId.SpeechServiceConnection_EnableAudioLogging, \"true\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ad7cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    " # recognizer.properties.set_property(\n",
    "        #     speechsdk.PropertyId.Speech_SegmentationSilenceTimeoutMs,\n",
    "        #     str(50),\n",
    "        # )\n",
    "        # recognizer.properties.set_property(\n",
    "        #     speechsdk.PropertyId.SpeechServiceConnection_EndSilenceTimeoutMs, str(vad_silence_timeout_ms)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dde851ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_config.set_property(speechsdk.PropertyId.Speech_SegmentationSilenceTimeoutMs, str(50))\n",
    "speech_config.set_property(\n",
    "    speechsdk.PropertyId.SpeechServiceConnection_EndSilenceTimeoutMs, str(50)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "580431cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AudioProcessingOptions' from 'azure.cognitiveservices.speech.audio' (c:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\audioagent\\Lib\\site-packages\\azure\\cognitiveservices\\speech\\audio.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mazure\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcognitiveservices\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspeech\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maudio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     AudioStreamFormat,\n\u001b[32m      3\u001b[39m     PushAudioInputStream,\n\u001b[32m      4\u001b[39m     AudioProcessingOptions,\n\u001b[32m      5\u001b[39m     AudioProcessingConstants,\n\u001b[32m      6\u001b[39m )\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'AudioProcessingOptions' from 'azure.cognitiveservices.speech.audio' (c:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\audioagent\\Lib\\site-packages\\azure\\cognitiveservices\\speech\\audio.py)"
     ]
    }
   ],
   "source": [
    "from azure.cognitiveservices.speech.audio import (\n",
    "    AudioStreamFormat,\n",
    "    PushAudioInputStream,\n",
    "    AudioProcessingOptions,\n",
    "    AudioProcessingConstants,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f608c27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_config = speechsdk.audio.AudioConfig(stream=push_stream)\n",
    "\n",
    "# Use the instance's speech_config\n",
    "recognizer = speechsdk.SpeechRecognizer(\n",
    "    speech_config=self.speech_config,\n",
    "    audio_config=audio_config,\n",
    "    language=language,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ebe497b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechCoreTranslator:\n",
    "    \"\"\"\n",
    "    A class that serves as the core for handling Azure AI Services Speech SDK functionality.\n",
    "    It encapsulates the processes involved in translating and transcribing speech.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.speech_key = os.getenv(\"AZURE_SPEECH_KEY\")\n",
    "        self.speech_region = os.getenv(\"AZURE_SPEECH_REGION\")\n",
    "        self.connection_string = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "        self.speech_config = speechsdk.SpeechConfig(\n",
    "            subscription=self.speech_key, region=self.speech_region\n",
    "        )\n",
    "        self.speech_config.set_property(\n",
    "            speechsdk.PropertyId.SpeechServiceConnection_EnableAudioLogging, \"true\"\n",
    "        )\n",
    "        self.supported_languages = [\n",
    "            \"en-US\",  # English (United States)\n",
    "            \"es-ES\",  # Spanish (Spain)\n",
    "            \"fr-FR\",  # French (France)\n",
    "        ]\n",
    "\n",
    "    def add_supported_language(self, language):\n",
    "        \"\"\"\n",
    "        Appends a language to the list of supported languages.\n",
    "        Parameters:\n",
    "        language (str): The language to be added.\n",
    "        \"\"\"\n",
    "        self.supported_languages.append(language)\n",
    "\n",
    "    def create_realtime_recognizer(\n",
    "        self,\n",
    "        push_stream: speechsdk.audio.PushAudioInputStream,\n",
    "        loop: asyncio.AbstractEventLoop,\n",
    "        message_queue: asyncio.Queue,\n",
    "        language: str = \"en-US\",\n",
    "        vad_silence_timeout_ms: int = 1000,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Creates and configures a SpeechRecognizer for real-time continuous recognition\n",
    "        using a PushAudioInputStream and sets up callbacks to use an asyncio Queue.\n",
    "\n",
    "        Args:\n",
    "            push_stream: The PushAudioInputStream to use for audio input.\n",
    "            loop: The asyncio event loop for scheduling callbacks.\n",
    "            message_queue: The asyncio Queue to put recognized text onto.\n",
    "            language: The language code for recognition (e.g., \"en-US\").\n",
    "\n",
    "        Returns:\n",
    "            A configured speechsdk.SpeechRecognizer instance.\n",
    "        \"\"\"\n",
    "        audio_config = speechsdk.audio.AudioConfig(stream=push_stream)\n",
    "\n",
    "        # Use the instance's speech_config\n",
    "        recognizer = speechsdk.SpeechRecognizer(\n",
    "            speech_config=self.speech_config,\n",
    "            audio_config=audio_config,\n",
    "            language=language,\n",
    "        )\n",
    "\n",
    "        # recognizer.properties.set_property(\n",
    "        #     speechsdk.PropertyId.Speech_SegmentationSilenceTimeoutMs,\n",
    "        #     str(50),\n",
    "        # )\n",
    "        # recognizer.properties.set_property(\n",
    "        #     speechsdk.PropertyId.SpeechServiceConnection_EndSilenceTimeoutMs, str(vad_silence_timeout_ms)) \n",
    "\n",
    "        # --- Define Callbacks ---\n",
    "        def recognizing_cb(evt: speechsdk.SpeechRecognitionEventArgs):\n",
    "            if evt.result.reason == speechsdk.ResultReason.RecognizingSpeech:\n",
    "                logger.info(f\"PARTIAL → {evt.result.text}\")\n",
    "\n",
    "        def recognized_cb(evt: speechsdk.SpeechRecognitionEventArgs):\n",
    "            if evt.result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "                logger.info(f\"FINAL   → {evt.result.text}\")\n",
    "                # Put the final recognized text onto the queue for processing\n",
    "                if evt.result.text:  # Avoid putting empty strings on the queue\n",
    "                    asyncio.run_coroutine_threadsafe(\n",
    "                        message_queue.put(evt.result.text), loop\n",
    "                    )\n",
    "            elif evt.result.reason == speechsdk.ResultReason.NoMatch:\n",
    "                logger.info(\"NOMATCH: Speech could not be recognized.\")\n",
    "\n",
    "        def session_stopped_cb(evt: speechsdk.SessionEventArgs):\n",
    "            logger.info(f\"Session stopped: {evt}\")\n",
    "            # You might want to signal the end of the stream or handle cleanup here\n",
    "            # Example: asyncio.run_coroutine_threadsafe(message_queue.put(None), loop)\n",
    "\n",
    "        def canceled_cb(evt: speechsdk.SpeechRecognitionCanceledEventArgs):\n",
    "            logger.error(f\"Recognition Canceled: {evt.reason}\")\n",
    "            if evt.reason == speechsdk.CancellationReason.Error:\n",
    "                logger.error(f\"Canceled details: {evt.error_details}\")\n",
    "            # Signal the end or error state\n",
    "            # Example: asyncio.run_coroutine_threadsafe(message_queue.put(None), loop)\n",
    "\n",
    "        # Connect callbacks\n",
    "        recognizer.recognizing.connect(recognizing_cb)\n",
    "        recognizer.recognized.connect(recognized_cb)\n",
    "        recognizer.session_stopped.connect(session_stopped_cb)\n",
    "        recognizer.canceled.connect(canceled_cb)\n",
    "\n",
    "        logger.info(f\"Realtime recognizer created for language: {language}\")\n",
    "        return recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce637c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "speach = SpeechCoreTranslator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcaa3e0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot construct AudioConfig with the given arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mspeach\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_realtime_recognizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpush_stream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m=\u001b[49m\u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_event_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage_queue\u001b[49m\u001b[43m=\u001b[49m\u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mQueue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43men-US\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mSpeechCoreTranslator.create_realtime_recognizer\u001b[39m\u001b[34m(self, push_stream, loop, message_queue, language, vad_silence_timeout_ms)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_realtime_recognizer\u001b[39m(\n\u001b[32m     32\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     33\u001b[39m     push_stream: speechsdk.audio.PushAudioInputStream,\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     vad_silence_timeout_ms: \u001b[38;5;28mint\u001b[39m = \u001b[32m1000\u001b[39m,\n\u001b[32m     38\u001b[39m ):\n\u001b[32m     39\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[33;03m    Creates and configures a SpeechRecognizer for real-time continuous recognition\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[33;03m    using a PushAudioInputStream and sets up callbacks to use an asyncio Queue.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m \u001b[33;03m        A configured speechsdk.SpeechRecognizer instance.\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     audio_config = \u001b[43mspeechsdk\u001b[49m\u001b[43m.\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAudioConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpush_stream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m     \u001b[38;5;66;03m# Use the instance's speech_config\u001b[39;00m\n\u001b[32m     55\u001b[39m     recognizer = speechsdk.SpeechRecognizer(\n\u001b[32m     56\u001b[39m         speech_config=\u001b[38;5;28mself\u001b[39m.speech_config,\n\u001b[32m     57\u001b[39m         audio_config=audio_config,\n\u001b[32m     58\u001b[39m         language=language,\n\u001b[32m     59\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\audioagent\\Lib\\site-packages\\azure\\cognitiveservices\\speech\\audio.py:390\u001b[39m, in \u001b[36mAudioConfig.__init__\u001b[39m\u001b[34m(self, use_default_microphone, filename, stream, device_name)\u001b[39m\n\u001b[32m    388\u001b[39m         _call_hr_fn(fn=_sdk_lib.audio_config_create_audio_input_from_a_microphone, *[ctypes.byref(handle), c_device])\n\u001b[32m    389\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mcannot construct AudioConfig with the given arguments\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    391\u001b[39m \u001b[38;5;28mself\u001b[39m.__handle = _Handle(handle, _sdk_lib.audio_config_is_handle_valid, _sdk_lib.audio_config_release)\n\u001b[32m    392\u001b[39m prop_handle = _spx_handle(\u001b[32m0\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: cannot construct AudioConfig with the given arguments"
     ]
    }
   ],
   "source": [
    "speach.create_realtime_recognizer(\n",
    "    push_stream=None,\n",
    "    loop=asyncio.get_event_loop(),\n",
    "    message_queue=asyncio.Queue(),\n",
    "    language=\"en-US\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0020346",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AudioProcessingOptions' from 'azure.cognitiveservices.speech.audio' (c:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\audioagent\\Lib\\site-packages\\azure\\cognitiveservices\\speech\\audio.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mazure\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcognitiveservices\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspeech\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maudio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     AudioProcessingOptions,\n\u001b[32m      3\u001b[39m     AudioProcessingConstants,\n\u001b[32m      4\u001b[39m )\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'AudioProcessingOptions' from 'azure.cognitiveservices.speech.audio' (c:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\audioagent\\Lib\\site-packages\\azure\\cognitiveservices\\speech\\audio.py)"
     ]
    }
   ],
   "source": [
    "from azure.cognitiveservices.speech.audio import (\n",
    "    AudioProcessingOptions,\n",
    "    AudioProcessingConstants,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audioagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
