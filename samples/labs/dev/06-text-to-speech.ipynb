{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f553369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed directory to: c:\\Users\\pablosal\\Desktop\\gbb-ai-audio-agent\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "# set the directory to the location of the script\n",
    "try:\n",
    "    os.chdir(\"..\")\n",
    "    target_directory = os.getenv(\n",
    "        \"TARGET_DIRECTORY\", os.getcwd()\n",
    "    )  # Use environment variable if available\n",
    "    if os.path.exists(target_directory):\n",
    "        os.chdir(target_directory)\n",
    "        print(f\"Changed directory to: {os.getcwd()}\")\n",
    "        logging.info(f\"Successfully changed directory to: {os.getcwd()}\")\n",
    "    else:\n",
    "        logging.error(f\"Directory does not exist: {target_directory}\")\n",
    "except Exception as e:\n",
    "    logging.exception(f\"An error occurred while changing directory: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed996c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "from azure.cognitiveservices.speech.transcription import ConversationTranscriber\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load credentials\n",
    "load_dotenv()\n",
    "KEY = os.getenv(\"AZURE_SPEECH_KEY\")\n",
    "REGION = os.getenv(\"AZURE_SPEECH_REGION\", \"eastus\")\n",
    "AUDIO = Path(\"podcast_voice_tests/ground_truth/Podcast_sample_CLT_SD.wav\")\n",
    "OUT_DIR = Path(\"podcast_voice_tests/ground_truth\")\n",
    "TRANSCRIPT_FILE = OUT_DIR / \"Podcast_sample_CLT_SD_transcript.txt\"\n",
    "\n",
    "# Validate\n",
    "if not KEY:\n",
    "    print(\"Missing AZURE_SPEECH_KEY\")\n",
    "    sys.exit(1)\n",
    "if not AUDIO.exists():\n",
    "    print(f\"Missing audio at {AUDIO}\")\n",
    "    sys.exit(1)\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def make_speech_config():\n",
    "    cfg = speechsdk.SpeechConfig(subscription=KEY, region=REGION)\n",
    "    # Enable continuous language detection\n",
    "    cfg.set_property(\n",
    "        property_id=speechsdk.PropertyId.SpeechServiceConnection_LanguageIdMode,\n",
    "        value=\"Continuous\",\n",
    "    )\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def transcribe_conversation(audio_path: Path, out_file: Path):\n",
    "    \"\"\"Transcribe audio and save to a text file.\"\"\"\n",
    "    speech_cfg = make_speech_config()\n",
    "    audio_cfg = speechsdk.AudioConfig(filename=str(audio_path))\n",
    "    transcriber = ConversationTranscriber(speech_cfg, audio_cfg)\n",
    "\n",
    "    done = False\n",
    "    with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
    "\n",
    "        def stop_cb(evt):\n",
    "            nonlocal done\n",
    "            print(\"[CONVERSATION] Session stopped.\")\n",
    "            done = True\n",
    "\n",
    "        def on_transcribed(evt):\n",
    "            speaker = evt.result.speaker_id or \"?\"\n",
    "            text = evt.result.text\n",
    "            line = f\"[Speaker {speaker}] {text}\"\n",
    "            print(line)\n",
    "            f.write(line + \"\\n\")\n",
    "\n",
    "        transcriber.transcribed.connect(on_transcribed)\n",
    "        transcriber.session_stopped.connect(stop_cb)\n",
    "        transcriber.canceled.connect(stop_cb)\n",
    "\n",
    "        print(\"[CONVERSATION] Starting transcription…\")\n",
    "        transcriber.start_transcribing_async()\n",
    "        while not done:\n",
    "            pass\n",
    "        transcriber.stop_continuous_recognition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb1e4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribe_conversation(AUDIO, TRANSCRIPT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3183b2",
   "metadata": {},
   "source": [
    "## Exploring TTS Models (Azure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18394fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_KEY = \"\"\n",
    "AZURE_REGION = \"northcentralus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9367643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Synthesizing with en-US-AlloyMultilingualNeuralHD → podcast_en-US-AlloyMultilingualNeuralHD.wav\n",
      "⏳ Synthesizing with en-US-NovaMultilingualNeuralHD → podcast_en-US-NovaMultilingualNeuralHD.wav\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Output directory for test WAVs\n",
    "OUTPUT_DIR = Path(\"podcast_voice_tests\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# A short “podcast” excerpt for testing\n",
    "PODCAST_TEXT = \"\"\"\\\n",
    "Hello and welcome to “Tech Talks Weekly.” \n",
    "I’m your host, Alice, bringing you the latest in AI. \n",
    "Next up, Bob will dive into our featured topic: high-definition neural voices!\\\n",
    "\"\"\"\n",
    "\n",
    "# Define voices to test: a mix of Azure AI Speech HD and OpenAI NeuralHD\n",
    "VOICE_LIST_HD = [\n",
    "    # Azure AI Speech HD voices\n",
    "    \"en-US-Ava:DragonHDLatestNeural\",\n",
    "    \"en-US-Andrew3:DragonHDLatestNeural\",\n",
    "    \"es-ES-Ximena:DragonHDLatestNeural\",\n",
    "]\n",
    "\n",
    "VOICE_LIST_OPENAI = [\n",
    "    # Azure AI Speech HD voices\n",
    "    \"en-US-AlloyMultilingualNeuralHD\",\n",
    "    \"en-US-NovaMultilingualNeuralHD\",\n",
    "]\n",
    "\n",
    "\n",
    "def synthesize_to_file(ssml: str, voice_name: str, out_path: Path):\n",
    "    \"\"\"\n",
    "    Synthesize the given SSML and write a WAV file.\n",
    "    \"\"\"\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=AZURE_KEY, region=AZURE_REGION)\n",
    "    speech_config.speech_synthesis_voice_name = voice_name\n",
    "    # Use high-quality WAV\n",
    "    speech_config.set_speech_synthesis_output_format(\n",
    "        speechsdk.SpeechSynthesisOutputFormat.Riff48Khz16BitMonoPcm\n",
    "    )\n",
    "    audio_cfg = speechsdk.audio.AudioOutputConfig(filename=str(out_path))\n",
    "    synthesizer = speechsdk.SpeechSynthesizer(speech_config, audio_cfg)\n",
    "\n",
    "    result = synthesizer.speak_ssml_async(ssml).get()\n",
    "    if result.reason != speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "        error = result.cancellation_details\n",
    "        print(\n",
    "            f\"[ERROR] {voice_name} failed: {error.error_details if error else result.reason}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def build_plain_ssml(text: str, voice_name: str, locale: str = \"en-US\") -> str:\n",
    "    \"\"\"\n",
    "    Wrap plain text in a single <voice> tag for SSML synthesis.\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "<speak version=\"1.0\" xmlns=\"http://www.w3.org/2001/10/synthesis\" \n",
    "       xmlns:mstts=\"https://www.w3.org/2001/mstts\" xml:lang=\"{locale}\">\n",
    "  <voice name=\"{voice_name}\">{text}</voice>\n",
    "</speak>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# --- SINGLE-VOICE TEST ------------------------------------------------------------\n",
    "\n",
    "for voice in VOICE_LIST_OPENAI:\n",
    "    ssml = build_plain_ssml(PODCAST_TEXT, voice)\n",
    "    filename = OUTPUT_DIR / f\"podcast_{voice.replace(':','-')}.wav\"\n",
    "    print(f\"⏳ Synthesizing with {voice} → {filename.name}\")\n",
    "    synthesize_to_file(ssml, voice, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aa505c",
   "metadata": {},
   "source": [
    "## Azure AI Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e2e32e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthesizing to podcast_voice_tests\\podcast_dynamic_HD.wav...\n",
      "Synthesis complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\"\"\"\n",
    "This script reads a ground-truth transcript with speaker labels,\n",
    "builds an SSML document assigning distinct Azure HD voices to each speaker,\n",
    "and synthesizes the result into a single WAV file.\n",
    "\"\"\"\n",
    "\n",
    "# Load Azure credentials from environment or .env\n",
    "# Load Azure credentials\n",
    "load_dotenv()\n",
    "AZURE_KEY = os.getenv(\"AZURE_SPEECH_KEY\")\n",
    "AZURE_REGION = os.getenv(\"AZURE_SPEECH_REGION\", \"eastus\")\n",
    "\n",
    "if not AZURE_KEY:\n",
    "    print(\"Error: Please set AZURE_SPEECH_KEY in your environment or .env\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Paths & files\n",
    "OUTPUT_DIR = Path(\"podcast_voice_tests\")\n",
    "GROUND_DIR = OUTPUT_DIR / \"ground_truth\"\n",
    "TRANSCRIPT = GROUND_DIR / \"Podcast_sample_CLT_SD_transcript.txt\"\n",
    "OUTPUT_WAV = OUTPUT_DIR / \"podcast_dynamic_HD.wav\"\n",
    "\n",
    "# Ensure dirs exist\n",
    "for d in (OUTPUT_DIR, GROUND_DIR):\n",
    "    d.mkdir(exist_ok=True)\n",
    "\n",
    "if not TRANSCRIPT.exists():\n",
    "    print(f\"Error: Transcript file not found at {TRANSCRIPT}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Map each speaker to a voice persona\n",
    "VOICE_MAP = {\n",
    "    \"Speaker Guest-1\": \"en-US-Andrew3:DragonHDLatestNeural\",  # Male\n",
    "    \"Speaker Guest-2\": \"en-US-Ava:DragonHDLatestNeural\",  # Female\n",
    "}\n",
    "\n",
    "\n",
    "def load_segments(path: Path):\n",
    "    \"\"\"Parse the transcript into (speaker, text) segments.\"\"\"\n",
    "    segments = []\n",
    "    for line in path.read_text(encoding=\"utf-8\").splitlines():\n",
    "        line = line.strip()\n",
    "        if not line or not line.startswith(\"[\") or \"]\" not in line:\n",
    "            continue\n",
    "        speaker_label, text = line.split(\"]\", 1)\n",
    "        speaker = speaker_label.lstrip(\"[\").strip()\n",
    "        segments.append((speaker, text.strip()))\n",
    "    return segments\n",
    "\n",
    "\n",
    "def build_ssml(segments):\n",
    "    \"\"\"Build SSML with express-as styles and prosody for natural flow.\"\"\"\n",
    "    ssml = [\n",
    "        '<speak version=\"1.0\"',\n",
    "        '       xmlns=\"http://www.w3.org/2001/10/synthesis\"',\n",
    "        '       xmlns:mstts=\"https://www.w3.org/2001/mstts\"',\n",
    "        '       xml:lang=\"en-US\">',\n",
    "    ]\n",
    "    for i, (speaker, text) in enumerate(segments):\n",
    "        voice = VOICE_MAP.get(speaker, next(iter(VOICE_MAP.values())))\n",
    "        rate = \"+3%\" if i % 2 == 0 else \"-2%\"\n",
    "        ssml.append(\n",
    "            f\"\"\"\n",
    "  <voice name=\"{voice}\">\n",
    "    <mstts:express-as style=\"chat\">\n",
    "      <prosody rate=\"{rate}\">\n",
    "        {text}\n",
    "      </prosody>\n",
    "    </mstts:express-as>\n",
    "  </voice>\"\"\".strip()\n",
    "        )\n",
    "    ssml.append(\"</speak>\")\n",
    "    return \"\\n\".join(ssml)\n",
    "\n",
    "\n",
    "def synthesize_ssml(ssml: str, out_path: Path):\n",
    "    \"\"\"Synthesize the SSML to a WAV file using Azure TTS.\"\"\"\n",
    "    config = speechsdk.SpeechConfig(subscription=AZURE_KEY, region=AZURE_REGION)\n",
    "    config.set_speech_synthesis_output_format(\n",
    "        speechsdk.SpeechSynthesisOutputFormat.Riff48Khz16BitMonoPcm\n",
    "    )\n",
    "    audio_config = speechsdk.audio.AudioOutputConfig(filename=str(out_path))\n",
    "    synthesizer = speechsdk.SpeechSynthesizer(config, audio_config)\n",
    "\n",
    "    print(f\"Synthesizing to {out_path}...\")\n",
    "    result = synthesizer.speak_ssml_async(ssml).get()\n",
    "    if result.reason != speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "        err = result.cancellation_details\n",
    "        raise RuntimeError(f\"TTS failed: {err.error_details if err else result.reason}\")\n",
    "    print(\"Synthesis complete.\")\n",
    "\n",
    "\n",
    "segments = load_segments(TRANSCRIPT)\n",
    "if not segments:\n",
    "    print(\"Error: No valid segments found in transcript.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "ssml_doc = build_ssml(segments)\n",
    "synthesize_ssml(ssml_doc, OUTPUT_WAV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8c2bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthesizing to podcast_voice_tests\\podcast_dynamic_AOAI.wav...\n",
      "Synthesis complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\"\"\"\n",
    "This script reads a ground-truth transcript with speaker labels,\n",
    "builds an SSML document assigning distinct Azure HD voices to each speaker,\n",
    "and synthesizes the result into a single WAV file.\n",
    "\"\"\"\n",
    "\n",
    "# Load Azure credentials from environment or .env\n",
    "# Load Azure credentials\n",
    "\n",
    "\n",
    "if not AZURE_KEY:\n",
    "    print(\"Error: Please set AZURE_SPEECH_KEY in your environment or .env\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Paths & files\n",
    "OUTPUT_DIR = Path(\"podcast_voice_tests\")\n",
    "GROUND_DIR = OUTPUT_DIR / \"ground_truth\"\n",
    "TRANSCRIPT = GROUND_DIR / \"Podcast_sample_CLT_SD_transcript.txt\"\n",
    "OUTPUT_WAV = OUTPUT_DIR / \"podcast_dynamic_AOAI.wav\"\n",
    "\n",
    "# Ensure dirs exist\n",
    "for d in (OUTPUT_DIR, GROUND_DIR):\n",
    "    d.mkdir(exist_ok=True)\n",
    "\n",
    "if not TRANSCRIPT.exists():\n",
    "    print(f\"Error: Transcript file not found at {TRANSCRIPT}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# Map each speaker to a voice persona\n",
    "VOICE_MAP = {\n",
    "    \"Speaker Guest-1\": \"en-US-AlloyMultilingualNeuralHD\",  # Male\n",
    "    \"Speaker Guest-2\": \"en-US-NovaMultilingualNeuralHD\",  # Female\n",
    "}\n",
    "\n",
    "\n",
    "def load_segments(path: Path):\n",
    "    \"\"\"Parse the transcript into (speaker, text) segments.\"\"\"\n",
    "    segments = []\n",
    "    for line in path.read_text(encoding=\"utf-8\").splitlines():\n",
    "        line = line.strip()\n",
    "        if not line or not line.startswith(\"[\") or \"]\" not in line:\n",
    "            continue\n",
    "        speaker_label, text = line.split(\"]\", 1)\n",
    "        speaker = speaker_label.lstrip(\"[\").strip()\n",
    "        segments.append((speaker, text.strip()))\n",
    "    return segments\n",
    "\n",
    "\n",
    "def build_ssml(segments):\n",
    "    \"\"\"Build SSML with express-as styles and prosody for natural flow.\"\"\"\n",
    "    ssml = [\n",
    "        '<speak version=\"1.0\"',\n",
    "        '       xmlns=\"http://www.w3.org/2001/10/synthesis\"',\n",
    "        '       xmlns:mstts=\"https://www.w3.org/2001/mstts\"',\n",
    "        '       xml:lang=\"en-US\">',\n",
    "    ]\n",
    "    for i, (speaker, text) in enumerate(segments):\n",
    "        voice = VOICE_MAP.get(speaker, next(iter(VOICE_MAP.values())))\n",
    "        rate = \"+3%\" if i % 2 == 0 else \"-2%\"\n",
    "        ssml.append(\n",
    "            f\"\"\"\n",
    "  <voice name=\"{voice}\">\n",
    "    <mstts:express-as style=\"chat\">\n",
    "      <prosody rate=\"{rate}\">\n",
    "        {text}\n",
    "      </prosody>\n",
    "    </mstts:express-as>\n",
    "  </voice>\"\"\".strip()\n",
    "        )\n",
    "    ssml.append(\"</speak>\")\n",
    "    return \"\\n\".join(ssml)\n",
    "\n",
    "\n",
    "def synthesize_ssml(ssml: str, out_path: Path):\n",
    "    \"\"\"Synthesize the SSML to a WAV file using Azure TTS.\"\"\"\n",
    "    config = speechsdk.SpeechConfig(subscription=AZURE_KEY, region=AZURE_REGION)\n",
    "    config.set_speech_synthesis_output_format(\n",
    "        speechsdk.SpeechSynthesisOutputFormat.Riff48Khz16BitMonoPcm\n",
    "    )\n",
    "    audio_config = speechsdk.audio.AudioOutputConfig(filename=str(out_path))\n",
    "    synthesizer = speechsdk.SpeechSynthesizer(config, audio_config)\n",
    "\n",
    "    print(f\"Synthesizing to {out_path}...\")\n",
    "    result = synthesizer.speak_ssml_async(ssml).get()\n",
    "    if result.reason != speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "        err = result.cancellation_details\n",
    "        raise RuntimeError(f\"TTS failed: {err.error_details if err else result.reason}\")\n",
    "    print(\"Synthesis complete.\")\n",
    "\n",
    "\n",
    "segments = load_segments(TRANSCRIPT)\n",
    "if not segments:\n",
    "    print(\"Error: No valid segments found in transcript.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "ssml_doc = build_ssml(segments)\n",
    "synthesize_ssml(ssml_doc, OUTPUT_WAV)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audioagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
