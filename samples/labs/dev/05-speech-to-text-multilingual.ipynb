{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "343b4d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed directory to: c:\\Users\\pablosal\\Desktop\\gbb-ai-audio-agent\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "# set the directory to the location of the script\n",
    "try:\n",
    "    os.chdir(\"../../../\")\n",
    "    target_directory = os.getenv(\n",
    "        \"TARGET_DIRECTORY\", os.getcwd()\n",
    "    )  # Use environment variable if available\n",
    "    if os.path.exists(target_directory):\n",
    "        os.chdir(target_directory)\n",
    "        print(f\"Changed directory to: {os.getcwd()}\")\n",
    "        logging.info(f\"Successfully changed directory to: {os.getcwd()}\")\n",
    "    else:\n",
    "        logging.error(f\"Directory does not exist: {target_directory}\")\n",
    "except Exception as e:\n",
    "    logging.exception(f\"An error occurred while changing directory: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "930a1a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-08-20 12:07:57,250] INFO - src.speech.speech_recognizer: Azure Monitor tracing initialized for speech recognizer\n",
      "INFO:src.speech.speech_recognizer:Azure Monitor tracing initialized for speech recognizer\n",
      "[2025-08-20 12:07:57,260] INFO - src.speech.speech_recognizer: Creating SpeechConfig with API key authentication\n",
      "INFO:src.speech.speech_recognizer:Creating SpeechConfig with API key authentication\n",
      "[2025-08-20 12:07:57,270] INFO - src.speech.text_to_speech: Azure Monitor tracing initialized for speech synthesizer\n",
      "INFO:src.speech.text_to_speech:Azure Monitor tracing initialized for speech synthesizer\n",
      "[2025-08-20 12:07:57,278] INFO - src.speech.text_to_speech: Creating SpeechConfig with API key authentication\n",
      "INFO:src.speech.text_to_speech:Creating SpeechConfig with API key authentication\n",
      "[2025-08-20 12:07:57,288] INFO - src.speech.text_to_speech: Speech synthesizer initialized successfully\n",
      "INFO:src.speech.text_to_speech:Speech synthesizer initialized successfully\n"
     ]
    }
   ],
   "source": [
    "from src.speech.text_to_speech import SpeechSynthesizer\n",
    "from src.speech.speech_recognizer import StreamingSpeechRecognizerFromBytes\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "if \"az_speech_recognizer_stream_client\" not in locals():\n",
    "    az_speech_recognizer_stream_client = StreamingSpeechRecognizerFromBytes(\n",
    "        vad_silence_timeout_ms=800,\n",
    "        use_semantic_segmentation=False,\n",
    "        audio_format=\"pcm\",\n",
    "        candidate_languages=[\"en-US\", \"fr-FR\", \"de-DE\", \"es-ES\", \"it-IT\"],\n",
    "        enable_diarisation=True,\n",
    "        speaker_count_hint=2,\n",
    "        enable_neural_fe=False,\n",
    "    )\n",
    "\n",
    "if \"az_speach_synthesizer_client\" not in locals():\n",
    "    az_speach_synthesizer_client = SpeechSynthesizer()\n",
    "\n",
    "# Ensure Azure OpenAI client is initialized only if not already defined\n",
    "if \"client\" not in locals():\n",
    "    client = AzureOpenAI(\n",
    "        api_version=\"2025-02-01-preview\",\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d74aee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define end-of-sentence markers for TTS\n",
    "TTS_ENDS = {\".\", \"!\", \"?\", \";\", \"\\n\"}\n",
    "\n",
    "# Flags and buffers\n",
    "is_synthesizing = False\n",
    "user_buffer = \"\"\n",
    "assistant_buffer = \"\"\n",
    "tts_thread = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c00c034",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-08-20 12:08:02,473] INFO - src.speech.speech_recognizer: Starting recognition from byte stream‚Ä¶\n",
      "INFO:src.speech.speech_recognizer:Starting recognition from byte stream‚Ä¶\n",
      "[2025-08-20 12:08:02,482] INFO - src.speech.speech_recognizer: Speech-SDK prepare_start ‚Äì format=pcm  neuralFE=False  diar=True\n",
      "INFO:src.speech.speech_recognizer:Speech-SDK prepare_start ‚Äì format=pcm  neuralFE=False  diar=True\n",
      "[2025-08-20 12:08:02,494] INFO - src.speech.speech_recognizer: Speech-SDK ready (neuralFE=False, diarisation=True, speakers=2)\n",
      "INFO:src.speech.speech_recognizer:Speech-SDK ready (neuralFE=False, diarisation=True, speakers=2)\n",
      "[2025-08-20 12:08:02,504] INFO - src.speech.speech_recognizer: Recognition started.\n",
      "INFO:src.speech.speech_recognizer:Recognition started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéôÔ∏è Speak now...\n",
      "üßæ User (final) in en-US: Hello, how are you doing? Are you doing well?\n",
      "Hello!Hi there, I am a assistant_speak callback!\n",
      "Syntetixing: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-08-20 12:08:10,075] INFO - src.speech.text_to_speech: [üîä] Starting streaming speech synthesis for text: Hello!...\n",
      "INFO:src.speech.text_to_speech:[üîä] Starting streaming speech synthesis for text: Hello!...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm here and ready to help you.Hi there, I am a assistant_speak callback!\n",
      "Syntetixing: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-08-20 12:08:10,118] INFO - src.speech.text_to_speech: [üîä] Starting streaming speech synthesis for text: I'm here and ready to help you....\n",
      "INFO:src.speech.text_to_speech:[üîä] Starting streaming speech synthesis for text: I'm here and ready to help you....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " How can I assist you today?Hi there, I am a assistant_speak callback!\n",
      "Syntetixing: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-08-20 12:08:10,158] INFO - src.speech.text_to_speech: [üîä] Starting streaming speech synthesis for text: How can I assist you today?...\n",
      "INFO:src.speech.text_to_speech:[üîä] Starting streaming speech synthesis for text: How can I assist you today?...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üßæ User (final) in en-US: Hello, I'm here and ready to help you.\n",
      "That's great to hear!Hi there, I am a assistant_speak callback!\n",
      "Syntetixing: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-08-20 12:08:14,770] INFO - src.speech.text_to_speech: [üîä] Starting streaming speech synthesis for text: That's great to hear!...\n",
      "INFO:src.speech.text_to_speech:[üîä] Starting streaming speech synthesis for text: That's great to hear!...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " How can I assist you today?Hi there, I am a assistant_speak callback!\n",
      "Syntetixing: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-08-20 12:08:14,813] INFO - src.speech.text_to_speech: [üîä] Starting streaming speech synthesis for text: How can I assist you today?...\n",
      "INFO:src.speech.text_to_speech:[üîä] Starting streaming speech synthesis for text: How can I assist you today?...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üßæ User (final) in en-US: How can I assist you today?\n",
      "Feel free to ask me any questions or let me know what information you need!Hi there, I am a assistant_speak callback!\n",
      "Syntetixing: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-08-20 12:08:16,776] INFO - src.speech.text_to_speech: [üîä] Starting streaming speech synthesis for text: Feel free to ask me any questions or let me know w...\n",
      "INFO:src.speech.text_to_speech:[üîä] Starting streaming speech synthesis for text: Feel free to ask me any questions or let me know w...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm here to help.Hi there, I am a assistant_speak callback!\n",
      "Syntetixing: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-08-20 12:08:16,817] INFO - src.speech.text_to_speech: [üîä] Starting streaming speech synthesis for text: I'm here to help....\n",
      "INFO:src.speech.text_to_speech:[üîä] Starting streaming speech synthesis for text: I'm here to help....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üßæ User (final) in en-US: That's great to hear.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 93\u001b[39m\n\u001b[32m     90\u001b[39m                     collected_messages.clear()\n\u001b[32m     91\u001b[39m     \u001b[38;5;28mprint\u001b[39m()  \u001b[38;5;66;03m# finish line after streaming LLM response\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m time.sleep(\u001b[32m0.1\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßæ User (final) in en-US: How can I assist you today?\n",
      "üßæ User (final) in en-US: Feel free to ask me any questions or let me know what information you need.\n",
      "üßæ User (final) in en-US: I'm here to help you.\n"
     ]
    }
   ],
   "source": [
    "import os, time, threading\n",
    "\n",
    "\n",
    "def on_final(text, lang):\n",
    "    global user_buffer\n",
    "    user_buffer += text.strip() + \"\\n\"\n",
    "    print(f\"üßæ User (final) in {lang}: {text}\")\n",
    "\n",
    "\n",
    "def assistant_speak(text):\n",
    "    global is_synthesizing\n",
    "    print(\"Hi there, I am a assistant_speak callback!\")\n",
    "    is_synthesizing = True\n",
    "    print(\"Syntetixing:\", is_synthesizing)\n",
    "    az_speach_synthesizer_client.start_speaking_text(text)\n",
    "\n",
    "\n",
    "def on_partial(text, lang):\n",
    "    global is_synthesizing\n",
    "    if is_synthesizing:\n",
    "        # az_speach_synthesizer_client.stop_speaking()\n",
    "        is_synthesizing = False\n",
    "    print(f\"üó£Ô∏è User (partial) in {lang}: {text}\")\n",
    "\n",
    "\n",
    "az_speech_recognizer_stream_client.set_partial_result_callback(on_partial)\n",
    "az_speech_recognizer_stream_client.set_final_result_callback(on_final)\n",
    "\n",
    "# Start recognition\n",
    "az_speech_recognizer_stream_client.start()\n",
    "print(\"üéôÔ∏è Speak now...\")\n",
    "\n",
    "# Start mic streaming thread\n",
    "import pyaudio\n",
    "\n",
    "RATE, CHANNELS, CHUNK = 16000, 1, 1024\n",
    "audio = pyaudio.PyAudio()\n",
    "stream = audio.open(\n",
    "    format=pyaudio.paInt16,\n",
    "    channels=CHANNELS,\n",
    "    rate=RATE,\n",
    "    input=True,\n",
    "    frames_per_buffer=CHUNK,\n",
    ")\n",
    "\n",
    "\n",
    "def mic_loop():\n",
    "    while True:\n",
    "        data = stream.read(CHUNK, exception_on_overflow=False)\n",
    "        az_speech_recognizer_stream_client.write_bytes(data)\n",
    "\n",
    "\n",
    "threading.Thread(target=mic_loop, daemon=True).start()\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "\n",
    "user_buffer = \"\"  # This should be filled in by your STT callback as before\n",
    "\n",
    "while True:\n",
    "    if user_buffer:\n",
    "        full_conversation = (\n",
    "            \"\\n\".join([f\"{m['role'].capitalize()}: {m['content']}\" for m in messages])\n",
    "            + f\"\\nUser: {user_buffer}\"\n",
    "        )\n",
    "        messages.append({\"role\": \"user\", \"content\": user_buffer})\n",
    "        user_buffer = \"\"  # clear after using\n",
    "\n",
    "        completion = client.chat.completions.create(\n",
    "            stream=True,\n",
    "            messages=messages,\n",
    "            max_tokens=4096,\n",
    "            temperature=1.0,\n",
    "            top_p=1.0,\n",
    "            model=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    "        )\n",
    "\n",
    "        collected_messages = []\n",
    "        last_tts_request = None\n",
    "\n",
    "        for chunk in completion:\n",
    "            if chunk.choices and hasattr(chunk.choices[0].delta, \"content\"):\n",
    "                chunk_text = chunk.choices[0].delta.content\n",
    "                if chunk_text:\n",
    "                    collected_messages.append(chunk_text)\n",
    "                    print(chunk_text, end=\"\", flush=True)\n",
    "                    # Check for sentence end to stream to TTS\n",
    "                    if chunk_text in TTS_ENDS:\n",
    "                        text = \"\".join(collected_messages).strip()\n",
    "                        last_tts_request = assistant_speak(text)\n",
    "                        collected_messages.clear()\n",
    "        print()  # finish line after streaming LLM response\n",
    "\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfa1d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-08-20 12:08:25,019] INFO - src.speech.text_to_speech: [üõë] Stopping speech synthesis...\n",
      "INFO:src.speech.text_to_speech:[üõë] Stopping speech synthesis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-08-20 12:08:25,192] WARNING - src.speech.speech_recognizer: Recognition canceled: SpeechRecognitionCanceledEventArgs(session_id=f950613fe5c84370ad72d907b4fdd49c, result=SpeechRecognitionResult(result_id=311b0a90945c4301a6df2bc92bf2c447, text=\"\", reason=ResultReason.Canceled))\n",
      "WARNING:src.speech.speech_recognizer:Recognition canceled: SpeechRecognitionCanceledEventArgs(session_id=f950613fe5c84370ad72d907b4fdd49c, result=SpeechRecognitionResult(result_id=311b0a90945c4301a6df2bc92bf2c447, text=\"\", reason=ResultReason.Canceled))\n",
      "[2025-08-20 12:08:25,210] WARNING - src.speech.speech_recognizer: Reason: CancellationReason.EndOfStream, Error: \n",
      "WARNING:src.speech.speech_recognizer:Reason: CancellationReason.EndOfStream, Error: \n",
      "[2025-08-20 12:08:25,222] INFO - src.speech.speech_recognizer: Session stopped.\n",
      "INFO:src.speech.speech_recognizer:Session stopped.\n"
     ]
    }
   ],
   "source": [
    "az_speech_recognizer_stream_client.close_stream()\n",
    "az_speach_synthesizer_client.stop_speaking()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audioagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
