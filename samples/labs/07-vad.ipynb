{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bb10de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed directory to: c:\\Users\\pablosal\\Desktop\\gbb-ai-audio-agent\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "# set the directory to the location of the script\n",
    "try:\n",
    "    os.chdir(\"../..\")\n",
    "    target_directory = os.getenv(\n",
    "        \"TARGET_DIRECTORY\", os.getcwd()\n",
    "    )  # Use environment variable if available\n",
    "    if os.path.exists(target_directory):\n",
    "        os.chdir(target_directory)\n",
    "        print(f\"Changed directory to: {os.getcwd()}\")\n",
    "        logging.info(f\"Successfully changed directory to: {os.getcwd()}\")\n",
    "    else:\n",
    "        logging.error(f\"Directory does not exist: {target_directory}\")\n",
    "except Exception as e:\n",
    "    logging.exception(f\"An error occurred while changing directory: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d625f8f0",
   "metadata": {},
   "source": [
    "## Azure AI Speech (VAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1059370f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import time, threading, logging, os\n",
    "from typing import Optional, Iterator\n",
    "import numpy as np, pyaudio, torch\n",
    "\n",
    "from src.speech.text_to_speech import SpeechSynthesizer\n",
    "from src.speech.speech_recognizer import StreamingSpeechRecognizerFromBytes\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "logger = logging.getLogger(\"nb.voice\")\n",
    "if not logger.handlers:\n",
    "    h = logging.StreamHandler()\n",
    "    h.setFormatter(logging.Formatter(\"%(asctime)s %(levelname)s %(message)s\"))\n",
    "    logger.addHandler(h)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# ---------------- Silero VAD setup ----------------\n",
    "try:\n",
    "    from silero_vad import load_silero_vad, VADIterator\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"pip install silero-vad torch\") from e\n",
    "\n",
    "def int2float(x: np.ndarray) -> np.ndarray:\n",
    "    y = x.astype(\"float32\")\n",
    "    y *= 1.0 / 32768.0\n",
    "    return y\n",
    "\n",
    "class PCMFramer:\n",
    "    def __init__(self, frame_bytes: int = 640) -> None:  # 20ms @ 16k mono PCM16\n",
    "        self.buf = bytearray()\n",
    "        self.n = frame_bytes\n",
    "    def feed(self, chunk: bytes) -> Iterator[bytes]:\n",
    "        if not chunk:\n",
    "            return iter(())\n",
    "        self.buf.extend(chunk)\n",
    "        out = []\n",
    "        while len(self.buf) >= self.n:\n",
    "            out.append(bytes(self.buf[: self.n]))\n",
    "            del self.buf[: self.n]\n",
    "        return iter(out)\n",
    "\n",
    "# ---------------- Globals for re-entrant notebook runs ----------------\n",
    "STATE = {\n",
    "    \"stop\": threading.Event(),\n",
    "    \"mic_thread\": None,\n",
    "    \"audio\": None,\n",
    "    \"stream\": None,\n",
    "    \"recognizer\": None,\n",
    "    \"tts\": None,\n",
    "    \"client\": None,\n",
    "    \"vad\": None,\n",
    "    \"framer\": PCMFramer(1024),  # keep CHUNK=1024 for notebook compatibility\n",
    "    \"user_buf\": \"\",\n",
    "    \"is_synth\": False,\n",
    "    \"vad_started\": False,\n",
    "    \"vad_trigs\": 0,\n",
    "    \"barge_latch\": False,\n",
    "    \"last_barge_ms\": 0,\n",
    "    \"pending_tts\": [],\n",
    "}\n",
    "\n",
    "# ---- tuning toggles ----\n",
    "VAD_START_N = 2                 # need 2×20ms frames to confirm start\n",
    "VAD_THRESH = 1                 # raise toward 0.7 if noisy; lower toward 0.58 if too slow\n",
    "VAD_END_SIL_MS = 350           # min silence for end-of-speech\n",
    "PARTIAL_FALLBACK = False        # set True after you confirm VAD is solid\n",
    "BARGE_DEBOUNCE_MS = 250         # don’t spam stop_speaking()\n",
    "COOLDOWN_AFTER_BARGE_MS = 350   # don’t feed STT for this long after barge-in (absorbs TTS tail)\n",
    "TTS_ENDS = {\".\", \"!\", \"?\", \";\", \"\\n\"}\n",
    "\n",
    "# ---------------- helpers ----------------\n",
    "def _safe_stop_tts():\n",
    "    now = int(time.time() * 1000)\n",
    "    if now - STATE[\"last_barge_ms\"] < BARGE_DEBOUNCE_MS:\n",
    "        return\n",
    "    STATE[\"last_barge_ms\"] = now\n",
    "    if STATE[\"is_synth\"]:\n",
    "        try:\n",
    "            STATE[\"tts\"].stop_speaking()\n",
    "            logger.info(\"TTS stopped (barge-in)\")\n",
    "        except Exception as e:\n",
    "            logger.warning(\"stop_speaking failed: %s\", e)\n",
    "        STATE[\"is_synth\"] = False\n",
    "    STATE[\"barge_latch\"] = True\n",
    "    # start a short cooldown window where we mute mic->STT\n",
    "    STATE[\"cooldown_until_ms\"] = now + COOLDOWN_AFTER_BARGE_MS\n",
    "\n",
    "def _on_partial(text: str, lang: str):\n",
    "    if PARTIAL_FALLBACK and not STATE[\"barge_latch\"] and STATE[\"is_synth\"]:\n",
    "        _safe_stop_tts()\n",
    "        logger.info(\"Barge-in via STT partial (fallback)\")\n",
    "    # optional: print partials\n",
    "    # logger.debug(\"partial[%s]: %s\", lang, text)\n",
    "\n",
    "def _on_final(text: str, lang: str):\n",
    "    STATE[\"user_buf\"] += text.strip() + \"\\n\"\n",
    "    logger.info(\"final[%s]: %s\", lang, text)\n",
    "\n",
    "def _assistant_speak(text: str):\n",
    "    if not text:\n",
    "        return\n",
    "    # half-duplex: if user talking or we just barged-in, queue this\n",
    "    if STATE[\"vad_started\"] or STATE[\"barge_latch\"]:\n",
    "        STATE[\"pending_tts\"].append(text)\n",
    "        logger.info(\"queued TTS (%d chars)\", len(text))\n",
    "        return\n",
    "    STATE[\"is_synth\"] = True\n",
    "    STATE[\"tts\"].start_speaking_text(text)\n",
    "    logger.info(\"speaking now: %.60r\", text)\n",
    "\n",
    "def _flush_tts_queue_if_idle():\n",
    "    if STATE[\"vad_started\"] or STATE[\"barge_latch\"] or STATE[\"is_synth\"]:\n",
    "        return\n",
    "    while STATE[\"pending_tts\"]:\n",
    "        text = STATE[\"pending_tts\"].pop(0)\n",
    "        STATE[\"is_synth\"] = True\n",
    "        STATE[\"tts\"].start_speaking_text(text)\n",
    "        logger.info(\"flushing queued: %.60r\", text)\n",
    "\n",
    "# ---------------- mic thread ----------------\n",
    "def _mic_loop():\n",
    "    RATE, CHANNELS, CHUNK = 16000, 1, 1024  # keep your CHUNK\n",
    "    audio = pyaudio.PyAudio()\n",
    "    stream = audio.open(format=pyaudio.paInt16, channels=CHANNELS, rate=RATE,\n",
    "                        input=True, frames_per_buffer=CHUNK)\n",
    "    STATE[\"audio\"], STATE[\"stream\"] = audio, stream\n",
    "\n",
    "    framer = STATE[\"framer\"]\n",
    "    vad = STATE[\"vad\"]\n",
    "    cooldown_until = 0\n",
    "\n",
    "    while not STATE[\"stop\"].is_set():\n",
    "        try:\n",
    "            data = stream.read(CHUNK, exception_on_overflow=False)\n",
    "\n",
    "            now = int(time.time() * 1000)\n",
    "            push_ok = True\n",
    "\n",
    "            # Half-duplex: if TTS is active and we haven't barged-in, mute STT feed\n",
    "            if STATE[\"is_synth\"] and not STATE[\"barge_latch\"] and not STATE[\"vad_started\"]:\n",
    "                push_ok = False\n",
    "\n",
    "            # Short cooldown after barge-in to absorb TTS tail\n",
    "            if \"cooldown_until_ms\" in STATE and now < STATE[\"cooldown_until_ms\"]:\n",
    "                push_ok = False\n",
    "\n",
    "            if push_ok and STATE[\"recognizer\"] is not None:\n",
    "                STATE[\"recognizer\"].write_bytes(data)\n",
    "\n",
    "            # VAD on 20ms frames\n",
    "            for f in framer.feed(data):\n",
    "                x = torch.from_numpy(int2float(np.frombuffer(f, dtype=np.int16)))\n",
    "                seg = vad(x)  # updates .triggered internally\n",
    "\n",
    "                if getattr(vad, \"triggered\", False) and not STATE[\"vad_started\"]:\n",
    "                    STATE[\"vad_trigs\"] += 1\n",
    "                    if STATE[\"vad_trigs\"] >= VAD_START_N and not STATE[\"barge_latch\"]:\n",
    "                        STATE[\"vad_started\"] = True\n",
    "                        STATE[\"vad_trigs\"] = 0\n",
    "                        _safe_stop_tts()\n",
    "                        logger.info(\"VAD start\")\n",
    "                elif not getattr(vad, \"triggered\", False):\n",
    "                    STATE[\"vad_trigs\"] = 0\n",
    "\n",
    "                # END: when iterator returns a chunk, that marks end-of-speech\n",
    "                if seg is not None and STATE[\"vad_started\"]:\n",
    "                    STATE[\"vad_started\"] = False\n",
    "                    STATE[\"barge_latch\"] = False   # allow TTS again\n",
    "                    logger.info(\"VAD end\")\n",
    "                    _flush_tts_queue_if_idle()\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(\"mic loop err: %s\", e)\n",
    "            time.sleep(0.01)\n",
    "\n",
    "    # cleanup\n",
    "    try:\n",
    "        stream.stop_stream(); stream.close(); audio.terminate()\n",
    "    except Exception:\n",
    "        pass\n",
    "    STATE[\"audio\"], STATE[\"stream\"] = None, None\n",
    "\n",
    "# ---------------- public API for the notebook ----------------\n",
    "def start_pipeline():\n",
    "    \"\"\"Idempotent start; safe to re-run in Jupyter.\"\"\"\n",
    "    # stop prior run if alive\n",
    "    if STATE[\"mic_thread\"] and STATE[\"mic_thread\"].is_alive():\n",
    "        stop_pipeline()\n",
    "\n",
    "    # Azure clients\n",
    "    recognizer = StreamingSpeechRecognizerFromBytes(\n",
    "        use_semantic_segmentation=False,\n",
    "        vad_silence_timeout_ms=800,\n",
    "        audio_format=\"pcm\",\n",
    "        candidate_languages=[\"en-US\", \"fr-FR\", \"de-DE\", \"es-ES\", \"it-IT\"],\n",
    "    )\n",
    "    recognizer.set_partial_result_callback(_on_partial)\n",
    "    recognizer.set_final_result_callback(_on_final)\n",
    "    recognizer.start()\n",
    "    STATE[\"recognizer\"] = recognizer\n",
    "\n",
    "    tts = SpeechSynthesizer(voice=\"en-US-Ava:DragonHDLatestNeural\", playback=\"always\")\n",
    "    STATE[\"tts\"] = tts\n",
    "\n",
    "    # Silero VAD\n",
    "    model = load_silero_vad()\n",
    "    STATE[\"vad\"] = VADIterator(\n",
    "        model, threshold=VAD_THRESH, sampling_rate=16000,\n",
    "        min_silence_duration_ms=VAD_END_SIL_MS, speech_pad_ms=120\n",
    "    )\n",
    "\n",
    "    # reset state latches\n",
    "    STATE[\"stop\"].clear()\n",
    "    STATE[\"user_buf\"] = \"\"\n",
    "    STATE[\"is_synth\"] = False\n",
    "    STATE[\"vad_started\"] = False\n",
    "    STATE[\"vad_trigs\"] = 0\n",
    "    STATE[\"barge_latch\"] = False\n",
    "    STATE[\"last_barge_ms\"] = 0\n",
    "    STATE[\"pending_tts\"].clear()\n",
    "    STATE[\"cooldown_until_ms\"] = 0\n",
    "\n",
    "    # start mic thread\n",
    "    th = threading.Thread(target=_mic_loop, daemon=True)\n",
    "    STATE[\"mic_thread\"] = th\n",
    "    th.start()\n",
    "    logger.info(\"🎙️ Pipeline started. Speak now...\")\n",
    "\n",
    "def stop_pipeline():\n",
    "    \"\"\"Stop mic thread, STT, and TTS cleanly.\"\"\"\n",
    "    try:\n",
    "        STATE[\"stop\"].set()\n",
    "        if STATE[\"mic_thread\"]:\n",
    "            STATE[\"mic_thread\"].join(timeout=1.5)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        if STATE[\"recognizer\"]:\n",
    "            STATE[\"recognizer\"].stop()\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        if STATE[\"tts\"]:\n",
    "            STATE[\"tts\"].stop_speaking()\n",
    "    except Exception:\n",
    "        pass\n",
    "    STATE[\"mic_thread\"] = None\n",
    "    logger.info(\"Pipeline stopped.\")\n",
    "\n",
    "# ---------------- LLM loop example (kept simple) ----------------\n",
    "def llm_step_and_tts(client: AzureOpenAI, messages: list[dict]):\n",
    "    \"\"\"Call AOAI streaming, speak sentence-by-sentence with half-duplex rules.\"\"\"\n",
    "    collected = []\n",
    "    for chunk in client.chat.completions.create(\n",
    "        stream=True,\n",
    "        messages=messages,\n",
    "        max_tokens=4096,\n",
    "        temperature=1.0,\n",
    "        top_p=1.0,\n",
    "        model=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    "    ):\n",
    "        if chunk.choices and hasattr(chunk.choices[0].delta, \"content\"):\n",
    "            piece = chunk.choices[0].delta.content\n",
    "            if not piece:\n",
    "                continue\n",
    "            collected.append(piece)\n",
    "            if piece in TTS_ENDS:\n",
    "                _assistant_speak(\"\".join(collected).strip())\n",
    "                collected.clear()\n",
    "    if collected:\n",
    "        _assistant_speak(\"\".join(collected).strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abd08f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-08-11 16:57:05,033] INFO - micro: Azure Monitor tracing initialized for speech recognizer\n",
      "INFO:micro:Azure Monitor tracing initialized for speech recognizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-08-11 16:57:05,049] INFO - micro: Creating SpeechConfig with API key authentication\n",
      "INFO:micro:Creating SpeechConfig with API key authentication\n",
      "[2025-08-11 16:57:05,064] INFO - micro: Starting recognition from byte stream…\n",
      "INFO:micro:Starting recognition from byte stream…\n",
      "[2025-08-11 16:57:05,083] INFO - micro: Speech-SDK prepare_start – format=pcm  neuralFE=False  diar=True\n",
      "INFO:micro:Speech-SDK prepare_start – format=pcm  neuralFE=False  diar=True\n",
      "[2025-08-11 16:57:05,103] INFO - micro: Speech-SDK ready (neuralFE=False, diarisation=True, speakers=2)\n",
      "INFO:micro:Speech-SDK ready (neuralFE=False, diarisation=True, speakers=2)\n",
      "[2025-08-11 16:57:05,124] INFO - micro: Recognition started.\n",
      "INFO:micro:Recognition started.\n",
      "[2025-08-11 16:57:05,146] INFO - micro: Azure Monitor tracing initialized for speech synthesizer\n",
      "INFO:micro:Azure Monitor tracing initialized for speech synthesizer\n",
      "[2025-08-11 16:57:05,164] INFO - micro: Creating SpeechConfig with API key authentication\n",
      "INFO:micro:Creating SpeechConfig with API key authentication\n",
      "[2025-08-11 16:57:05,189] INFO - micro: Speech synthesizer initialized successfully\n",
      "INFO:micro:Speech synthesizer initialized successfully\n",
      "2025-08-11 16:57:05,646 INFO 🎙️ Pipeline started. Speak now...\n",
      "INFO:nb.voice:🎙️ Pipeline started. Speak now...\n",
      "2025-08-11 16:57:11,917 INFO final[en-US]: Because services.\n",
      "INFO:nb.voice:final[en-US]: Because services.\n",
      "[2025-08-11 16:57:12,954] INFO - micro: [🔊] Starting streaming speech synthesis for text: It seems like your message got cut off or is missi...\n",
      "INFO:micro:[🔊] Starting streaming speech synthesis for text: It seems like your message got cut off or is missi...\n",
      "2025-08-11 16:57:12,972 INFO speaking now: 'It seems like your message got cut off or is missing some c\n",
      "INFO:nb.voice:speaking now: 'It seems like your message got cut off or is missing some c\n",
      "[2025-08-11 16:57:13,058] INFO - micro: [🔊] Starting streaming speech synthesis for text: Could you please provide more details or clarify w...\n",
      "INFO:micro:[🔊] Starting streaming speech synthesis for text: Could you please provide more details or clarify w...\n",
      "2025-08-11 16:57:13,076 INFO speaking now: 'Could you please provide more details or clarify what you\\'\n",
      "INFO:nb.voice:speaking now: 'Could you please provide more details or clarify what you\\'\n",
      "2025-08-11 16:58:04,575 INFO final[en-US]: That's where all the magic happens. But like nobody, nobody tied it together yet.\n",
      "INFO:nb.voice:final[en-US]: That's where all the magic happens. But like nobody, nobody tied it together yet.\n",
      "[2025-08-11 16:58:04,620] INFO - micro: Session stopped.\n",
      "INFO:micro:Session stopped.\n",
      "WARNING:opentelemetry.trace.status:description should only be set when status_code is set to StatusCode.ERROR\n",
      "[2025-08-11 16:58:04,627] INFO - micro: Recognition stopped.\n",
      "INFO:micro:Recognition stopped.\n",
      "[2025-08-11 16:58:04,633] INFO - micro: [🛑] Stopping speech synthesis...\n",
      "INFO:micro:[🛑] Stopping speech synthesis...\n",
      "2025-08-11 16:58:04,641 INFO Pipeline stopped.\n",
      "INFO:nb.voice:Pipeline stopped.\n"
     ]
    }
   ],
   "source": [
    "client = AzureOpenAI(\n",
    "    api_version=\"2025-02-01-preview\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    ")\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "\n",
    "start_pipeline()\n",
    "try:\n",
    "    while True:\n",
    "        if STATE[\"user_buf\"]:\n",
    "            messages.append({\"role\": \"user\", \"content\": STATE[\"user_buf\"]})\n",
    "            STATE[\"user_buf\"] = \"\"\n",
    "            llm_step_and_tts(client, messages)\n",
    "        time.sleep(0.05)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    stop_pipeline()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audioagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
