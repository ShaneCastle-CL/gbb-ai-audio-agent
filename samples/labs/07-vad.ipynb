{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bb10de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed directory to: c:\\Users\\pablosal\\Desktop\\gbb-ai-audio-agent\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "# set the directory to the location of the script\n",
    "try:\n",
    "    os.chdir(\"../..\")\n",
    "    target_directory = os.getenv(\n",
    "        \"TARGET_DIRECTORY\", os.getcwd()\n",
    "    )  # Use environment variable if available\n",
    "    if os.path.exists(target_directory):\n",
    "        os.chdir(target_directory)\n",
    "        print(f\"Changed directory to: {os.getcwd()}\")\n",
    "        logging.info(f\"Successfully changed directory to: {os.getcwd()}\")\n",
    "    else:\n",
    "        logging.error(f\"Directory does not exist: {target_directory}\")\n",
    "except Exception as e:\n",
    "    logging.exception(f\"An error occurred while changing directory: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d625f8f0",
   "metadata": {},
   "source": [
    "## Azure AI Speech (VAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61629fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 21:01:11,259 - micro - MainProcess - INFO     Speech synthesizer initialized successfully (text_to_speech.py:__init__:122)\n",
      "INFO:micro:Speech synthesizer initialized successfully\n"
     ]
    }
   ],
   "source": [
    "from src.speech.text_to_speech import SpeechSynthesizer\n",
    "from src.speech.speech_recognizer import StreamingSpeechRecognizerFromBytes\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "if \"az_speech_recognizer_stream_client\" not in locals():\n",
    "    az_speech_recognizer_stream_client = StreamingSpeechRecognizerFromBytes(\n",
    "        use_semantic_segmentation=False,\n",
    "        vad_silence_timeout_ms=800,\n",
    "        audio_format=\"pcm\",\n",
    "        candidate_languages=[\"en-US\", \"fr-FR\", \"de-DE\", \"es-ES\", \"it-IT\"],\n",
    "\n",
    "    )\n",
    "\n",
    "if \"az_speach_synthesizer_client\" not in locals():\n",
    "    az_speach_synthesizer_client = SpeechSynthesizer(voice=\"en-US-Ava:DragonHDLatestNeural\", playback=\"always\")\n",
    "\n",
    "# Ensure Azure OpenAI client is initialized only if not already defined\n",
    "if \"client\" not in locals():\n",
    "    client = AzureOpenAI(\n",
    "        api_version=\"2025-02-01-preview\",\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cb2ca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define end-of-sentence markers for TTS\n",
    "TTS_ENDS = {\".\", \"!\", \"?\", \";\", \"\\n\"}\n",
    "\n",
    "# Flags and buffers\n",
    "is_synthesizing = False\n",
    "user_buffer = \"\"\n",
    "assistant_buffer = \"\"\n",
    "tts_thread = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3152203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 21:01:12,659 - micro - MainProcess - INFO     Starting recognition from byte stream... (speech_recognizer.py:start:91)\n",
      "INFO:micro:Starting recognition from byte stream...\n",
      "2025-07-20 21:01:12,693 - micro - MainProcess - INFO     Recognition started. (speech_recognizer.py:start:153)\n",
      "INFO:micro:Recognition started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎙️ Speak now...\n",
      "🗣️ User (partial) in en-US: umm look\n",
      "🗣️ User (partial) in en-US: umm look i'm trying\n",
      "🗣️ User (partial) in en-US: umm look i'm trying to\n",
      "🗣️ User (partial) in en-US: umm look i'm trying to explain\n",
      "🗣️ User (partial) in en-US: umm look i'm trying to explain madrid\n",
      "🗣️ User (partial) in en-US: umm look i'm trying to explain madrid uh but\n",
      "🗣️ User (partial) in en-US: umm look i'm trying to explain madrid uh but i mean\n",
      "🗣️ User (partial) in en-US: umm look i'm trying to explain madrid uh but i mean i don't\n",
      "🗣️ User (partial) in en-US: umm look i'm trying to explain madrid uh but i mean i don't know\n",
      "🗣️ User (partial) in en-US: umm look i'm trying to explain madrid uh but i mean i don't know if\n",
      "🗣️ User (partial) in en-US: umm look i'm trying to explain madrid uh but i mean i don't know if you\n",
      "🗣️ User (partial) in en-US: umm look i'm trying to explain madrid uh but i mean i don't know if you are able\n",
      "🗣️ User (partial) in en-US: umm look i'm trying to explain madrid uh but i mean i don't know if you are able to\n",
      "🗣️ User (partial) in en-US: umm look i'm trying to explain madrid uh but i mean i don't know if you are able to understand\n",
      "🗣️ User (partial) in en-US: umm look i'm trying to explain madrid uh but i mean i don't know if you are able to understand what\n",
      "🗣️ User (partial) in en-US: umm look i'm trying to explain madrid uh but i mean i don't know if you are able to understand what i'm\n",
      "🗣️ User (partial) in en-US: umm look i'm trying to explain madrid uh but i mean i don't know if you are able to understand what i'm trying\n",
      "🗣️ User (partial) in en-US: umm look i'm trying to explain madrid uh but i mean i don't know if you are able to understand what i'm trying to\n",
      "🗣️ User (partial) in en-US: umm look i'm trying to explain madrid uh but i mean i don't know if you are able to understand what i'm trying to say\n",
      "🗣️ User (partial) in en-US: umm look i'm trying to explain madrid uh but i mean i don't know if you are able to understand what i'm trying to say i mean\n",
      "🗣️ User (partial) in en-US: umm look i'm trying to explain madrid uh but i mean i don't know if you are able to understand what i'm trying to say i mean i don't know\n",
      "🗣️ User (partial) in en-US: umm look i'm trying to explain madrid uh but i mean i don't know if you are able to understand what i'm trying to say i mean i don't know like\n",
      "🗣️ User (partial) in en-US: umm look i'm trying to explain madrid uh but i mean i don't know if you are able to understand what i'm trying to say i mean i don't know like i\n",
      "🗣️ User (partial) in en-US: umm look i'm trying to explain madrid uh but i mean i don't know if you are able to understand what i'm trying to say i mean i don't know like i just don't\n",
      "🗣️ User (partial) in en-US: umm look i'm trying to explain madrid uh but i mean i don't know if you are able to understand what i'm trying to say i mean i don't know like i just don't know\n",
      "🗣️ User (partial) in en-US: umm look i'm trying to explain madrid uh but i mean i don't know if you are able to understand what i'm trying to say i mean i don't know like i just don't know if\n",
      "🗣️ User (partial) in en-US: umm look i'm trying to explain madrid uh but i mean i don't know if you are able to understand what i'm trying to say i mean\n",
      "🧾 User (final) in en-US: Umm, look, I'm trying to explain Madrid, uh, but I mean, I don't know if you are able to understand what I'm trying to say I mean.\n",
      "Of course, I can help you with that!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 21:01:39,399 - micro - MainProcess - INFO     [🔊] Starting streaming speech synthesis for text: Of course, I can help you with that!... (text_to_speech.py:start_speaking_text:270)\n",
      "INFO:micro:[🔊] Starting streaming speech synthesis for text: Of course, I can help you with that!...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there, I am a assistant_speak callback!\n",
      "Syntetixing: True\n",
      " It sounds like you're trying to describe Madrid."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 21:01:39,421 - micro - MainProcess - INFO     [🔊] Starting streaming speech synthesis for text: It sounds like you're trying to describe Madrid.... (text_to_speech.py:start_speaking_text:270)\n",
      "INFO:micro:[🔊] Starting streaming speech synthesis for text: It sounds like you're trying to describe Madrid....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there, I am a assistant_speak callback!\n",
      "Syntetixing: True\n",
      " What specific aspects of the city do you want to explain?"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 21:01:39,451 - micro - MainProcess - INFO     [🔊] Starting streaming speech synthesis for text: What specific aspects of the city do you want to e... (text_to_speech.py:start_speaking_text:270)\n",
      "INFO:micro:[🔊] Starting streaming speech synthesis for text: What specific aspects of the city do you want to e...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there, I am a assistant_speak callback!\n",
      "Syntetixing: True\n",
      " For example, are you looking to talk about its culture, history, attractions, or maybe its cuisine?"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 21:01:39,808 - micro - MainProcess - INFO     [🔊] Starting streaming speech synthesis for text: For example, are you looking to talk about its cul... (text_to_speech.py:start_speaking_text:270)\n",
      "INFO:micro:[🔊] Starting streaming speech synthesis for text: For example, are you looking to talk about its cul...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there, I am a assistant_speak callback!\n",
      "Syntetixing: True\n",
      " Let me know how I can assist you!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 21:01:39,846 - micro - MainProcess - INFO     [🔊] Starting streaming speech synthesis for text: Let me know how I can assist you!... (text_to_speech.py:start_speaking_text:270)\n",
      "INFO:micro:[🔊] Starting streaming speech synthesis for text: Let me know how I can assist you!...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there, I am a assistant_speak callback!\n",
      "Syntetixing: True\n",
      "\n",
      "🗣️ User (partial) in es-ES: no yo\n",
      "🗣️ User (partial) in es-ES: no yo no\n",
      "🗣️ User (partial) in es-ES: no yo no creo\n",
      "🗣️ User (partial) in es-ES: no yo no creo que\n",
      "🗣️ User (partial) in es-ES: no yo no creo que te inter\n",
      "🗣️ User (partial) in es-ES: no yo no creo que te enteres de nada\n",
      "🗣️ User (partial) in es-ES: no yo no creo que te enteres de nada de\n",
      "🗣️ User (partial) in es-ES: no yo no creo que te enteres de nada de lo que\n",
      "🗣️ User (partial) in es-ES: no yo no creo que te enteres de nada de lo que te estoy\n",
      "🗣️ User (partial) in es-ES: no yo no creo que te enteres de nada de lo que te estoy hablando\n",
      "🗣️ User (partial) in es-ES: no yo no creo que te enteres de nada de lo que te estoy hablando porque\n",
      "🗣️ User (partial) in es-ES: no yo no creo que te enteres de nada de lo que te estoy hablando porque no no\n",
      "🗣️ User (partial) in es-ES: no yo no creo que te enteres de nada de lo que te estoy hablando porque no no escuchas\n",
      "🗣️ User (partial) in es-ES: no yo no creo que te enteres de nada de lo que te estoy hablando porque no no escuchas bien\n",
      "🗣️ User (partial) in es-ES: no yo no creo que te enteres de nada de lo que te estoy hablando porque no no escuchas bien no sé si\n",
      "🗣️ User (partial) in es-ES: no yo no creo que te enteres de nada de lo que te estoy hablando porque no no escuchas bien no sé si estás escuch\n",
      "🗣️ User (partial) in es-ES: no yo no creo que te enteres de nada de lo que te estoy hablando porque no no escuchas bien no sé si estás escuchando bien\n",
      "🧾 User (final) in es-ES: No, yo no creo que te enteres de nada de lo que te estoy hablando porque no no escuchas bien, no sé si estás escuchando bien.\n",
      "Entiendo que puede ser frustrante."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 21:02:07,184 - micro - MainProcess - INFO     [🔊] Starting streaming speech synthesis for text: Entiendo que puede ser frustrante.... (text_to_speech.py:start_speaking_text:270)\n",
      "INFO:micro:[🔊] Starting streaming speech synthesis for text: Entiendo que puede ser frustrante....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there, I am a assistant_speak callback!\n",
      "Syntetixing: True\n",
      " Estoy aquí para ayudarte a explicar lo que quieras sobre Madrid."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 21:02:07,206 - micro - MainProcess - INFO     [🔊] Starting streaming speech synthesis for text: Estoy aquí para ayudarte a explicar lo que quieras... (text_to_speech.py:start_speaking_text:270)\n",
      "INFO:micro:[🔊] Starting streaming speech synthesis for text: Estoy aquí para ayudarte a explicar lo que quieras...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there, I am a assistant_speak callback!\n",
      "Syntetixing: True\n",
      " Si me das un poco más de contexto o detalles sobre lo que te gustaría explicar, podría ser más útil."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 21:02:07,730 - micro - MainProcess - INFO     [🔊] Starting streaming speech synthesis for text: Si me das un poco más de contexto o detalles sobre... (text_to_speech.py:start_speaking_text:270)\n",
      "INFO:micro:[🔊] Starting streaming speech synthesis for text: Si me das un poco más de contexto o detalles sobre...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there, I am a assistant_speak callback!\n",
      "Syntetixing: True\n",
      " ¿Te gustaría hablar sobre la cultura, la historia, los lugares turísticos, o algo más específico?"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 21:02:07,767 - micro - MainProcess - INFO     [🔊] Starting streaming speech synthesis for text: ¿Te gustaría hablar sobre la cultura, la historia,... (text_to_speech.py:start_speaking_text:270)\n",
      "INFO:micro:[🔊] Starting streaming speech synthesis for text: ¿Te gustaría hablar sobre la cultura, la historia,...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there, I am a assistant_speak callback!\n",
      "Syntetixing: True\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 21:02:18,998 - micro - MainProcess - INFO     Session stopped. (speech_recognizer.py:_on_session_stopped:282)\n",
      "INFO:micro:Session stopped.\n",
      "2025-07-20 21:02:19,004 - micro - MainProcess - INFO     Recognition stopped. (speech_recognizer.py:stop:228)\n",
      "INFO:micro:Recognition stopped.\n",
      "2025-07-20 21:02:19,004 - micro - MainProcess - INFO     [🛑] Stopping speech synthesis... (text_to_speech.py:stop_speaking:291)\n",
      "INFO:micro:[🛑] Stopping speech synthesis...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting...\n",
      "Stream stopped and audio terminated.\n"
     ]
    }
   ],
   "source": [
    "import os, time, threading\n",
    "\n",
    "def on_final(text, lang):\n",
    "    global user_buffer\n",
    "    user_buffer += text.strip() + \"\\n\"\n",
    "    print(f\"🧾 User (final) in {lang}: {text}\")\n",
    "\n",
    "\n",
    "def assistant_speak(text):\n",
    "    global is_synthesizing\n",
    "    print(\"Hi there, I am a assistant_speak callback!\")\n",
    "    is_synthesizing = True\n",
    "    print(\"Syntetixing:\", is_synthesizing)\n",
    "    az_speach_synthesizer_client.start_speaking_text(text)\n",
    "\n",
    "\n",
    "def on_partial(text, lang):\n",
    "    global is_synthesizing\n",
    "    if is_synthesizing:\n",
    "        # az_speach_synthesizer_client.stop_speaking()\n",
    "        is_synthesizing = False\n",
    "    print(f\"🗣️ User (partial) in {lang}: {text}\")\n",
    "\n",
    "\n",
    "az_speech_recognizer_stream_client.set_partial_result_callback(on_partial)\n",
    "az_speech_recognizer_stream_client.set_final_result_callback(on_final)\n",
    "\n",
    "# Start recognition\n",
    "az_speech_recognizer_stream_client.start()\n",
    "print(\"🎙️ Speak now...\")\n",
    "\n",
    "# Start mic streaming thread\n",
    "import pyaudio\n",
    "\n",
    "RATE, CHANNELS, CHUNK = 16000, 1, 1024\n",
    "audio = pyaudio.PyAudio()\n",
    "stream = audio.open(\n",
    "    format=pyaudio.paInt16,\n",
    "    channels=CHANNELS,\n",
    "    rate=RATE,\n",
    "    input=True,\n",
    "    frames_per_buffer=CHUNK,\n",
    ")\n",
    "\n",
    "\n",
    "def mic_loop():\n",
    "    while True:\n",
    "        data = stream.read(CHUNK, exception_on_overflow=False)\n",
    "        az_speech_recognizer_stream_client.write_bytes(data)\n",
    "\n",
    "\n",
    "threading.Thread(target=mic_loop, daemon=True).start()\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "\n",
    "user_buffer = \"\"  # This should be filled in by your STT callback as before\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        if user_buffer:\n",
    "            full_conversation = (\n",
    "                \"\\n\".join([f\"{m['role'].capitalize()}: {m['content']}\" for m in messages])\n",
    "                + f\"\\nUser: {user_buffer}\"\n",
    "            )\n",
    "            messages.append({\"role\": \"user\", \"content\": user_buffer})\n",
    "            user_buffer = \"\"  # clear after using\n",
    "\n",
    "            completion = client.chat.completions.create(\n",
    "                stream=True,\n",
    "                messages=messages,\n",
    "                max_tokens=4096,\n",
    "                temperature=1.0,\n",
    "                top_p=1.0,\n",
    "                model=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    "            )\n",
    "\n",
    "            collected_messages = []\n",
    "            last_tts_request = None\n",
    "\n",
    "            for chunk in completion:\n",
    "                if chunk.choices and hasattr(chunk.choices[0].delta, \"content\"):\n",
    "                    chunk_text = chunk.choices[0].delta.content\n",
    "                    if chunk_text:\n",
    "                        collected_messages.append(chunk_text)\n",
    "                        print(chunk_text, end=\"\", flush=True)\n",
    "                        # Check for sentence end to stream to TTS\n",
    "                        if chunk_text in TTS_ENDS:\n",
    "                            text = \"\".join(collected_messages).strip()\n",
    "                            last_tts_request = assistant_speak(text)\n",
    "                            collected_messages.clear()\n",
    "            print()  # finish line after streaming LLM response\n",
    "\n",
    "        time.sleep(0.1)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting...\")\n",
    "\n",
    "finally:\n",
    "    az_speech_recognizer_stream_client.stop()\n",
    "    az_speach_synthesizer_client.stop_speaking()\n",
    "    print(\"Stream stopped and audio terminated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83d48a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "speech_key   = os.getenv(\"AZURE_SPEECH_KEY\")\n",
    "speech_region = os.getenv(\"AZURE_SPEECH_REGION\")\n",
    "\n",
    "speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=speech_region)\n",
    "\n",
    "# --- 1) segmentation knobs -------------------------------------------------\n",
    "speech_config.set_property(          # content-aware breaks\n",
    "    speechsdk.PropertyId.Speech_SegmentationStrategy, \"Semantic\")\n",
    "speech_config.set_property(          # max 60 s per phrase fallback\n",
    "    speechsdk.PropertyId.Speech_SegmentationMaximumTimeMs, \"60000\")\n",
    "speech_config.set_property(          # 300 ms silence guard (ignored for Semantic\n",
    "    speechsdk.PropertyId.Speech_SegmentationSilenceTimeoutMs, \"300\")\n",
    "\n",
    "# optional: Stable partials every word\n",
    "speech_config.set_property(\n",
    "    speechsdk.PropertyId.SpeechServiceResponse_StablePartialResultThreshold, \"1\")\n",
    "\n",
    "# --- 2) PushAudioInputStream from the mic (16-kHz PCM) ---------------------\n",
    "import pyaudio, threading, time\n",
    "RATE, CHUNK = 16_000, 1024\n",
    "pa = pyaudio.PyAudio()\n",
    "mic = pa.open(format=pyaudio.paInt16, channels=1, rate=RATE,\n",
    "              input=True, frames_per_buffer=CHUNK)\n",
    "\n",
    "stream_format = speechsdk.audio.AudioStreamFormat(samples_per_second=RATE,\n",
    "                                                  bits_per_sample=16, channels=1)\n",
    "push_stream = speechsdk.audio.PushAudioInputStream(stream_format)\n",
    "audio_config = speechsdk.audio.AudioConfig(stream=push_stream)\n",
    "\n",
    "recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config,\n",
    "                                        audio_config=audio_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ce3cb06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.cognitiveservices.speech.SpeechRecognizer at 0x1f0c16292d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4112edb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎤 Speak; Ctrl-C to exit\n",
      "Session stopped\n"
     ]
    }
   ],
   "source": [
    "import os, azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "speech_key   = os.getenv(\"AZURE_SPEECH_KEY\")\n",
    "speech_region = os.getenv(\"AZURE_SPEECH_REGION\")\n",
    "\n",
    "speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=speech_region)\n",
    "\n",
    "# --- 1) segmentation knobs -------------------------------------------------\n",
    "speech_config.set_property(          # content-aware breaks\n",
    "    speechsdk.PropertyId.Speech_SegmentationStrategy, \"Semantic\")\n",
    "speech_config.set_property(          # max 60 s per phrase fallback\n",
    "    speechsdk.PropertyId.Speech_SegmentationMaximumTimeMs, \"60000\")\n",
    "# speech_config.set_property(          # 300 ms silence guard (ignored for Semantic\n",
    "#     speechsdk.PropertyId.Speech_SegmentationSilenceTimeoutMs, \"300\")\n",
    "\n",
    "# optional: Stable partials every word\n",
    "speech_config.set_property(\n",
    "    speechsdk.PropertyId.SpeechServiceResponse_StablePartialResultThreshold, \"1\")\n",
    "\n",
    "# --- 2) PushAudioInputStream from the mic (16-kHz PCM) ---------------------\n",
    "import pyaudio, threading, time\n",
    "RATE, CHUNK = 16_000, 1024\n",
    "pa = pyaudio.PyAudio()\n",
    "mic = pa.open(format=pyaudio.paInt16, channels=1, rate=RATE,\n",
    "              input=True, frames_per_buffer=CHUNK)\n",
    "\n",
    "stream_format = speechsdk.audio.AudioStreamFormat(samples_per_second=RATE,\n",
    "                                                  bits_per_sample=16, channels=1)\n",
    "push_stream = speechsdk.audio.PushAudioInputStream(stream_format)\n",
    "audio_config = speechsdk.audio.AudioConfig(stream=push_stream)\n",
    "\n",
    "recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config,\n",
    "                                        audio_config=audio_config)\n",
    "\n",
    "def mic_loop():\n",
    "    while running:\n",
    "        data = mic.read(CHUNK, exception_on_overflow=False)\n",
    "        push_stream.write(data)\n",
    "\n",
    "running = True\n",
    "threading.Thread(target=mic_loop, daemon=True).start()\n",
    "\n",
    "# --- 3) handlers ------------------------------------------------------------\n",
    "def on_partial(evt):\n",
    "    print(f\"[partial] {evt.result.text}\")\n",
    "\n",
    "def on_final(evt):\n",
    "    print(f\"[final]   {evt.result.text}\")\n",
    "\n",
    "recognizer.recognizing.connect(on_partial)\n",
    "recognizer.recognized.connect(on_final)\n",
    "recognizer.session_stopped.connect(lambda evt: print(\"Session stopped\"))\n",
    "\n",
    "recognizer.start_continuous_recognition_async().get()\n",
    "print(\"🎤 Speak; Ctrl-C to exit\")\n",
    "try:\n",
    "    while True: time.sleep(0.5)\n",
    "except KeyboardInterrupt:\n",
    "    running = False\n",
    "    recognizer.stop_continuous_recognition_async().get()\n",
    "    mic.close(); pa.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0be197a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.44.0\n"
     ]
    }
   ],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "print(speechsdk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4224df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-cognitiveservices-speech==1.45.0\n",
      "  Downloading azure_cognitiveservices_speech-1.45.0-py3-none-win_amd64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: azure-core>=1.31.0 in c:\\users\\pablosal\\appdata\\local\\anaconda3\\envs\\audioagent\\lib\\site-packages (from azure-cognitiveservices-speech==1.45.0) (1.33.0)\n",
      "Requirement already satisfied: requests>=2.21.0 in c:\\users\\pablosal\\appdata\\local\\anaconda3\\envs\\audioagent\\lib\\site-packages (from azure-core>=1.31.0->azure-cognitiveservices-speech==1.45.0) (2.32.3)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\pablosal\\appdata\\local\\anaconda3\\envs\\audioagent\\lib\\site-packages (from azure-core>=1.31.0->azure-cognitiveservices-speech==1.45.0) (1.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\pablosal\\appdata\\local\\anaconda3\\envs\\audioagent\\lib\\site-packages (from azure-core>=1.31.0->azure-cognitiveservices-speech==1.45.0) (4.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pablosal\\appdata\\local\\anaconda3\\envs\\audioagent\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-cognitiveservices-speech==1.45.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pablosal\\appdata\\local\\anaconda3\\envs\\audioagent\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-cognitiveservices-speech==1.45.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pablosal\\appdata\\local\\anaconda3\\envs\\audioagent\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-cognitiveservices-speech==1.45.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pablosal\\appdata\\local\\anaconda3\\envs\\audioagent\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-cognitiveservices-speech==1.45.0) (2025.1.31)\n",
      "Downloading azure_cognitiveservices_speech-1.45.0-py3-none-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   -------------------------------------- - 2.4/2.4 MB 14.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 12.6 MB/s eta 0:00:00\n",
      "Installing collected packages: azure-cognitiveservices-speech\n",
      "  Attempting uninstall: azure-cognitiveservices-speech\n",
      "    Found existing installation: azure-cognitiveservices-speech 1.44.0\n",
      "    Uninstalling azure-cognitiveservices-speech-1.44.0:\n",
      "      Successfully uninstalled azure-cognitiveservices-speech-1.44.0\n",
      "Successfully installed azure-cognitiveservices-speech-1.45.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\audioagent\\Lib\\site-packages\\azure\\cognitiveservices\\~-eech'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "!pip install azure-cognitiveservices-speech==1.45.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e6985a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.45.0\n"
     ]
    }
   ],
   "source": [
    "import azure.cognitiveservices.speech as s; print(s.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0635b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab0a74d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213c9196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e750ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ac1430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e718c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch>=2.6.0\n",
    "# !pip install torchaudio>=2.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fd9e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\audioagent\\Lib\\site-packages\\torch\\hub.py:330: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/snakers4/silero-vad/zipball/master\" to C:\\Users\\pablosal/.cache\\torch\\hub\\master.zip\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Couldn't find appropriate backend to handle uri ./samples/labs/test_audio.wav and format None.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m vad_iter = VADIterator(vad_model, threshold=\u001b[32m0.5\u001b[39m, sampling_rate=\u001b[32m16000\u001b[39m,\n\u001b[32m      7\u001b[39m                        min_silence_duration_ms=\u001b[32m150\u001b[39m, speech_pad_ms=\u001b[32m60\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# ----------------- 3. Read a WAV -----------------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m wav, sr = \u001b[43mtorchaudio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./samples/labs/test_audio.wav\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m      \u001b[38;5;66;03m# mono, 16-kHz PCM\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m sr == \u001b[32m16000\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mResample or pick a 16-kHz file\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# ----------------- 4. Stream through VAD -----------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\audioagent\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:204\u001b[39m, in \u001b[36mget_load_func.<locals>.load\u001b[39m\u001b[34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m    119\u001b[39m     uri: Union[BinaryIO, \u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    120\u001b[39m     frame_offset: \u001b[38;5;28mint\u001b[39m = \u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    126\u001b[39m     backend: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    127\u001b[39m ) -> Tuple[torch.Tensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[32m    129\u001b[39m \n\u001b[32m    130\u001b[39m \u001b[33;03m    By default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    202\u001b[39m \u001b[33;03m            `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     backend = \u001b[43mdispatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m backend.load(uri, frame_offset, num_frames, normalize, channels_first, \u001b[38;5;28mformat\u001b[39m, buffer_size)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\audioagent\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:116\u001b[39m, in \u001b[36mget_load_func.<locals>.dispatcher\u001b[39m\u001b[34m(uri, format, backend_name)\u001b[39m\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m backend.can_decode(uri, \u001b[38;5;28mformat\u001b[39m):\n\u001b[32m    115\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m backend\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find appropriate backend to handle uri \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muri\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and format \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mformat\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Couldn't find appropriate backend to handle uri ./samples/labs/test_audio.wav and format None."
     ]
    }
   ],
   "source": [
    "import torch, torchaudio, IPython\n",
    "from src.vad.vad_iterator import VADIterator, int2float\n",
    "\n",
    "# ----------------- 2. Load model -----------------\n",
    "vad_model, utils = torch.hub.load('snakers4/silero-vad', 'silero_vad')\n",
    "vad_iter = VADIterator(vad_model, threshold=0.5, sampling_rate=16000,\n",
    "                       min_silence_duration_ms=150, speech_pad_ms=60)\n",
    "\n",
    "# ----------------- 3. Read a WAV -----------------\n",
    "wav, sr = torchaudio.load('./samples/labs/test')      # mono, 16-kHz PCM\n",
    "assert sr == 16000, \"Resample or pick a 16-kHz file\"\n",
    "\n",
    "# ----------------- 4. Stream through VAD -----------------\n",
    "CHUNK = int(0.03 * sr)   # 30 ms\n",
    "segments = []\n",
    "for i in range(0, wav.shape[1], CHUNK):\n",
    "    frame = wav[:, i:i+CHUNK]\n",
    "    out = vad_iter(frame)\n",
    "    if out is not None:               # finished utterance\n",
    "        seg = torch.cat(out, dim=1)   # stitch frames\n",
    "        segments.append(seg)\n",
    "        print(f\"Segment {len(segments)} | {seg.shape[1]/sr:.2f} s\")\n",
    "\n",
    "# ----------------- 5. Listen back -----------------\n",
    "for i, seg in enumerate(segments, 1):\n",
    "    print(f\"▶️  Segment {i}\")\n",
    "    IPython.display.display(IPython.display.Audio(seg.squeeze(), rate=sr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audioagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
