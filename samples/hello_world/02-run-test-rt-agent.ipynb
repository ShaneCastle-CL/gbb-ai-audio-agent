{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc6b4e04",
   "metadata": {},
   "source": [
    "## **Building Your Voice-to-Voice Agent with Azure AI Speech and AOAI**\n",
    "\n",
    "This notebook provides a step-by-step guide to create a voice-to-voice agent using Azure AI Speech services and Azure OpenAI. It walks you through the process of configuring speech recognition, integrating external tools, and generating human-like responses for real-time interactions.\n",
    "\n",
    "1. **Audio Ingestion**: Ensure the capability to record audio is set up.  \n",
    "2. **Azure Speech-to-Text (STT)**: Converts live audio into transcribed text for LLM processing.  \n",
    "3. **Azure OpenAI with Function Calling & Streaming**: Understands patient intent, routes queries, and dynamically calls backend tools in real time.  \n",
    "4. **Azure Text-to-Speech (TTS)**: Delivers natural, empathetic voice responses back to the user in chunks.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82656daa",
   "metadata": {},
   "source": [
    "## **Prerequisites & Environment Setup**\n",
    "\n",
    "Before we start building our first ARTAgent, make sure you have the following setup:\n",
    "\n",
    "**üîß Environment Setup**\n",
    "\n",
    "1. **Python 3.11+** - Required for the ARTAgent framework\n",
    "2. **Dependencies** - Install the required packages:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "**‚òÅÔ∏è Required Azure Services**\n",
    "\n",
    "This notebook requires **2 main Azure services** to function properly:\n",
    "\n",
    "### **1. Azure Speech Services** üé§üîä\n",
    "For Speech-to-Text (STT) and Text-to-Speech (TTS) capabilities.\n",
    "\n",
    "**Create Azure Speech Service:**\n",
    "- üîó **Azure Portal**: [Create Speech Service](https://portal.azure.com/#create/Microsoft.CognitiveServicesSpeechServices)\n",
    "- üìñ **Documentation**: [Speech Service Setup Guide](https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/overview)\n",
    "\n",
    "**What you'll need from this service:**\n",
    "- API Key (`AZURE_OPENAI_STT_TTS_KEY`)\n",
    "- Endpoint URL (`AZURE_OPENAI_STT_TTS_ENDPOINT`)\n",
    "- Region (`AZURE_SPEECH_REGION`)\n",
    "\n",
    "### **2. Azure OpenAI Service** ü§ñ\n",
    "For GPT-4o chat completions with streaming and function calling.\n",
    "\n",
    "**Create Azure OpenAI Service:**\n",
    "- üîó **Azure Portal**: [Create Azure OpenAI](https://portal.azure.com/#create/Microsoft.CognitiveServicesOpenAI)\n",
    "- üìñ **Documentation**: [Azure OpenAI Setup Guide](https://docs.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource)\n",
    "\n",
    "**Required Model Deployment:**\n",
    "- Deploy **GPT-4o-mini** (or GPT-4o) model in your Azure OpenAI resource. Or pick your own. \n",
    "- üìñ **Model Deployment Guide**: [Deploy Models in Azure OpenAI](https://docs.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource#deploy-a-model)\n",
    "\n",
    "**What you'll need from this service:**\n",
    "- API Key (`AZURE_OPENAI_KEY`)\n",
    "- Endpoint URL (`AZURE_OPENAI_ENDPOINT`)\n",
    "- Deployment Name (`AZURE_OPENAI_CHAT_DEPLOYMENT_ID`)\n",
    "\n",
    "**üîê Security & Environment Variables**\n",
    "\n",
    "**IMPORTANT:** Never hardcode API keys in your notebooks or code files. Always use environment variables or `.env` files.\n",
    "\n",
    "**Option 1: Create a `.env` file in the main directory**\n",
    "\n",
    "Create a file named `.env` in the project root directory with the following structure:\n",
    "\n",
    "```bash\n",
    "# Azure Speech Services\n",
    "AZURE_OPENAI_STT_TTS_KEY=your_azure_speech_key_here\n",
    "AZURE_OPENAI_STT_TTS_ENDPOINT=https://your-speech-service.cognitiveservices.azure.com\n",
    "AZURE_SPEECH_REGION=eastus\n",
    "\n",
    "# Azure OpenAI Services  \n",
    "AZURE_OPENAI_KEY=your_azure_openai_key_here\n",
    "AZURE_OPENAI_ENDPOINT=https://your-openai-service.openai.azure.com/\n",
    "AZURE_OPENAI_API_VERSION=2024-12-01-preview\n",
    "AZURE_OPENAI_CHAT_DEPLOYMENT_ID=gpt-4o-mini\n",
    "```\n",
    "\n",
    "**Option 2: Set system environment variables**\n",
    "\n",
    "You can also set these as system environment variables in your operating system.\n",
    "\n",
    "**üìÇ Project Structure**\n",
    "\n",
    "The code below automatically sets up the correct working directory for the notebook to access the ARTAgent framework and all dependencies.\n",
    "\n",
    "**üõ°Ô∏è Security Best Practices**\n",
    "\n",
    "- ‚úÖ Use environment variables or `.env` files for sensitive data\n",
    "- ‚úÖ Add `.env` to your `.gitignore` file\n",
    "- ‚úÖ Use different keys for development, staging, and production\n",
    "- ‚ùå Never commit API keys to version control\n",
    "- ‚ùå Never share API keys in screenshots or documentation\n",
    "\n",
    "**üí∞ Cost Considerations**\n",
    "\n",
    "- **Azure Speech Services**: Pay-per-use pricing for STT/TTS operations\n",
    "- **Azure OpenAI**: Pay-per-token pricing for GPT model usage\n",
    "- üìñ **Pricing Details**: [Azure Speech Pricing](https://azure.microsoft.com/en-us/pricing/details/cognitive-services/speech-services/) | [Azure OpenAI Pricing](https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a124a50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All required environment variables are set!\n",
      "üîí API keys are properly loaded from environment variables.\n",
      "\n",
      "üìã Configuration Summary:\n",
      "   Azure Speech Region: eastus\n",
      "   Azure OpenAI Endpoint: https://aoai-ai-factory-eus-dev.openai.azure.com/\n",
      "   Azure Speech TTS Endpoint: https://azure-ai-services-eastus-test.cognitiveservices.azure.com/\n",
      "   OpenAI API Version: 2024-12-01-preview\n",
      "   Chat Deployment ID: gpt-4o-mini\n",
      "   üîê API Keys: ‚úÖ Loaded\n"
     ]
    }
   ],
   "source": [
    "# üîê Load Environment Variables Securely\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file if it exists\n",
    "load_dotenv()\n",
    "\n",
    "# Required environment variables for Azure services\n",
    "REQUIRED_ENV_VARS = [\n",
    "    \"AZURE_SPEECH_ENDPOINT\", \n",
    "    \"AZURE_SPEECH_REGION\",\n",
    "    \"AZURE_OPENAI_KEY\",\n",
    "    \"AZURE_OPENAI_ENDPOINT\",\n",
    "    \"AZURE_OPENAI_API_VERSION\",\n",
    "    \"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"\n",
    "]\n",
    "\n",
    "# Validate that all required environment variables are set\n",
    "missing_vars = []\n",
    "for var in REQUIRED_ENV_VARS:\n",
    "    if not os.getenv(var):\n",
    "        missing_vars.append(var)\n",
    "\n",
    "if missing_vars:\n",
    "    print(\"‚ùå Missing required environment variables:\")\n",
    "    for var in missing_vars:\n",
    "        print(f\"   - {var}\")\n",
    "    print(\"\\nüí° Please set these variables in your .env file or system environment.\")\n",
    "    print(\"üìñ See the previous cell for instructions on setting up environment variables.\")\n",
    "else:\n",
    "    print(\"‚úÖ All required environment variables are set!\")\n",
    "    print(\"üîí API keys are properly loaded from environment variables.\")\n",
    "    \n",
    "# Display non-sensitive configuration for verification\n",
    "print(f\"\\nüìã Configuration Summary:\")\n",
    "print(f\"   Azure Speech Region: {os.getenv('AZURE_SPEECH_REGION', 'Not set')}\")\n",
    "print(f\"   Azure OpenAI Endpoint: {os.getenv('AZURE_OPENAI_ENDPOINT', 'Not set')}\")\n",
    "print(f\"   Azure Speech TTS Endpoint: {os.getenv('AZURE_SPEECH_ENDPOINT', 'Not set')}\")\n",
    "print(f\"   OpenAI API Version: {os.getenv('AZURE_OPENAI_API_VERSION', 'Not set')}\")\n",
    "print(f\"   Chat Deployment ID: {os.getenv('AZURE_OPENAI_CHAT_DEPLOYMENT_ID', 'Not set')}\")\n",
    "print(f\"   üîê API Keys: {'‚úÖ Loaded' if os.getenv('AZURE_OPENAI_KEY') else '‚ùå Missing'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed102ccb",
   "metadata": {},
   "source": [
    "## **Test Audio Capture from your microphone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71801339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available audio devices:\n",
      "0: Microsoft Sound Mapper - Input\n",
      "1: Headset (Shiva‚Äôs AirPods Pro #2\n",
      "2: Surface Stereo Microphones (Sur\n",
      "3: Microphone (Microsoft Surface T\n",
      "4: Microphone (Lumina Camera - Raw\n",
      "5: Microsoft Sound Mapper - Output\n",
      "6: Headphones (Shiva‚Äôs AirPods Pro\n",
      "7: Speakers (Dell USB Audio)\n",
      "8: Surface Omnisonic Speakers (Sur\n",
      "9: Headset (Microsoft Surface Thun\n",
      "10: Primary Sound Capture Driver\n",
      "11: Headset (Shiva‚Äôs AirPods Pro #2)\n",
      "12: Surface Stereo Microphones (Surface High Definition Audio)\n",
      "13: Microphone (Microsoft Surface Thunderbolt(TM) 4 Dock Audio)\n",
      "14: Microphone (Lumina Camera - Raw)\n",
      "15: Primary Sound Driver\n",
      "16: Headphones (Shiva‚Äôs AirPods Pro #2)\n",
      "17: Speakers (Dell USB Audio)\n",
      "18: Surface Omnisonic Speakers (Surface High Definition Audio)\n",
      "19: Headset (Microsoft Surface Thunderbolt(TM) 4 Dock Audio)\n",
      "20: Speakers (Dell USB Audio)\n",
      "21: Headphones (Shiva‚Äôs AirPods Pro #2)\n",
      "22: Surface Omnisonic Speakers (Surface High Definition Audio)\n",
      "23: Headset (Microsoft Surface Thunderbolt(TM) 4 Dock Audio)\n",
      "24: Headset (Shiva‚Äôs AirPods Pro #2)\n",
      "25: Surface Stereo Microphones (Surface High Definition Audio)\n",
      "26: Microphone (Microsoft Surface Thunderbolt(TM) 4 Dock Audio)\n",
      "27: Microphone (Lumina Camera - Raw)\n",
      "28: Headphones ()\n",
      "29: Microphone (Microsoft Surface Thunderbolt(TM) 4 Dock Audio)\n",
      "30: Output (Microsoft Surface Thunderbolt(TM) 4 Dock Audio)\n",
      "31: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Shiva‚Äôs AirPods Pro #2))\n",
      "32: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Shiva‚Äôs AirPods Pro #2))\n",
      "33: Speakers (Dell USB Audio)\n",
      "34: Microphone (Dell USB Audio)\n",
      "35: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Shiva‚Äôs AirPods Pro #2 - Find My))\n",
      "36: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Shiva‚Äôs AirPods Pro #2 - Find My))\n",
      "37: Headphones 1 (Realtek HD Audio 2nd output with SST)\n",
      "38: Headphones 2 (Realtek HD Audio 2nd output with SST)\n",
      "39: PC Speaker (Realtek HD Audio 2nd output with SST)\n",
      "40: Speakers 1 (Realtek HD Audio output with SST)\n",
      "41: Speakers 2 (Realtek HD Audio output with SST)\n",
      "42: PC Speaker (Realtek HD Audio output with SST)\n",
      "43: Microphone Array (Realtek HD Audio Mic input)\n",
      "44: Headset Microphone (Headset Microphone)\n",
      "45: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Pablo‚Äôs AirPods #3))\n",
      "46: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Pablo‚Äôs AirPods #3))\n",
      "47: Input ()\n",
      "48: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Pablo‚Äôs AirPods #4))\n",
      "49: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Pablo‚Äôs AirPods #4))\n",
      "50: Headphones ()\n",
      "51: Output (@System32\\drivers\\bthhfenum.sys,#4;%1 Hands-Free HF Audio%0\n",
      ";(iPhone de Pablo))\n",
      "52: Input (@System32\\drivers\\bthhfenum.sys,#4;%1 Hands-Free HF Audio%0\n",
      ";(iPhone de Pablo))\n",
      "53: Headphones ()\n",
      "54: Microphone (Lumina Camera - Raw)\n",
      "55: Headphones ()\n",
      "56: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Pablo‚Äôs AirPods Pro - Find My))\n",
      "57: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Pablo‚Äôs AirPods Pro - Find My))\n",
      "58: Headphones ()\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "\n",
    "\n",
    "def list_audio_devices():\n",
    "    \"\"\"\n",
    "    List all available audio devices using PyAudio.\n",
    "\n",
    "    This function initializes PyAudio, retrieves the list of audio devices,\n",
    "    and prints their names. It also includes error handling to ensure proper\n",
    "    cleanup of resources.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        p = pyaudio.PyAudio()\n",
    "        print(\"Available audio devices:\")\n",
    "        for ii in range(p.get_device_count()):\n",
    "            device_name = p.get_device_info_by_index(ii).get(\"name\")\n",
    "            print(f\"{ii}: {device_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while listing audio devices: {e}\")\n",
    "    finally:\n",
    "        # Ensure PyAudio resources are released\n",
    "        if \"p\" in locals():\n",
    "            p.terminate()\n",
    "\n",
    "\n",
    "# Call the function to list audio devices\n",
    "list_audio_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e9ade96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording complete. Saving audio...\n",
      "Audio saved to test_audio.wav. Playing back...\n",
      "Playback complete.\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "\n",
    "def test_microphone():\n",
    "    \"\"\"\n",
    "    Test the microphone by recording audio and playing it back.\n",
    "\n",
    "    This function captures audio from the default input device (microphone),\n",
    "    saves it to a temporary WAV file, and plays it back to ensure the microphone\n",
    "    is working correctly.\n",
    "    \"\"\"\n",
    "    # Audio configuration\n",
    "    chunk = 1024  # Number of frames per buffer\n",
    "    format = pyaudio.paInt16  # 16-bit audio format\n",
    "    channels = 1  # Mono audio\n",
    "    rate = 44100  # Sampling rate (44.1 kHz)\n",
    "    record_seconds = 5  # Duration of the recording\n",
    "    output_filename = \"test_audio.wav\"\n",
    "\n",
    "    # Initialize PyAudio\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    try:\n",
    "        # Open the microphone stream\n",
    "        print(\"Recording...\")\n",
    "        stream = p.open(\n",
    "            format=format,\n",
    "            channels=channels,\n",
    "            rate=rate,\n",
    "            input=True,\n",
    "            frames_per_buffer=chunk,\n",
    "        )\n",
    "\n",
    "        frames = []\n",
    "\n",
    "        # Record audio in chunks\n",
    "        for _ in range(0, int(rate / chunk * record_seconds)):\n",
    "            data = stream.read(chunk)\n",
    "            frames.append(data)\n",
    "\n",
    "        print(\"Recording complete. Saving audio...\")\n",
    "\n",
    "        # Save the recorded audio to a WAV file\n",
    "        with wave.open(output_filename, \"wb\") as wf:\n",
    "            wf.setnchannels(channels)\n",
    "            wf.setsampwidth(p.get_sample_size(format))\n",
    "            wf.setframerate(rate)\n",
    "            wf.writeframes(b\"\".join(frames))\n",
    "\n",
    "        print(f\"Audio saved to {output_filename}. Playing back...\")\n",
    "\n",
    "        # Play back the recorded audio\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "\n",
    "        # Open the WAV file for playback\n",
    "        wf = wave.open(output_filename, \"rb\")\n",
    "        playback_stream = p.open(\n",
    "            format=p.get_format_from_width(wf.getsampwidth()),\n",
    "            channels=wf.getnchannels(),\n",
    "            rate=wf.getframerate(),\n",
    "            output=True,\n",
    "        )\n",
    "\n",
    "        # Read and play audio data\n",
    "        data = wf.readframes(chunk)\n",
    "        while data:\n",
    "            playback_stream.write(data)\n",
    "            data = wf.readframes(chunk)\n",
    "\n",
    "        playback_stream.stop_stream()\n",
    "        playback_stream.close()\n",
    "\n",
    "        print(\"Playback complete.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Terminate PyAudio\n",
    "        p.terminate()\n",
    "\n",
    "\n",
    "# Run the microphone test\n",
    "test_microphone()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e9da16",
   "metadata": {},
   "source": [
    "## **Define Clients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c3f52ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Changed directory to: c:\\Users\\pablosal\\Desktop\\gbb-ai-audio-agent\n",
      "üìÅ Current working directory: c:\\Users\\pablosal\\Desktop\\gbb-ai-audio-agent\n"
     ]
    }
   ],
   "source": [
    "# üìÇ Setup Working Directory for ARTAgent Framework Access\n",
    "import os\n",
    "# Navigate to the project root directory\n",
    "# This ensures we can import ARTAgent framework modules properly\n",
    "try:\n",
    "    # Move up two directories from samples/hello_world/ to project root\n",
    "    os.chdir(\"../../\")\n",
    "    \n",
    "    # Allow override via environment variable for different setups\n",
    "    target_directory = os.getenv(\n",
    "        \"TARGET_DIRECTORY\", os.getcwd()\n",
    "    )  # Use environment variable if available\n",
    "    \n",
    "    # Verify the target directory exists before changing\n",
    "    if os.path.exists(target_directory):\n",
    "        os.chdir(target_directory)\n",
    "        print(f\"‚úÖ Changed directory to: {os.getcwd()}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Directory does not exist: {target_directory}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error changing directory: {e}\")\n",
    "    \n",
    "# Verify we're in the correct location\n",
    "print(f\"üìÅ Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b87bf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import logger \n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "\n",
    "from utils.ml_logging import get_logger\n",
    "\n",
    "timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "pid = os.getpid()\n",
    "tid = threading.get_ident()\n",
    "user = os.getenv(\"USER\") or os.getenv(\"USERNAME\") or \"unknown\"\n",
    "\n",
    "logger = get_logger(f\"run_test_{user}_{timestamp}_{pid}_{tid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2852b31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Settings \n",
    "\n",
    "VOICE = \"en-US-Ava:DragonHDLatestNeural\" \n",
    "VAD_SILENCE_TIMEOUT_MS = 800\n",
    "USE_SEMANTIC_VAD = False\n",
    "CANDIDATE_LANGUAGES = [\"en-US\", \"fr-FR\", \"de-DE\", \"es-ES\", \"it-IT\"]\n",
    "AOAI_TEMPERATURE = 1\n",
    "AOAI_MODEL = \"gpt-4o\"  # Default model, can be overridden in agent config\n",
    "TTS_ENDS = [\".\", \"!\", \"?\"]\n",
    "\n",
    "PROMPT_STORE_DIR = \"samples/hello_world/agents/prompt_store\"\n",
    "PROMPT_LOCATION = \"samples/hello_world/agents/prompt_store/templates/customer_support_agent.jinja\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8a11c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-08-20 12:51:33,491] INFO - src.speech.speech_recognizer: Azure Monitor tracing initialized for speech recognizer\n",
      "[2025-08-20 12:51:33,495] INFO - src.speech.speech_recognizer: Creating SpeechConfig with API key authentication\n",
      "[2025-08-20 12:51:33,500] INFO - src.speech.text_to_speech: Azure Monitor tracing initialized for speech synthesizer\n",
      "[2025-08-20 12:51:33,505] INFO - src.speech.text_to_speech: Creating SpeechConfig with API key authentication\n",
      "[2025-08-20 12:51:33,513] INFO - src.speech.text_to_speech: Speech synthesizer initialized successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Templates found: ['customer_support_agent.jinja']\n"
     ]
    }
   ],
   "source": [
    "from src.speech.text_to_speech import SpeechSynthesizer\n",
    "from src.speech.speech_recognizer import StreamingSpeechRecognizerFromBytes\n",
    "from openai import AzureOpenAI\n",
    "from samples.hello_world.agents.prompt_store.prompt_manager import PromptManager\n",
    "\n",
    "if \"az_speech_recognizer_stream_client\" not in locals():\n",
    "    az_speech_recognizer_stream_client = StreamingSpeechRecognizerFromBytes(\n",
    "        region=os.getenv(\"AZURE_SPEECH_REGION\"), \n",
    "        vad_silence_timeout_ms=VAD_SILENCE_TIMEOUT_MS,\n",
    "        use_semantic_segmentation=USE_SEMANTIC_VAD,\n",
    "        audio_format=\"pcm\",\n",
    "        candidate_languages=CANDIDATE_LANGUAGES,\n",
    "        enable_diarisation=True,\n",
    "        speaker_count_hint=2,\n",
    "        enable_neural_fe=False,\n",
    "    )\n",
    "    \n",
    "\n",
    "if \"az_speech_synthesizer_client\" not in locals():\n",
    "    az_speech_synthesizer_client = SpeechSynthesizer(region=os.getenv(\"AZURE_SPEECH_REGION\"),  # Fixed: was AZURE_REGION\n",
    "                                                     voice=VOICE)\n",
    "\n",
    "# Ensure Azure OpenAI client is initialized only if not already defined\n",
    "if \"client\" not in locals():\n",
    "    client = AzureOpenAI(\n",
    "        api_version=\"2025-02-01-preview\",\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    )\n",
    "\n",
    "if \"prompt_manager\" not in locals():\n",
    "    prompt_manager = PromptManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5524cb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\n'\n",
      " 'You are a helpful customer support agent for Demo Corp. Your role is to:\\n'\n",
      " '\\n'\n",
      " 'üéØ **Primary Responsibilities:**\\n'\n",
      " '- Answer product questions accurately and helpfully\\n'\n",
      " '- Help customers check order status and tracking\\n'\n",
      " '- Process return and exchange requests\\n'\n",
      " '- Provide basic troubleshooting guidance\\n'\n",
      " '- Escalate complex issues to human agents when needed\\n'\n",
      " '\\n'\n",
      " 'üó®Ô∏è **Communication Style:**\\n'\n",
      " '- Be friendly, professional, and empathetic\\n'\n",
      " '- Use clear, concise language\\n'\n",
      " '- Always confirm understanding before taking action\\n'\n",
      " '- Provide specific next steps when possible\\n'\n",
      " '\\n'\n",
      " 'üõ†Ô∏è **Available Tools:**\\n'\n",
      " '- `search_product_catalog`: Find product information, specs, pricing\\n'\n",
      " '- `check_order_status`: Look up order details and shipping status  \\n'\n",
      " '- `create_return_request`: Initiate return/exchange process\\n'\n",
      " '- `escalate_to_human`: Transfer to live agent for complex issues\\n'\n",
      " '\\n'\n",
      " 'üö´ **Important Constraints:**\\n'\n",
      " '- Only use the tools provided - do not make up information\\n'\n",
      " '- If you cannot help with a request, escalate to a human agent\\n'\n",
      " '- Always verify customer identity before accessing order information\\n'\n",
      " '- Be honest about limitations and processing times\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Customer: \\n'\n",
      " '\\n'\n",
      " 'How can I help you today?')\n"
     ]
    }
   ],
   "source": [
    "# get prompt \n",
    "PROMPT = prompt_manager.get_prompt('customer_support_agent.jinja')\n",
    "import pprint\n",
    "\n",
    "pprint.pprint(PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ad30d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'search_product_catalog': {'type': 'function',\n",
       "  'function': {'name': 'search_product_catalog',\n",
       "   'description': 'Search the product catalog for information about products including specs, pricing, and availability',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'query': {'type': 'string',\n",
       "      'description': 'Search term or product ID to look up in catalog'}},\n",
       "    'required': ['query']}}},\n",
       " 'check_order_status': {'type': 'function',\n",
       "  'function': {'name': 'check_order_status',\n",
       "   'description': 'Check the status and tracking information for a customer order',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'order_id': {'type': 'string',\n",
       "      'description': 'The order ID to look up (e.g., ORD123456)'}},\n",
       "    'required': ['order_id']}}},\n",
       " 'create_return_request': {'type': 'function',\n",
       "  'function': {'name': 'create_return_request',\n",
       "   'description': 'Create a return request for a customer order',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'order_id': {'type': 'string',\n",
       "      'description': 'The order ID for the return request'},\n",
       "     'reason': {'type': 'string',\n",
       "      'description': 'Reason for the return (e.g., defective item, wrong size)'}},\n",
       "    'required': ['order_id', 'reason']}}},\n",
       " 'escalate_to_human': {'type': 'function',\n",
       "  'function': {'name': 'escalate_to_human',\n",
       "   'description': 'Escalate the conversation to a human agent when the issue is too complex',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'reason': {'type': 'string',\n",
       "      'description': 'Reason for escalation (e.g., complex technical issue, customer request)'}},\n",
       "    'required': ['reason']}}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import Tools \n",
    "\n",
    "from samples.hello_world.agents.tool_store.tool_registry import TOOL_REGISTRY\n",
    "\n",
    "TOOL_REGISTRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dfcfad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Voice-to-Voice Agent functions loaded!\n",
      "\n",
      "üöÄ Usage Options:\n",
      "1. start_voice_agent()\n",
      "2. await process_user_input()  # one-shot\n",
      "3. stop_voice_agent()\n",
      "4. await full_conversation_loop()  # continuous\n"
     ]
    }
   ],
   "source": [
    "# üéØ Complete Voice-to-Voice Agent with Streaming Tool Calls (FINAL PROD)\n",
    "# - Solid barge-in (partials stop TTS cleanly, debounced)\n",
    "# - Parallel tool-calls support\n",
    "# - Tools passed as a LIST (not dict) to Azure OpenAI\n",
    "# - Mic loop race fixed\n",
    "# - Avoids micro-fragment TTS (e.g., \"99.\")\n",
    "\n",
    "import os, time, threading, json, asyncio\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "# Audio capture\n",
    "RATE, CHANNELS, CHUNK = 16000, 1, 1024\n",
    "\n",
    "# Barge-in tuning\n",
    "_TTS_STOP_DEBOUNCE_SEC = 0.3\n",
    "_MIN_TTS_CHARS = 8  # don't speak super tiny fragments\n",
    "\n",
    "# Tools & registry\n",
    "from samples.hello_world.agents.tool_store.customer_support_tools import (\n",
    "    search_product_catalog,\n",
    "    check_order_status,\n",
    "    create_return_request,\n",
    "    escalate_to_human\n",
    ")\n",
    "from samples.hello_world.agents.tool_store.tool_registry import TOOL_REGISTRY\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Clients (Speech + Azure OpenAI)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "if \"az_speech_synthesizer_client\" not in locals():\n",
    "    az_speech_synthesizer_client = SpeechSynthesizer(\n",
    "        key=os.getenv(\"AZURE_SPEECH_KEY\"),\n",
    "        region=os.getenv(\"AZURE_SPEECH_REGION\"),\n",
    "        voice=VOICE,\n",
    "        # make sure your wrapper routes to default speaker or let you pass output_device_id\n",
    "        use_default_speaker=True,\n",
    "    )\n",
    "\n",
    "if \"az_speech_recognizer_stream_client\" not in locals():\n",
    "    az_speech_recognizer_stream_client = StreamingSpeechRecognizerFromBytes(\n",
    "        region=os.getenv(\"AZURE_SPEECH_REGION\"),\n",
    "        vad_silence_timeout_ms=VAD_SILENCE_TIMEOUT_MS,\n",
    "        use_semantic_segmentation=USE_SEMANTIC_VAD,\n",
    "        audio_format=\"pcm\",\n",
    "        candidate_languages=CANDIDATE_LANGUAGES,\n",
    "        enable_diarisation=True,\n",
    "        speaker_count_hint=2,\n",
    "        enable_neural_fe=False,\n",
    "    )\n",
    "\n",
    "if \"client\" not in locals():\n",
    "    client = AzureOpenAI(\n",
    "        api_version=\"2025-02-01-preview\",\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    )\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Tools map (function name -> callable)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "function_mapping = {\n",
    "    \"search_product_catalog\": search_product_catalog,\n",
    "    \"check_order_status\": check_order_status,\n",
    "    \"create_return_request\": create_return_request,\n",
    "    \"escalate_to_human\": escalate_to_human,\n",
    "}\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Global state\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "user_buffer = \"\"\n",
    "is_synthesizing = False\n",
    "conversation_active = False\n",
    "audio_stream = None\n",
    "audio_interface = None\n",
    "_mic_thread = None  # guard for mic thread\n",
    "_tts_future = None\n",
    "_last_tts_stop = 0.0\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Utilities: TTS control (barge-in)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def speak(text: str):\n",
    "    \"\"\"Start TTS in a controlled way (sets flag, keeps future, ignores micro-fragments).\"\"\"\n",
    "    global is_synthesizing, _tts_future\n",
    "    if not text or len(text.strip()) < _MIN_TTS_CHARS:\n",
    "        return\n",
    "    is_synthesizing = True\n",
    "    try:\n",
    "        _tts_future = az_speech_synthesizer_client.start_speaking_text(text)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå TTS start error: {e}\")\n",
    "        is_synthesizing = False\n",
    "        _tts_future = None\n",
    "\n",
    "def _stop_tts(reason: str = \"\"):\n",
    "    \"\"\"Stop/cancel current TTS with debounce to avoid flapping on tiny partials.\"\"\"\n",
    "    global is_synthesizing, _tts_future, _last_tts_stop\n",
    "    now = time.time()\n",
    "    if now - _last_tts_stop < _TTS_STOP_DEBOUNCE_SEC:\n",
    "        return\n",
    "    _last_tts_stop = now\n",
    "\n",
    "    try:\n",
    "        if _tts_future:\n",
    "            try:\n",
    "                _tts_future.cancel()\n",
    "            except Exception:\n",
    "                pass\n",
    "            _tts_future = None\n",
    "        az_speech_synthesizer_client.stop_speaking()\n",
    "        if reason:\n",
    "            print(f\"üõë TTS stopped (barge-in): {reason}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è stop_speaking() error (ignored): {e}\")\n",
    "    finally:\n",
    "        is_synthesizing = False\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Tool-call streaming state (parallel-friendly)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "class _SingleToolState:\n",
    "    def __init__(self, call_id: str):\n",
    "        self.call_id = call_id\n",
    "        self.name = \"\"\n",
    "        self.args_json = []  # fragments\n",
    "\n",
    "    @property\n",
    "    def args_str(self) -> str:\n",
    "        return \"\".join(self.args_json)\n",
    "\n",
    "def _ensure_tools_list(tools_like) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Azure OpenAI expects a list of tool objects; convert dict registries.\"\"\"\n",
    "    if tools_like is None:\n",
    "        return []\n",
    "    if isinstance(tools_like, dict):\n",
    "        return list(tools_like.values())\n",
    "    if isinstance(tools_like, list):\n",
    "        return tools_like\n",
    "    raise TypeError(\"tools must be list[tool] or dict[name->tool]\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# LLM streaming with tool-calls\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "async def process_streaming_response_with_tools(\n",
    "    messages: List[Dict[str, Any]],\n",
    "    tools: List[Dict[str, Any]] = None\n",
    ") -> None:\n",
    "    \"\"\"Streams assistant text, handles tool-calls, then streams a follow-up.\"\"\"\n",
    "    tools = _ensure_tools_list(tools or TOOL_REGISTRY)\n",
    "\n",
    "    print(\"ü§ñ Processing GPT response...\")\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        stream=True,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",\n",
    "        max_tokens=4096,\n",
    "        temperature=AOAI_TEMPERATURE,\n",
    "        top_p=1.0,\n",
    "        model=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    "    )\n",
    "\n",
    "    collected_text: List[str] = []\n",
    "    tool_states: Dict[str, _SingleToolState] = {}\n",
    "\n",
    "    for chunk in response:\n",
    "        if not chunk.choices:\n",
    "            continue\n",
    "        delta = chunk.choices[0].delta\n",
    "\n",
    "        # Tool-calls (may be multiple)\n",
    "        if hasattr(delta, \"tool_calls\") and delta.tool_calls:\n",
    "            for tc in delta.tool_calls:\n",
    "                if tc.id and tc.id not in tool_states:\n",
    "                    tool_states[tc.id] = _SingleToolState(tc.id)\n",
    "                st = tool_states.get(getattr(tc, \"id\", \"\"))\n",
    "                if not st:\n",
    "                    continue\n",
    "                if hasattr(tc, \"function\") and tc.function:\n",
    "                    if getattr(tc.function, \"name\", None):\n",
    "                        st.name = tc.function.name\n",
    "                        print(f\"üõ†Ô∏è Tool call detected: {st.name} (id={st.call_id})\")\n",
    "                    if getattr(tc.function, \"arguments\", None):\n",
    "                        st.args_json.append(tc.function.arguments)\n",
    "\n",
    "        # Text streaming\n",
    "        elif hasattr(delta, \"content\") and delta.content:\n",
    "            text_chunk = delta.content\n",
    "            collected_text.append(text_chunk)\n",
    "            print(text_chunk, end=\"\", flush=True)\n",
    "\n",
    "            # Speak on sentence boundaries (but avoid tiny fragments)\n",
    "            if text_chunk in TTS_ENDS and sum(len(x) for x in collected_text) >= _MIN_TTS_CHARS:\n",
    "                sentence = \"\".join(collected_text).strip()\n",
    "                if sentence:\n",
    "                    print(f\"\\nüîä Speaking: {sentence}\")\n",
    "                    speak(sentence)\n",
    "                    collected_text.clear()\n",
    "\n",
    "    # Flush any remaining text\n",
    "    if collected_text:\n",
    "        remaining = \"\".join(collected_text).strip()\n",
    "        if remaining:\n",
    "            print(f\"\\nüîä Speaking final: {remaining}\")\n",
    "            speak(remaining)\n",
    "            messages.append({\"role\": \"assistant\", \"content\": remaining})\n",
    "\n",
    "    print()  # newline\n",
    "\n",
    "    # If tools were called, add assistant tool_calls message, execute, then follow up\n",
    "    if tool_states:\n",
    "        messages.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": None,\n",
    "            \"tool_calls\": [\n",
    "                {\"id\": st.call_id, \"type\": \"function\",\n",
    "                 \"function\": {\"name\": st.name, \"arguments\": st.args_str}}\n",
    "                for st in tool_states.values()\n",
    "            ],\n",
    "        })\n",
    "        await _execute_tools_and_followup(tool_states, messages, tools)\n",
    "\n",
    "async def _execute_tools_and_followup(\n",
    "    tool_states: Dict[str, _SingleToolState],\n",
    "    messages: List[Dict[str, Any]],\n",
    "    tools: List[Dict[str, Any]],\n",
    ") -> None:\n",
    "    for st in tool_states.values():\n",
    "        print(f\"\\nüîß Executing tool: {st.name} (id={st.call_id})\")\n",
    "        print(f\"üìù Arguments (raw): {st.args_str}\")\n",
    "\n",
    "        # Parse streamed args (with gentle repair)\n",
    "        try:\n",
    "            args = json.loads(st.args_str) if st.args_str.strip() else {}\n",
    "        except json.JSONDecodeError as e:\n",
    "            repaired = st.args_str.strip()\n",
    "            if repaired and not repaired.startswith(\"{\"):\n",
    "                repaired = \"{\" + repaired\n",
    "            if repaired and not repaired.endswith(\"}\"):\n",
    "                repaired = repaired + \"}\"\n",
    "            try:\n",
    "                args = json.loads(repaired)\n",
    "                print(\"‚ö†Ô∏è JSON repaired for tool args.\")\n",
    "            except Exception:\n",
    "                print(f\"‚ùå Error parsing tool arguments: {e}\")\n",
    "                args = {}\n",
    "\n",
    "        if st.name in function_mapping:\n",
    "            fn = function_mapping[st.name]\n",
    "            try:\n",
    "                result = await fn(args) if asyncio.iscoroutinefunction(fn) else fn(args)\n",
    "                print(f\"‚úÖ Tool result: {result}\")\n",
    "                messages.append({\n",
    "                    \"tool_call_id\": st.call_id, \"role\": \"tool\",\n",
    "                    \"name\": st.name,\n",
    "                    \"content\": json.dumps(result) if isinstance(result, (dict, list)) else str(result),\n",
    "                })\n",
    "            except Exception as e:\n",
    "                err_payload = {\"error\": f\"{type(e).__name__}: {e}\"}\n",
    "                print(f\"‚ùå Tool execution error: {err_payload}\")\n",
    "                messages.append({\n",
    "                    \"tool_call_id\": st.call_id, \"role\": \"tool\",\n",
    "                    \"name\": st.name, \"content\": json.dumps(err_payload),\n",
    "                })\n",
    "        else:\n",
    "            print(f\"‚ùå Unknown tool: {st.name}\")\n",
    "            messages.append({\n",
    "                \"tool_call_id\": st.call_id, \"role\": \"tool\",\n",
    "                \"name\": st.name, \"content\": json.dumps({\"error\": f\"Unknown tool {st.name}\"}),\n",
    "            })\n",
    "\n",
    "    print(\"\\nü§ñ Getting follow-up response...\")\n",
    "    await _process_followup_response(messages)\n",
    "\n",
    "async def _process_followup_response(messages: List[Dict[str, Any]]) -> None:\n",
    "    response = client.chat.completions.create(\n",
    "        stream=True,\n",
    "        messages=messages,\n",
    "        temperature=AOAI_TEMPERATURE,\n",
    "        top_p=1.0,\n",
    "        max_tokens=4096,\n",
    "        model=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    "    )\n",
    "\n",
    "    collected_text: List[str] = []\n",
    "    for chunk in response:\n",
    "        if chunk.choices and hasattr(chunk.choices[0].delta, \"content\"):\n",
    "            content = chunk.choices[0].delta.content\n",
    "            if content:\n",
    "                collected_text.append(content)\n",
    "                print(content, end=\"\", flush=True)\n",
    "                if content in TTS_ENDS and sum(len(x) for x in collected_text) >= _MIN_TTS_CHARS:\n",
    "                    sentence = \"\".join(collected_text).strip()\n",
    "                    if sentence:\n",
    "                        print(f\"\\nüîä Speaking: {sentence}\")\n",
    "                        speak(sentence)\n",
    "                        collected_text.clear()\n",
    "\n",
    "    if collected_text:\n",
    "        final_text = \"\".join(collected_text).strip()\n",
    "        if final_text:\n",
    "            print(f\"\\nüîä Speaking final: {final_text}\")\n",
    "            speak(final_text)\n",
    "            messages.append({\"role\": \"assistant\", \"content\": final_text})\n",
    "\n",
    "    print()\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Speech setup (partials + final with language)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def setup_speech_recognition():\n",
    "    \"\"\"Wire STT callbacks. Partials will barge-in (stop TTS) when meaningful.\"\"\"\n",
    "    global user_buffer\n",
    "\n",
    "    def on_final(text: str, lang: str):\n",
    "        global user_buffer\n",
    "        print(f\"\\nüßæ User (final) in {lang}: {text}\")\n",
    "        user_buffer += text.strip() + \"\\n\"\n",
    "\n",
    "    def on_partial(text: str, lang: str):\n",
    "        print(f\"üó£Ô∏è User (partial) in {lang}: {text}\")\n",
    "        # Only barge-in on meaningful speech (>=3 chars)\n",
    "        if is_synthesizing and len(text.strip()) >= 3:\n",
    "            _stop_tts(\"user started speaking\")\n",
    "\n",
    "    az_speech_recognizer_stream_client.set_partial_result_callback(on_partial)\n",
    "    az_speech_recognizer_stream_client.set_final_result_callback(on_final)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Microphone loop\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def setup_microphone():\n",
    "    \"\"\"Stream mic PCM to recognizer.\"\"\"\n",
    "    global audio_stream, audio_interface, _mic_thread, conversation_active\n",
    "    try:\n",
    "        import pyaudio\n",
    "        audio_interface = pyaudio.PyAudio()\n",
    "        audio_stream = audio_interface.open(\n",
    "            format=pyaudio.paInt16,\n",
    "            channels=CHANNELS,\n",
    "            rate=RATE,\n",
    "            input=True,\n",
    "            frames_per_buffer=CHUNK,\n",
    "        )\n",
    "\n",
    "        def mic_loop():\n",
    "            while conversation_active and audio_stream:\n",
    "                try:\n",
    "                    data = audio_stream.read(CHUNK, exception_on_overflow=False)\n",
    "                    az_speech_recognizer_stream_client.write_bytes(data)\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Microphone error: {e}\")\n",
    "                    break\n",
    "\n",
    "        if not _mic_thread or not _mic_thread.is_alive():\n",
    "            _mic_thread = threading.Thread(target=mic_loop, daemon=True)\n",
    "            _mic_thread.start()\n",
    "            print(\"‚úÖ Microphone setup complete (thread started)\")\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è Microphone thread already running\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Microphone setup failed: {e}\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Agent lifecycle\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def start_voice_agent():\n",
    "    \"\"\"Start agent, ensure mic loop and STT are ready before talking.\"\"\"\n",
    "    global conversation_active, user_buffer\n",
    "    print(\"üéØ Starting Voice-to-Voice Agent...\")\n",
    "\n",
    "    conversation_active = True   # set active BEFORE mic thread starts\n",
    "    user_buffer = \"\"\n",
    "\n",
    "    setup_speech_recognition()\n",
    "    setup_microphone()\n",
    "\n",
    "    az_speech_recognizer_stream_client.start()\n",
    "    print(\"üéôÔ∏è Speech recognition started\")\n",
    "    time.sleep(0.1)  # tiny delay for SDK readiness\n",
    "\n",
    "    print(\"\\n‚úÖ Voice-to-Voice Agent Ready!\")\n",
    "    print(\"üí° Speak to interact with the customer support agent\")\n",
    "    print(\"üõë Use stop_voice_agent() to end the conversation\")\n",
    "\n",
    "def stop_voice_agent():\n",
    "    \"\"\"Stop agent and clean up resources.\"\"\"\n",
    "    global conversation_active, audio_stream, audio_interface\n",
    "    print(\"üõë Stopping Voice-to-Voice Agent...\")\n",
    "    conversation_active = False\n",
    "\n",
    "    try:\n",
    "        az_speech_recognizer_stream_client.stop()\n",
    "        print(\"‚úÖ Speech recognition stopped\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è recognizer.stop() error (ignored): {e}\")\n",
    "\n",
    "    try:\n",
    "        _stop_tts(\"agent stopping\")  # ensures synthesis is halted\n",
    "        print(\"‚úÖ Text-to-speech stopped\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è tts.stop() error (ignored): {e}\")\n",
    "\n",
    "    try:\n",
    "        if audio_stream:\n",
    "            audio_stream.stop_stream()\n",
    "            audio_stream.close()\n",
    "        if audio_interface:\n",
    "            audio_interface.terminate()\n",
    "        print(\"‚úÖ Microphone stream closed\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è mic close error (ignored): {e}\")\n",
    "\n",
    "    print(\"üéØ Voice-to-Voice Agent stopped. All resources cleaned up.\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Conversation loops\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "async def process_user_input():\n",
    "    \"\"\"Process any pending user input from STT buffer (single turn).\"\"\"\n",
    "    global user_buffer\n",
    "    if user_buffer.strip():\n",
    "        user_input = user_buffer.strip()\n",
    "        user_buffer = \"\"\n",
    "        print(f\"\\nüë§ Processing user input: {user_input}\")\n",
    "        messages = [{\"role\": \"system\", \"content\": PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": user_input}]\n",
    "        await process_streaming_response_with_tools(messages, TOOL_REGISTRY)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "async def full_conversation_loop():\n",
    "    \"\"\"Continuous conversation: start agent, then react to user speech.\"\"\"\n",
    "    greeting()\n",
    "    global user_buffer\n",
    "    print(\"üéØ Starting Complete Voice-to-Voice Conversation...\")\n",
    "    start_voice_agent()\n",
    "\n",
    "    global messages\n",
    "    messages = [{\"role\": \"system\", \"content\": PROMPT}]\n",
    "    try:\n",
    "        while conversation_active:\n",
    "            if user_buffer.strip():\n",
    "                user_input = user_buffer.strip()\n",
    "                user_buffer = \"\"\n",
    "                print(f\"\\nüë§ Processing user input: {user_input}\")\n",
    "                messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "                await process_streaming_response_with_tools(messages, TOOL_REGISTRY)\n",
    "            await asyncio.sleep(0.1)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n‚ö†Ô∏è Conversation interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error in conversation loop: {e}\")\n",
    "    finally:\n",
    "        stop_voice_agent()\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Optional: quick TTS sanity check (play once to confirm device/creds)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def greeting():\n",
    "  az_speech_synthesizer_client.start_speaking_text(\"Hi there from XYZ Customer service, How can I help you today?\")\n",
    "\n",
    "print(\"‚úÖ Voice-to-Voice Agent functions loaded!\")\n",
    "print(\"\\nüöÄ Usage Options:\")\n",
    "print(\"1. start_voice_agent()\")\n",
    "print(\"2. await process_user_input()  # one-shot\")\n",
    "print(\"3. stop_voice_agent()\")\n",
    "print(\"4. await full_conversation_loop()  # continuous\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a85fb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Start Complete Voice-to-Voice Conversation\n",
    "# This will start the agent and continuously process voice input\n",
    "# Press Ctrl+C or run the stop cell to end the conversation\n",
    "# test order id: ORD123456\n",
    "\n",
    "await full_conversation_loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audioagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
